{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "0   4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "1   7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "2   9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "3  12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "4  13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "\n",
       "   margin8   margin9    ...      texture55  texture56  texture57  texture58  \\\n",
       "0      0.0  0.005859    ...       0.006836   0.000000   0.015625   0.000977   \n",
       "1      0.0  0.033203    ...       0.000000   0.000000   0.006836   0.001953   \n",
       "2      0.0  0.011719    ...       0.128910   0.000000   0.000977   0.000000   \n",
       "3      0.0  0.003906    ...       0.012695   0.015625   0.002930   0.036133   \n",
       "4      0.0  0.005859    ...       0.000000   0.042969   0.016602   0.010742   \n",
       "\n",
       "   texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "0   0.015625        0.0        0.0   0.000000   0.003906   0.053711  \n",
       "1   0.013672        0.0        0.0   0.000977   0.037109   0.044922  \n",
       "2   0.000000        0.0        0.0   0.015625   0.000000   0.000000  \n",
       "3   0.013672        0.0        0.0   0.089844   0.000000   0.008789  \n",
       "4   0.041016        0.0        0.0   0.007812   0.009766   0.007812  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "0   4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "1   7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "2   9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "3  12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "4  13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "\n",
       "   margin8   margin9    ...      texture55  texture56  texture57  texture58  \\\n",
       "0      0.0  0.005859    ...       0.006836   0.000000   0.015625   0.000977   \n",
       "1      0.0  0.033203    ...       0.000000   0.000000   0.006836   0.001953   \n",
       "2      0.0  0.011719    ...       0.128910   0.000000   0.000977   0.000000   \n",
       "3      0.0  0.003906    ...       0.012695   0.015625   0.002930   0.036133   \n",
       "4      0.0  0.005859    ...       0.000000   0.042969   0.016602   0.010742   \n",
       "\n",
       "   texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "0   0.015625        0.0        0.0   0.000000   0.003906   0.053711  \n",
       "1   0.013672        0.0        0.0   0.000977   0.037109   0.044922  \n",
       "2   0.000000        0.0        0.0   0.015625   0.000000   0.000000  \n",
       "3   0.013672        0.0        0.0   0.089844   0.000000   0.008789  \n",
       "4   0.041016        0.0        0.0   0.007812   0.009766   0.007812  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                species   margin1   margin2   margin3   margin4  \\\n",
       "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "\n",
       "    margin5   margin6   margin7  margin8    ...      texture55  texture56  \\\n",
       "0  0.011719  0.009766  0.027344      0.0    ...       0.007812   0.000000   \n",
       "1  0.025391  0.001953  0.019531      0.0    ...       0.000977   0.000000   \n",
       "2  0.003906  0.005859  0.068359      0.0    ...       0.154300   0.000000   \n",
       "3  0.021484  0.019531  0.023438      0.0    ...       0.000000   0.000977   \n",
       "4  0.013672  0.015625  0.005859      0.0    ...       0.096680   0.000000   \n",
       "\n",
       "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "   texture63  texture64  \n",
       "0   0.000000   0.025391  \n",
       "1   0.039062   0.022461  \n",
       "2   0.020508   0.002930  \n",
       "3   0.000000   0.047852  \n",
       "4   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017561619528619524"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin1_mean = test['margin1'].mean(axis=0)\n",
    "margin1_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.009750</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.017562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.017562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.015609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "0   4  0.001969  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "1   7 -0.009750  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "2   9 -0.017562  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "3  12 -0.017562  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "4  13 -0.015609  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "\n",
       "   margin8   margin9    ...      texture55  texture56  texture57  texture58  \\\n",
       "0      0.0  0.005859    ...       0.006836   0.000000   0.015625   0.000977   \n",
       "1      0.0  0.033203    ...       0.000000   0.000000   0.006836   0.001953   \n",
       "2      0.0  0.011719    ...       0.128910   0.000000   0.000977   0.000000   \n",
       "3      0.0  0.003906    ...       0.012695   0.015625   0.002930   0.036133   \n",
       "4      0.0  0.005859    ...       0.000000   0.042969   0.016602   0.010742   \n",
       "\n",
       "   texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "0   0.015625        0.0        0.0   0.000000   0.003906   0.053711  \n",
       "1   0.013672        0.0        0.0   0.000977   0.037109   0.044922  \n",
       "2   0.000000        0.0        0.0   0.015625   0.000000   0.000000  \n",
       "3   0.013672        0.0        0.0   0.089844   0.000000   0.008789  \n",
       "4   0.041016        0.0        0.0   0.007812   0.009766   0.007812  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['margin1'] = test['margin1'] - margin1_mean\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           990\n",
       "species      990\n",
       "margin1      990\n",
       "margin2      990\n",
       "margin3      990\n",
       "margin4      990\n",
       "margin5      990\n",
       "margin6      990\n",
       "margin7      990\n",
       "margin8      990\n",
       "margin9      990\n",
       "margin10     990\n",
       "margin11     990\n",
       "margin12     990\n",
       "margin13     990\n",
       "margin14     990\n",
       "margin15     990\n",
       "margin16     990\n",
       "margin17     990\n",
       "margin18     990\n",
       "margin19     990\n",
       "margin20     990\n",
       "margin21     990\n",
       "margin22     990\n",
       "margin23     990\n",
       "margin24     990\n",
       "margin25     990\n",
       "margin26     990\n",
       "margin27     990\n",
       "margin28     990\n",
       "            ... \n",
       "texture35    990\n",
       "texture36    990\n",
       "texture37    990\n",
       "texture38    990\n",
       "texture39    990\n",
       "texture40    990\n",
       "texture41    990\n",
       "texture42    990\n",
       "texture43    990\n",
       "texture44    990\n",
       "texture45    990\n",
       "texture46    990\n",
       "texture47    990\n",
       "texture48    990\n",
       "texture49    990\n",
       "texture50    990\n",
       "texture51    990\n",
       "texture52    990\n",
       "texture53    990\n",
       "texture54    990\n",
       "texture55    990\n",
       "texture56    990\n",
       "texture57    990\n",
       "texture58    990\n",
       "texture59    990\n",
       "texture60    990\n",
       "texture61    990\n",
       "texture62    990\n",
       "texture63    990\n",
       "texture64    990\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                species   margin1   margin2   margin3   margin4  \\\n",
       "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "\n",
       "    margin5   margin6   margin7  margin8    ...      texture55  texture56  \\\n",
       "0  0.011719  0.009766  0.027344      0.0    ...       0.007812   0.000000   \n",
       "1  0.025391  0.001953  0.019531      0.0    ...       0.000977   0.000000   \n",
       "2  0.003906  0.005859  0.068359      0.0    ...       0.154300   0.000000   \n",
       "3  0.021484  0.019531  0.023438      0.0    ...       0.000000   0.000977   \n",
       "4  0.013672  0.015625  0.005859      0.0    ...       0.096680   0.000000   \n",
       "\n",
       "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "   texture63  texture64  \n",
       "0   0.000000   0.025391  \n",
       "1   0.039062   0.022461  \n",
       "2   0.020508   0.002930  \n",
       "3   0.000000   0.047852  \n",
       "4   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acer_Opalus', 'Pterocarya_Stenoptera', 'Quercus_Hartwissiana',\n",
       "       'Tilia_Tomentosa', 'Quercus_Variabilis', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Canariensis', 'Quercus_Rubra', 'Quercus_Brantii',\n",
       "       'Salix_Fragilis', 'Zelkova_Serrata', 'Betula_Austrosinensis',\n",
       "       'Quercus_Pontica', 'Quercus_Afares', 'Quercus_Coccifera',\n",
       "       'Fagus_Sylvatica', 'Phildelphus', 'Acer_Palmatum',\n",
       "       'Quercus_Pubescens', 'Populus_Adenopoda', 'Quercus_Trojana',\n",
       "       'Alnus_Sieboldiana', 'Quercus_Ilex', 'Arundinaria_Simonii',\n",
       "       'Acer_Platanoids', 'Quercus_Phillyraeoides', 'Cornus_Chinensis',\n",
       "       'Liriodendron_Tulipifera', 'Cytisus_Battandieri',\n",
       "       'Rhododendron_x_Russellianum', 'Alnus_Rubra',\n",
       "       'Eucalyptus_Glaucescens', 'Cercis_Siliquastrum',\n",
       "       'Cotinus_Coggygria', 'Celtis_Koraiensis', 'Quercus_Crassifolia',\n",
       "       'Quercus_Kewensis', 'Cornus_Controversa', 'Quercus_Pyrenaica',\n",
       "       'Callicarpa_Bodinieri', 'Quercus_Alnifolia', 'Acer_Saccharinum',\n",
       "       'Prunus_X_Shmittii', 'Prunus_Avium', 'Quercus_Greggii',\n",
       "       'Quercus_Suber', 'Quercus_Dolicholepis', 'Ilex_Cornuta',\n",
       "       'Tilia_Oliveri', 'Quercus_Semecarpifolia', 'Quercus_Texana',\n",
       "       'Ginkgo_Biloba', 'Liquidambar_Styraciflua', 'Quercus_Phellos',\n",
       "       'Quercus_Palustris', 'Alnus_Maximowiczii', 'Quercus_Agrifolia',\n",
       "       'Acer_Pictum', 'Acer_Rufinerve', 'Lithocarpus_Cleistocarpus',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Ilex_Aquifolium',\n",
       "       'Acer_Circinatum', 'Quercus_Coccinea', 'Quercus_Cerris',\n",
       "       'Quercus_Chrysolepis', 'Eucalyptus_Neglecta', 'Tilia_Platyphyllos',\n",
       "       'Alnus_Cordata', 'Populus_Nigra', 'Acer_Capillipes',\n",
       "       'Magnolia_Heptapeta', 'Acer_Mono', 'Cornus_Macrophylla',\n",
       "       'Crataegus_Monogyna', 'Quercus_x_Turneri', 'Quercus_Castaneifolia',\n",
       "       'Lithocarpus_Edulis', 'Populus_Grandidentata', 'Acer_Rubrum',\n",
       "       'Quercus_Imbricaria', 'Eucalyptus_Urnigera', 'Quercus_Crassipes',\n",
       "       'Viburnum_Tinus', 'Morus_Nigra', 'Quercus_Vulcanica',\n",
       "       'Alnus_Viridis', 'Betula_Pendula', 'Olea_Europaea',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_x_Hispanica', 'Quercus_Shumardii',\n",
       "       'Quercus_Rhysophylla', 'Castanea_Sativa', 'Ulmus_Bergmanniana',\n",
       "       'Quercus_Nigra', 'Salix_Intergra', 'Quercus_Infectoria_sub',\n",
       "       'Sorbus_Aria'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-601e21a0c2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "train['species'].unique().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['species'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [[6], [8], [10], [14],[18]]\n",
    "y_train = [[7], [9], [13], [17.5], [18]]\n",
    "X_test = [[6], [8],[11], [16]]\n",
    "y_test = [[8], [12], [15], [18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6], [8], [10], [14], [18]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFkCAYAAACw3EhvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkmSlPP/ARV4A3E3AqIIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1355775f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(0, 26, 100)\n",
    "yy = regressor.predict(xx.reshape(xx.shape[0], 1))\n",
    "plt.plot(xx, yy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.26262626,   0.52525253,   0.78787879,\n",
       "         1.05050505,   1.31313131,   1.57575758,   1.83838384,\n",
       "         2.1010101 ,   2.36363636,   2.62626263,   2.88888889,\n",
       "         3.15151515,   3.41414141,   3.67676768,   3.93939394,\n",
       "         4.2020202 ,   4.46464646,   4.72727273,   4.98989899,\n",
       "         5.25252525,   5.51515152,   5.77777778,   6.04040404,\n",
       "         6.3030303 ,   6.56565657,   6.82828283,   7.09090909,\n",
       "         7.35353535,   7.61616162,   7.87878788,   8.14141414,\n",
       "         8.4040404 ,   8.66666667,   8.92929293,   9.19191919,\n",
       "         9.45454545,   9.71717172,   9.97979798,  10.24242424,\n",
       "        10.50505051,  10.76767677,  11.03030303,  11.29292929,\n",
       "        11.55555556,  11.81818182,  12.08080808,  12.34343434,\n",
       "        12.60606061,  12.86868687,  13.13131313,  13.39393939,\n",
       "        13.65656566,  13.91919192,  14.18181818,  14.44444444,\n",
       "        14.70707071,  14.96969697,  15.23232323,  15.49494949,\n",
       "        15.75757576,  16.02020202,  16.28282828,  16.54545455,\n",
       "        16.80808081,  17.07070707,  17.33333333,  17.5959596 ,\n",
       "        17.85858586,  18.12121212,  18.38383838,  18.64646465,\n",
       "        18.90909091,  19.17171717,  19.43434343,  19.6969697 ,\n",
       "        19.95959596,  20.22222222,  20.48484848,  20.74747475,\n",
       "        21.01010101,  21.27272727,  21.53535354,  21.7979798 ,\n",
       "        22.06060606,  22.32323232,  22.58585859,  22.84848485,\n",
       "        23.11111111,  23.37373737,  23.63636364,  23.8989899 ,\n",
       "        24.16161616,  24.42424242,  24.68686869,  24.94949495,\n",
       "        25.21212121,  25.47474747,  25.73737374,  26.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.96551724],\n",
       "       [  2.22191745],\n",
       "       [  2.47831766],\n",
       "       [  2.73471787],\n",
       "       [  2.99111808],\n",
       "       [  3.24751829],\n",
       "       [  3.5039185 ],\n",
       "       [  3.7603187 ],\n",
       "       [  4.01671891],\n",
       "       [  4.27311912],\n",
       "       [  4.52951933],\n",
       "       [  4.78591954],\n",
       "       [  5.04231975],\n",
       "       [  5.29871996],\n",
       "       [  5.55512017],\n",
       "       [  5.81152038],\n",
       "       [  6.06792059],\n",
       "       [  6.32432079],\n",
       "       [  6.580721  ],\n",
       "       [  6.83712121],\n",
       "       [  7.09352142],\n",
       "       [  7.34992163],\n",
       "       [  7.60632184],\n",
       "       [  7.86272205],\n",
       "       [  8.11912226],\n",
       "       [  8.37552247],\n",
       "       [  8.63192268],\n",
       "       [  8.88832288],\n",
       "       [  9.14472309],\n",
       "       [  9.4011233 ],\n",
       "       [  9.65752351],\n",
       "       [  9.91392372],\n",
       "       [ 10.17032393],\n",
       "       [ 10.42672414],\n",
       "       [ 10.68312435],\n",
       "       [ 10.93952456],\n",
       "       [ 11.19592476],\n",
       "       [ 11.45232497],\n",
       "       [ 11.70872518],\n",
       "       [ 11.96512539],\n",
       "       [ 12.2215256 ],\n",
       "       [ 12.47792581],\n",
       "       [ 12.73432602],\n",
       "       [ 12.99072623],\n",
       "       [ 13.24712644],\n",
       "       [ 13.50352665],\n",
       "       [ 13.75992685],\n",
       "       [ 14.01632706],\n",
       "       [ 14.27272727],\n",
       "       [ 14.52912748],\n",
       "       [ 14.78552769],\n",
       "       [ 15.0419279 ],\n",
       "       [ 15.29832811],\n",
       "       [ 15.55472832],\n",
       "       [ 15.81112853],\n",
       "       [ 16.06752874],\n",
       "       [ 16.32392894],\n",
       "       [ 16.58032915],\n",
       "       [ 16.83672936],\n",
       "       [ 17.09312957],\n",
       "       [ 17.34952978],\n",
       "       [ 17.60592999],\n",
       "       [ 17.8623302 ],\n",
       "       [ 18.11873041],\n",
       "       [ 18.37513062],\n",
       "       [ 18.63153083],\n",
       "       [ 18.88793103],\n",
       "       [ 19.14433124],\n",
       "       [ 19.40073145],\n",
       "       [ 19.65713166],\n",
       "       [ 19.91353187],\n",
       "       [ 20.16993208],\n",
       "       [ 20.42633229],\n",
       "       [ 20.6827325 ],\n",
       "       [ 20.93913271],\n",
       "       [ 21.19553292],\n",
       "       [ 21.45193312],\n",
       "       [ 21.70833333],\n",
       "       [ 21.96473354],\n",
       "       [ 22.22113375],\n",
       "       [ 22.47753396],\n",
       "       [ 22.73393417],\n",
       "       [ 22.99033438],\n",
       "       [ 23.24673459],\n",
       "       [ 23.5031348 ],\n",
       "       [ 23.75953501],\n",
       "       [ 24.01593521],\n",
       "       [ 24.27233542],\n",
       "       [ 24.52873563],\n",
       "       [ 24.78513584],\n",
       "       [ 25.04153605],\n",
       "       [ 25.29793626],\n",
       "       [ 25.55433647],\n",
       "       [ 25.81073668],\n",
       "       [ 26.06713689],\n",
       "       [ 26.3235371 ],\n",
       "       [ 26.5799373 ],\n",
       "       [ 26.83633751],\n",
       "       [ 27.09273772],\n",
       "       [ 27.34913793]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(0, 26, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.26262626,   0.52525253,   0.78787879,\n",
       "         1.05050505,   1.31313131,   1.57575758,   1.83838384,\n",
       "         2.1010101 ,   2.36363636,   2.62626263,   2.88888889,\n",
       "         3.15151515,   3.41414141,   3.67676768,   3.93939394,\n",
       "         4.2020202 ,   4.46464646,   4.72727273,   4.98989899,\n",
       "         5.25252525,   5.51515152,   5.77777778,   6.04040404,\n",
       "         6.3030303 ,   6.56565657,   6.82828283,   7.09090909,\n",
       "         7.35353535,   7.61616162,   7.87878788,   8.14141414,\n",
       "         8.4040404 ,   8.66666667,   8.92929293,   9.19191919,\n",
       "         9.45454545,   9.71717172,   9.97979798,  10.24242424,\n",
       "        10.50505051,  10.76767677,  11.03030303,  11.29292929,\n",
       "        11.55555556,  11.81818182,  12.08080808,  12.34343434,\n",
       "        12.60606061,  12.86868687,  13.13131313,  13.39393939,\n",
       "        13.65656566,  13.91919192,  14.18181818,  14.44444444,\n",
       "        14.70707071,  14.96969697,  15.23232323,  15.49494949,\n",
       "        15.75757576,  16.02020202,  16.28282828,  16.54545455,\n",
       "        16.80808081,  17.07070707,  17.33333333,  17.5959596 ,\n",
       "        17.85858586,  18.12121212,  18.38383838,  18.64646465,\n",
       "        18.90909091,  19.17171717,  19.43434343,  19.6969697 ,\n",
       "        19.95959596,  20.22222222,  20.48484848,  20.74747475,\n",
       "        21.01010101,  21.27272727,  21.53535354,  21.7979798 ,\n",
       "        22.06060606,  22.32323232,  22.58585859,  22.84848485,\n",
       "        23.11111111,  23.37373737,  23.63636364,  23.8989899 ,\n",
       "        24.16161616,  24.42424242,  24.68686869,  24.94949495,\n",
       "        25.21212121,  25.47474747,  25.73737374,  26.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ],\n",
       "       [  0.26262626],\n",
       "       [  0.52525253],\n",
       "       [  0.78787879],\n",
       "       [  1.05050505],\n",
       "       [  1.31313131],\n",
       "       [  1.57575758],\n",
       "       [  1.83838384],\n",
       "       [  2.1010101 ],\n",
       "       [  2.36363636],\n",
       "       [  2.62626263],\n",
       "       [  2.88888889],\n",
       "       [  3.15151515],\n",
       "       [  3.41414141],\n",
       "       [  3.67676768],\n",
       "       [  3.93939394],\n",
       "       [  4.2020202 ],\n",
       "       [  4.46464646],\n",
       "       [  4.72727273],\n",
       "       [  4.98989899],\n",
       "       [  5.25252525],\n",
       "       [  5.51515152],\n",
       "       [  5.77777778],\n",
       "       [  6.04040404],\n",
       "       [  6.3030303 ],\n",
       "       [  6.56565657],\n",
       "       [  6.82828283],\n",
       "       [  7.09090909],\n",
       "       [  7.35353535],\n",
       "       [  7.61616162],\n",
       "       [  7.87878788],\n",
       "       [  8.14141414],\n",
       "       [  8.4040404 ],\n",
       "       [  8.66666667],\n",
       "       [  8.92929293],\n",
       "       [  9.19191919],\n",
       "       [  9.45454545],\n",
       "       [  9.71717172],\n",
       "       [  9.97979798],\n",
       "       [ 10.24242424],\n",
       "       [ 10.50505051],\n",
       "       [ 10.76767677],\n",
       "       [ 11.03030303],\n",
       "       [ 11.29292929],\n",
       "       [ 11.55555556],\n",
       "       [ 11.81818182],\n",
       "       [ 12.08080808],\n",
       "       [ 12.34343434],\n",
       "       [ 12.60606061],\n",
       "       [ 12.86868687],\n",
       "       [ 13.13131313],\n",
       "       [ 13.39393939],\n",
       "       [ 13.65656566],\n",
       "       [ 13.91919192],\n",
       "       [ 14.18181818],\n",
       "       [ 14.44444444],\n",
       "       [ 14.70707071],\n",
       "       [ 14.96969697],\n",
       "       [ 15.23232323],\n",
       "       [ 15.49494949],\n",
       "       [ 15.75757576],\n",
       "       [ 16.02020202],\n",
       "       [ 16.28282828],\n",
       "       [ 16.54545455],\n",
       "       [ 16.80808081],\n",
       "       [ 17.07070707],\n",
       "       [ 17.33333333],\n",
       "       [ 17.5959596 ],\n",
       "       [ 17.85858586],\n",
       "       [ 18.12121212],\n",
       "       [ 18.38383838],\n",
       "       [ 18.64646465],\n",
       "       [ 18.90909091],\n",
       "       [ 19.17171717],\n",
       "       [ 19.43434343],\n",
       "       [ 19.6969697 ],\n",
       "       [ 19.95959596],\n",
       "       [ 20.22222222],\n",
       "       [ 20.48484848],\n",
       "       [ 20.74747475],\n",
       "       [ 21.01010101],\n",
       "       [ 21.27272727],\n",
       "       [ 21.53535354],\n",
       "       [ 21.7979798 ],\n",
       "       [ 22.06060606],\n",
       "       [ 22.32323232],\n",
       "       [ 22.58585859],\n",
       "       [ 22.84848485],\n",
       "       [ 23.11111111],\n",
       "       [ 23.37373737],\n",
       "       [ 23.63636364],\n",
       "       [ 23.8989899 ],\n",
       "       [ 24.16161616],\n",
       "       [ 24.42424242],\n",
       "       [ 24.68686869],\n",
       "       [ 24.94949495],\n",
       "       [ 25.21212121],\n",
       "       [ 25.47474747],\n",
       "       [ 25.73737374],\n",
       "       [ 26.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.linspace(0, 26, 100)\n",
    "pp= xx.reshape(xx.shape[0], 1)\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFkCAYAAACw3EhvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkmSlPP/ARV4A3E3AqIIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb125690320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(0, 26, 100)\n",
    "yy = regressor.predict(xx.reshape(xx.shape[0], 1))\n",
    "plt.plot(xx, yy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quadratic_featurizer = PolynomialFeatures(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    6.,   36.],\n",
       "       [   1.,    8.,   64.],\n",
       "       [   1.,   10.,  100.],\n",
       "       [   1.,   14.,  196.],\n",
       "       [   1.,   18.,  324.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\n",
    "X_train_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_quadratic = quadratic_featurizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_quadratic = LinearRegression()\n",
    "regressor_quadratic.fit(X_train_quadratic, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAGHCAYAAAD2qfsmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xm8TfX6wPHPk6EipNstt7pUhqI5TRo0ieLXThpEVEgD\nShLVrXCbRJMbzWlUR3Wrwy0hGhUqR4Opw4koUqLIMfv+/njWybHtc84++6y91h6e9+u1X85ee+29\nnv3s5ezv+a7v9/mKcw5jjDHGmGTYKewAjDHGGJO5rKFhjDHGmKSxhoYxxhhjksYaGsYYY4xJGmto\nGGOMMSZprKFhjDHGmKSxhoYxxhhjksYaGsYYY4xJGmtoGGOMMSZprKFhAiUip4rIVhFpHnYsfhOR\nK7z3VjfsWEzZRORDEXk/7DhKIyL1vHPqsmLbBonI1jDjMqY8rKFhfCEil3u/EItu60TkOxEZLiJ7\nRe2eqXXvHZn73jJRun5WDkjphoaINBaRgdboNmANDeMvB9wOdAJ6Ap8C1wKficguAM65j4BdnXMf\nhxZl8ryIvrfFYQdiMtpdQLWwgyhDE2AgsH/IcZgUUDnsAEzGGe+cy/N+flZEVgJ9gPOAVwGccxvD\nCi4ZRKSac67Q6QqFob83EdkZ2OgCWjGx6P0HcSwDzrmtpMB5VgYhCT1Gdq6lJ+vRMMn2PvpL5wDY\ncYxGjEsuxW/ve/s8V8o+A7x9qojInSLypYj8LiJ/isjHInJaPEGKyCIRGSsiZ4nITO/Sz2wROT9q\nv6J4m4vIYyKyHFjiPRZzjIaInCMiH4nIahH5Q0Q+F5EOUfscLyLjvdjXeuMHTowj7qJ8theRu0Xk\nR2AtUMN7vJaIDBORxSKyXkTmi0h/EZGo19lDRF7y4lvl5fzwGOMDnheRNSJyoIiME5HVwKjyvA8R\n2c2LaaEX03IRmSgiRxbbp4GIvCEiy7zPYomI5IhIjajX6uR95oUi8pu3z34x8nSViCzw9psmIieX\nldtiz60kInd4z1/vxX2PiFSN2q/oHDpJRKZ7cReISOc4j1PLy+/vRZ8BsHuM/XYYoyEiXURkspfL\n9d65e02M5xbFeKqIfOHl4xsROdV7vJ13f52X1yNjvMZBIvJfL9/rvNc5t9jjlwOveXc/9M6hLVJs\nXJb3f+Jj0f+nq0XkbRFpEnWcUs81kz6sR8MkWwPv39+KbSv+l85H6KWW4vYH7gaWe/efAN6L2ucc\noGOxfWoCXYEc4Cn0i7YbMF5EjnPOfVNGnA5oBIz2jvc80AV4XURaOecmR+3/GPAL8G+gerHX2O6v\nOBG5AhgJzALuBX4HjgJaebEiImcA44AvgUHo9fcuwPsicrJz7ssyYge4A9gA3A/sDGwUkV2Bj4F/\neO9pCXAiMBioA9zoHV+At4FjvPf1HdoD9UL0+/HuVwYmAJ8AfYHCcr6PJ4F2wHBgLvA34GSgMfCV\niFQBJgJVgEeAn4F9gf9Dv3jXeMe7DbgT/cyeBv4OXA98JCJHOedWe/t1897/FOBh4EBgLLASiOcy\n10jgMvTL8wHgeOBW4GDggqjcNARe957zPHpOPiciXzrn5pZxnLHo5/M4MA84n5I/g+ht16Dn2Bhg\nM3Au8JiIiHPu8Rgxvox+Di8B/YCxInItcA/wKPrHwb/QXsiDip4sIoegefwRPY/WAhcDuSLSzjk3\nBj3nHgGuQ/8fz/OePtd7jc5ebsYD/dHLQNcCn3ifW9FnUuK5ZtKMc85udqvwDbgc2AKcjn5x7Au0\nB34F/gT+4e13qrdf8xJeZ2f0i2oJsFcJ+9QHVgHvAuJtE6By1H41gWXA03HEv9CL67xi22oAPwFf\nRr3PrcCHRceOkYO6xY7/BzpWpWopx/4OeCdGHgrQS1GlxX2qF8/86GOg42VWAwdGbb8X7Xrf17vf\nznuNXlH7TfLez2XFtj3nbbs70ffhfXaPlPKejvDiOb+UfeoCm4Cbo7Y38d7bLd79ymhD5cvi5wfa\nCN0KvF9Gfg/39nsiavtQLw+nxjiHTiy2bU9gHTC0jOOc5x3nxmLbBG2IR38GA4Et0XmO8ZrvAvNL\nOM+PK7btLO/YfxadE9727kT9X/XOiZns+H9tCjCv2P0Lop/rba+ONvAej9r+d++8eKLYthLPNbul\n180unRg/CTAZbVwsAV5Bv+jaOueWxfkajwOHAO2cc7/scACRakAu2kPS0Xm/kZza7O0jIlIbqIp+\nwRwd57GXOv2LDO8116ADPI+S7WfOOLTxUtY16LOA3YD7XAnjUryu6YZAjoj8reiGNnImA/FOA34+\nxjEuRP8S/CPqtSejX8BFr302+uX8TNTzi/6yjeWJCryP34HjReQfJbz2H0Vxeb0ysVzgxfZ61PF+\nQRtdp3v7HQvshX6BbS72/BeKHac0rdHP++Go7Q96x28TtX2Oc+6zojvOuRVoA+zAMo5zDtpw+iuv\n3vk1nJI/A4rtu6HoZxGp6eXiY+DA6MtNXoyfF7s/3ft3snPup6jtUhS793/qdLTHplZU3icCDUv5\nTIucBdQCRkc933nHOz3Gc56Isc2kEbt0YvzkgB7oL/rNwHLn3HfxPllErgauALo7574oYbdn0PEe\nzZxzq6Kefzl6OeBgtNu9yPdxhrAgxrZ879/90S+xIovieL363r+zS9mnoffviyU8vlVEajnnyvpS\njBVPQ+AwtOEXzaFfwKC9A8ucc+uj9omVD4DNzrkfYxwL4nsf/dGu8yUiMgO93PKic24hgHNukYg8\niH6WnUTkE/SywijnXQ5BL8ntVEKMxQfl1vXub7efc26ziMRzXtRD/9qPfv5yEfnde7y4WJdiVgG1\n4zjOMrfjQMe4/v+IyEnoZbwT2H5GikO/2NeUFKNzbrVePSP6My0654pib4A2PO5CL4lEKzqnSvuj\noqH3Gh+U8PzVUdtinWsmzVhDw/jtC7dt1kncROQ4YBjwlHNuZAn79EYvx1zqnPs26rFOaFfrm2i3\n9i9ot+u/KPuvyUSs8+l1inoV+wJfl7DPn3G8Tqx4dkLHtgwh9l/F+TG2xWNDjG1xvw/n3Osi8jE6\nBqElcBNws4ic75yb4O3TT0SeRy8ptESv+d8qIsc755Z6x9uK9sbEqikRT87KI94ZFFtK2F5mr0Si\nRORA9JLGXHSG1xK0odUGuIEdB/2XFGNZsRe9zgPouIlYSmqcFtkJzWUnto2vKm5z1P1Y55pJM9bQ\nMKETkT2B/wJ5QK8S9jkFHej4sHNudIxdLgAKnHMXRj3vznKE0iDGtqKBcIvK8TpFCtBf0odScq9K\ngffvGuec31UqC4DdnHOx/nos7gfgNBHZJapXo2FJTyjhWBDn+3DOLUe7xJ/wPv+ZwG0U+wJzzs1G\ne4PuFZETgM/QQY8D2JbbRc650r7cfvD2a4iOqwFARCqjPWNflRHqD+iXY0OK9S54l9J29x73ww/A\nGbLj9M2D43juuehlwnOLX/oQkTN9iq1I0Tm8KY7PuKSGWdHn9msSzneTomyMhgmViOyEjmyvDFwY\ndR29aJ863j4fo93usezw15iIHA80K0c4+0ix6awiUhPoDMyMNV4kDhPRLutbRWtbxDID/eV7k4hU\nj37Q+xJO1GtAMxFpGeN1a3m5B/1yr4oO/it6XNCia/H+JR/X+xCRnby8/sUbx7AUHTiKiNQQkUpR\nLzEb7bkoyuOb3v2BsYIRkT28H79ELx1d4zUuinQhxtTRGMahX4w3RG3vi+bmnTheIx7j0Mt91xZt\n8D6f6yj7Myg69//6fS4itdDLkL5xzv2KNtau9v5PbifqXF2L5i06xxPQyyP/ivo8Yr2GyRDWo2H8\nFG/3cPH9rkUHgD2O/kVXfL/lzrlJ6IC4PYH/AR2i9vnGu4zyNtBORHLRX/4HAlejX1C7xRlXPvCM\niByLdut2Q685X15K/CVyzq0RkT7o1MsvROQV9Hr9EWgF0S7OOSciV6JfNLNFayf8hM7aOR29Tn5e\nnPFHux+IAG97lyFmoKP+D0dnmuyPzgDIBT4HHhSRhuh0xAjbviTKbGyU433UAH4Ukf+il1j+RAcI\nHoM33RY4AxghIq+jn0lldHrpZuAN73jfi8jtaG/HAd57WIN+7m3RqZsPeWMxbkd7Tz4QkVfRnowu\nbOuFKe19fSMiLwBXeYMhP0Knt14GvOm00q0f/ofOTrrPez9z0M8oeiBnLBPRgaRvi8iT3nOuRM/h\nHRoEFdQTHWD8rYg8jfZy7I026PdFp26D9hRtQS+J7Y5eApnsnFvhTaN9EcgTkdFoQ7AueqlnCjpF\n2WSSsKe92C0zbmyb2nl0GfttN70Vb6peCbf3vX0+KGWfAcVe+2b0F18h+pfsOei4jYI44l+IDjhs\ngf6SLEQbKefH+z6Jmt5abHsb9Jfzn2hDYypwcdQ+h6Oj+X/xjv09WmfjtDjz2a6Ex6uhA/e+Q8dx\nLPdiuQGoVGy/PdCaCr+jjY/ngJPQXoOLiu33HPBHKfGU+j7Qv9rvQy+T/Y7+dZsHXFXsNfZHG2f5\n6F/Gv6JjEHbIBdqo+Mh7ndXeZ/YfoEHUflej4wcK0dkNJ6HF5CbHcW7shE4VXgCsRy+j3QVUidrv\ne2BMjOd/EOdxdkcHya4q9hkcTuzprZtjnGMzvXwVoD0uV0Sfj6XEuAX4T9S2et72PlHb9/di+8nL\nx2K0fkfbqP26ogPDN7LjNNnmaKN0pRdzPlp75Kh4zzW7pc+tqAaBMVlNRBYC3zrnImHHkipEpC3a\ng3Cyc25q2PEYY9JT6GM0RORW0ZLMq0XL574lIo2i9olVgnpcWDEbk2nEW/Su2P2i8QFFPQ7GGJOQ\nVBijcQp6Df5LNJ7BwEQRaeycKz5l7120K7Do+rhNezLGP8O94lhT0QGXF6A1GW51xYpBGWNMeYXe\n0HDOtS5+X3RtiF+ApujAoCIbnI56NiYZHElYbTKNvI8OxmwD7IKOR+jltl8nwxhjyi30hkYMu6O/\n8FdGbT9NdKXMVegvxdudc9H7GJMQ51wyinqlDedcDt4ib8YY46eUGgzqzd3/H1DDOXdqse0Xo6PF\nF6JlnQejU9mauVR6A8YYY4zZTqo1NB5Hl88+yZWyCJc3z7wAONPFqHroLdLTCp2GFr1+gzHGGGNK\ntgs6jXmCc+63ir5Yylw6EZER6EqJp5TWyABwzi0UkRVoyehY5ZVbAS/7H6UxxhiTNS5FV+GukJRo\naHiNjPOAU51zsVY/jN5/P+BvlLxK4CKAUaNG0bhxY7/CNGXo06cPDz8cvZq2SSbLefAs58HLxpw7\nBxMnwtChsHUr3HQTtG4NEm/95QqYO3cunTp1gsTWeNpB6A0NEXkM6ICWPF4rInt7D/3hnFvvrZsw\nEC0c9DPaizEErSRX0gqC6wEaN27M0UcfnczwTTG1atWyfAfMch48y3nwsi3nP/0EPXrA2LFw4YUw\nYgTsvXfZz0sCX4YehF6wC12NsSa6WM/SYreLvce3oGV4x6BllJ8GvkDL2W4KOlhTsp9//jnsELKO\n5Tx4lvPgZUvOnYNnnoFDDoHPP4c33oDXXw+tkeGb0Hs0nHOlNnacLlt9dkDhmAr46aefyt7J+Mpy\nHjzLefCyIecLF0L37jB5MnTpAg8+CLVrhx2VP1KhR8NkiKZNm4YdQtaxnAfPch68TM75li0wbBgc\neigsWAATJsCzz2ZOIwOsoWF81KFDh7BDyDqW8+BZzoOXqTmfMwdOPhluvBG6dYNZs6Bly7Cj8l9K\n1dHwi4gcDcyYMWNGVg0gMsYYk/o2bYIhQ+Cuu+CAA3Rcxsknhx3VNnl5eUW9SE2dcxVeVDH0MRrG\nGGNMtpgxA7p2hdmzoX9/GDAAdtml7OelM7t0YnzTpUuXsEPIOpbz4FnOg5cJOV+3Dm65BY4/Hnba\nSWeV3Htv5jcywHo0jI9aZuLFxRRnOQ+e5Tx46Z7zTz6BK6+ERYvgzjuhXz+oUiXsqIJjYzSMMcaY\nJFizRnsxHnsMmjWDkSMhHYpV2xgNY4wxJsVNmABXXQUrVsB//gM9e0KlSmFHFQ4bo2GMMcb4ZOVK\nuOIKOPtsaNRIp6xef332NjLAGhrGR1OmTAk7hKxjOQ+e5Tx46ZLzN96AJk0gN1cvk0ycqNNXs501\nNIxvhg4dGnYIWcdyHjzLefBSPec//6yLn114oY7FmDNHp7AGsdJqOrAxGsY3o0ePDjuErGM5D57l\nPHipmnPn4MUXoU8fqFwZXn0VLrrIGhjRrEfD+KZatWphh5B1LOfBs5wHLxVz/sMPcM45Oh6jTRvt\nxbj4YmtkxGINDWOMMSZOW7fCo4/qImizZsHbb8NLL8Gee4YdWeqyhoYxxhgTh+++g1NPhV69oFMn\n7cVo0ybsqFKfNTSMb/r16xd2CFnHch48y3nwws755s26CNoRR8CyZfDBB/D441CzZqhhpQ1raBjf\n1K1bN+wQso7lPHiW8+CFmfOvv9b1Sf71L+3J+OYbOO200MJJS1aC3BhjjImyYYMu4z5kCBx8MDz7\nLBx7bNhRBcNKkBtjjDFJNHUqdOsGCxbAHXfoeiVVq4YdVfqySyfGGGMMsHYt3HADnHQS1KgBeXkw\nYIA1MirKGhrGN/PmzQs7hKxjOQ+e5Tx4QeR80iSdsvrUU/DAA/DZZ3rfVJw1NIxv+vfvH3YIWcdy\nHjzLefCSmfPff9fLJGedBfvvr4M9b7wxuxdB85uN0TC+GTFiRNghZB3LefAs58FLVs7HjIFrr9VL\nJk8+CVdeCTvZn9++s5Qa39i0v+BZzoNnOQ+e3zn/5Re45BJo2xaOPhpmz4arrrJGRrJYj4Yxxpis\n4By88gr07q33X34ZOnSw9UmSzdpvxhhjMt6SJXDuuVo6/KyztHx4x47WyAiCNTSMb4YMGRJ2CFnH\nch48y3nwKpLzrVt1/MUhh+h01dxcyMmBvfbyMUBTKmtoGN8UFhaGHULWsZwHz3IevERzXlAAZ54J\n11wD7dtrL8Z55/kcnCmTlSA3xhiTUbZsgWHDtKpnnTrw9NPa4DDx8bsEufVoGGOMyRizZkGzZtCv\nH1x9NXz7rTUywmYNDWOMMWlv40b49791uuqff8Knn8LDD0P16mFHZqyhYXyzYsWKsEPIOpbz4FnO\ng1dWzr/4Apo2hbvvhptvhpkztVfDpAZraBjfdO3aNewQso7lPHiW8+CVlPPCQrjpJjjhBF347Msv\ndWn3nXcOOEBTKivYZXwzaNCgsEPIOpbz4FnOgxcr5x99pCXDlyyBe++Fvn2hsn2jpSTr0TC+sRk+\nwbOcB89yHrziOV+9WtcnOe00nVHy9dd6ucQaGanLPhpjjDFpYdw4nUny++8wYoQ2OGx9ktRnH5Ex\nxpiUtmIFdO4MbdpAkyY6hbVnT2tkpAv7mIxvRo4cGXYIWcdyHjzLeXCcg9degwMPHMk778Dzz8P4\n8VCvXtiRmfKwhobxTV5ehQvImXKynAfPch6MpUuhXTstHb7XXnnMmQOXX26LoKUjK0FujDEmZTgH\nzz0HN94Iu+wCjz2mDQ4THCtBbowxJiMtXAgtW0K3bnD++TB3rjUyMoE1NIwxxoRqyxZ45BE49FDI\nz9dxGM89B7Vrhx2Z8YM1NIwxxoRm7lxo3hx694auXXVGSatWYUdl/GQNDeObSCQSdghZx3IePMu5\nPzZt0oqeRx6p01c/+QSGD4caNXbc13Ke3qxgl/FNr169wg4h61jOg2c5r7i8PB2H8e23upz7gAGw\n664l7285T2/Wo2F807Jly7BDyDqW8+BZzhO3bh3ceiscdxxs3QrTp8PgwaU3MsBynu6sR8MYY0zS\nTZmivRiLFsGgQbo+SZUqYUdlgmA9GsYYY5JmzRq47jod8LnHHjBzJtx+uzUysok1NIxvcnNzww4h\n61jOg2c5j9/EiTpl9dln4eGHtVejSZPyv47lPL1ZQ8P4JicnJ+wQso7lPHiW87KtWgVduug01YYN\nddBn795QqVJir2c5T29WgtwYY4xv3noLevTQgZ8PPqi1MWx9kvSScSXIReRWEflcRFaLyHIReUtE\nGsXY704RWSoihSLynog0CCNeY4wxO/r5Z7joIi0ZftxxMHu2Dv60RoYJvaEBnAIMB44HWgBVgIki\n8teEJxG5GegFXAUcB6wFJohI1eDDNcYYU8Q5ePFFHXvx0UcwejTk5sK++4YdmUkVoU9vdc61Ln5f\nRK4AfgGaAlO8zb2Bu5xzb3v7XAYsB9oCrwUWrDHGmL8sXgxXX61rk1x6KQwbBnvuGXZUJtWkQo9G\ntN0BB6wEEJEDgDrA5KIdnHOrgelAszACNLF16dIl7BCyjuU8eJZzLbb12GNwyCE60PPtt2HUqOQ1\nMizn6S2lGhoiIsAwYIpzbo63uQ7a8Fgetfty7zGTIqx6X/As58HL9pzn58Npp0HPntqLMXs2tGmT\n3GNme87TXUo1NIDHgCbAJWEHYsqvQ4cOYYeQdSznwcvWnG/eDEOGwOGHw9Kl8MEH8MQTUKtW8o+d\nrTnPFCnT0BCREUBr4DTn3LJiD/0MCLB31FP29h4rUevWrYlEItvdmjVrtkPxl4kTJ8ZcHbBnz56M\nHDlyu215eXlEIhFWrFix3faBAwcyZMiQ7bYtXryYSCTCvHnztts+fPhw+vXrt922wsJCIpEIU6ZM\n2W57Tk5OzG7D9u3b2/uw92Hvw95HIO/j8sv7ccIJ8K9/aZXPadMKeeih9HsfmfJ5+Pk+cnJy/vpu\nrFOnDpFIhD59+uzwnIpIiToaXiPjPOBU59z3MR5fCtzvnHvYu18TvXRymXPu9Rj7Wx0NY4ypoA0b\n4O674b774OCDYeRInbpqMlsm1tF4DLgU6AisFZG9vdsuxXYbBtwuIueKyGHAi8CPwJjgIzYliW5N\nm+SznAcvW3I+bRocfbReLrn9dpgxI7xGRrbkPFOF3tAArgFqAh8CS4vdLi7awTk3FK218SQ622RX\n4Bzn3MaggzUlGzp0aNghZB3LefAyPedr10KfPnDiibDbbpCXBwMHQtUQqxZles4zXUpcOvGbXToJ\nR2FhIdWqVQs7jKxiOQ9eJud88mTo3h2WLdNLJjfckPj6JH7K5Jynooy7dGIyh/0iCJ7lPHiZmPPf\nf9cGRosWUK+e1sbo2zc1GhmQmTnPJqFXBjXGGBOesWPh2mthzRqdrtq9O+xkf4IaH9npZIwxWejX\nX6FDBzjvPDjySC28dfXV1sgw/rNTyvgmem63ST7LefDSPefOwSuvQOPG8N578NJLWkL8n/8MO7KS\npXvOs501NIxv6tatG3YIWcdyHrx0zvmPP0IkoqXDW7SAOXOgU6fUX8o9nXNubNaJMcZkvK1b4Zln\noF8/qF4dHn9cL5kYE4vNOjHGGBO3ggI480wdf3HRRdqLYY0MEyRraBhjTAbasgUefBAOOwwWLdLx\nGM88A7vvHnZkJttYQ8P4JnrxH5N8lvPgpUPOZ83Syp79+sFVV2ldjBYtwo4qcemQc1Mya2gY3/Tv\n3z/sELKO5Tx4qZzzjRvh3//WNUpWr4YpU2DYMC0lns5SOeembFawy/hmxIgRYYeQdSznwUvVnH/x\nBXTrpmMwbrlFF0LbZZeyn5cOUjXnJj7Wo2F8Y1PQgmc5D16q5bywEPr3hxNOgMqV4csvdZ2STGlk\nQOrl3JSP9WgYY0ya+ugjuPJKWLIE7rlH1yepUiXsqIzZnvVoGGNMmlm9WtcnOe002Htv+PprvVxi\njQyTiqyhYXwzZMiQsEPIOpbz4IWd83ffhUMP1dLhjzwCH38MBx0UakhJF3bOTcXYpRPjm8LCwrBD\nyDqW8+CFlfPffoMbboBRo+Css+Cpp2D//eN8snOwbp2uB79mTYktk/z8fAoKCjj0jz/4Z+XKULUq\n7Lzztn932QVq1oS//Q1q1/btvZXFzvP0ZiXIjTEmhTkHr78OvXrBpk3w8MNw+eWlrE/y2WdaqWv5\ncvjlF1i1ShsYmzfr45UqbfvZs3LlSjp27MyECeMAeAtoW1pQHTroymylBf3CC7Dffrpa2z//CdWq\nxfuWTcj8LkFuPRrGGJOili2Dm7r/wYJ35vGvpvPpdko+NQ45F+TYkp+0ZQusXQsHHgjNmmnPw+67\nQ61a+m/NmtoQKNZS6dixM5MmTQNGAc3pyHvsulNfWjRvyqsvPQ8bNmiRjsJC7REpqzdj5Uro0mX7\nbXvtBQ0aQP36+u8VV4DNJskK1tAwxphU8vrruC9n8OO738Ksb3nZLdHtM4Bl+8CxB8OxpTQ0TjkF\nxo+P+3D5+fleT8Yo4FIA1tGVdVur8tqHnbl73ToaNmxYvvfwt7/ppZqfftIpMUuWwMKFsGCB3iZM\ngHPPtYZGlrCGhvHNihUr2HPPPcMOI6tYzoOX7JxvGHgPf3y/km82HIY7pCO79zqMGsc1hoYNoUYN\n349XUFDg/dQ86pFTAViwYEH5Gxqg4znq19dbIgYPhvffh6ZNWdGkCXuefbb2ipi0Y7NOjG+6du0a\ndghZx3IevIRyvmULzJypq5qVsssjj0CdRdM5du/FVHr3Hf5v1n3UuOZSrSmehEYGQP2/GgIfRz3y\nEQANGjRIynHLVK+e1k4fNYqul1+u83gbNtQBKk8+Cd9/H05cptysR8P4ZtCgQWGHkHUs58GLK+fO\nwXffweTJevvwQx2UWbUqtG0LUT0ic+dq4a3PPoOePXdm8OCktSt20KhRI1q1as2kSdezZYtDezI+\nolKl3rRo0Tqx3gw/dOyoN2DQuHE6NuSzz/T28staSGT48HBiM+Vis06MMRmvaNpmgwYNkvvFuWKF\nVs4aP17HJ1SporXBzzwTzjhDx1YUqw2+aRPcf78uhFavHowcqUMsgrZq1So6dOj016wTgFatWpOT\nM4raAU5jjduaNTowde+9w44kI9msE2OMiVP0tE1I8hdozZpaprN9ey12ccopUL16zF1nzoSuXeGb\nb+Cmm2DQINh1V/9Dikft2rUZP/4d5s+fz4IFC5LfIKuoGjXK7vIZM0aLjbRtC5GINUpCZA0NY0zG\nip62CR8zadL1dOjQifHj3/H/gFWr6jKqpVi/Hu68E4YOhSZNYPp0OOYY/0NJRMOGDVO7gVEeO++s\n03yvuQY19nEWAAAgAElEQVSuvlqn+rZtC+3aJT5A1STEBoMa34wcOTLsELKO5bxkRdM2t2x5BJ22\n+U/gUrZs+Q8TJoxj/vz58b/YnDlwxx3QrBkjn3oq4Zg+/RSOPFLraQ0apCutpkojI5UldJ6ffbaO\njVm+HJ59Fv7+dxgwQGt4HHOMjvMwgbCGhvFNXl6FL+WZcrKclyyeaZul+uEHGDJEWwaHHKIDDxs3\nJu/zz8sdy59/wvXX65WU3XfXyya3364dIKZsFTrP99xTi4Pl5uoYmtde0wExK1b4Fp8pnQ0GNcZk\npPz8fA466CCKF6JSo4DO5Ofn73iZYMsWLa39/PNaw2HXXbWwVIcOcM452h1fTu+9B927azXwe+7R\nBkelSom/L2OSzQaDGmNMHBKatrnTTnD33bDPPtrYaNcu4Xmmq1ZB377w3HM64eT997UquEkT06fr\n4N7GjcOOJO3ZpRNjTMbKyRlFixYnAJ2BukBnWrQ4gZycUbGfIAJ5efDBB1oYKsFGRm6uDvR84w14\n+mmYNMkaGWln0CD9EJs31yVz160LO6K0ZQ0NY0zGKpq2mZ+fz7hx48jPz2f8u2+XPrW1hOmo8Vi+\nHC6+GM4/X0tmzJmjhbhKXGnVpK7cXBg9WmuhdO4M++6r85AXLgw7srRjDQ3jm0gkEnYIWcdyHp+G\nBxzAOYWFNOzWDV56qUKvFSvnzunLNmminSGvvKJlHPbdt0KHMp5QzvOdd9Z6KJMnQ34+dOums1fq\n14fzzoPFi4OPKU1ZQ8P4plevXmGHkHUs52X4/XctvVm/Plx4oW7bb78KvWR0zhcvhjZt4LLLdEbl\n3Lk6dtR6MfwT+nnesKGeRz/+qOus/PYbpGLF1BRls06MMZnnxx9h2DD9Uti4ES65BHr31sXJfLJ1\nKzzxBNx8M9SqpT//3//59vLGhMZmnRhjTGnee0+7GKpVg+uu0/mkder4eoj583Xsxccfw1VXaZXP\nWrV8PYQxGcMunRhjMsuJJ+o3/5IlcO+9vjYyNm/Wlz78cF0z7f33tdPEGhlmO1OnagvUlrIHrKFh\nfJSbmxt2CFnHch5D9epwww2+r7P+zTe6XMYtt+TSo4feP/10Xw9hSpB25/myZToauFEjHbwzb17Y\nEYXKGhrGNzk5OWGHkHUs58m3YYMukdG0qa5MfsYZOTz4oF6ZMcFIu/O8XTudBvvww9rt1aQJdOqk\n19yyULkbGiKyq4hUK3a/nojcICIt/Q3NpJtXX3017BCyTtbl/JtvdAXOKVMCOdy0aTp+dPBguO02\nreU1aVKW5TwFpOV5XjRGqKAARozQec+NG+s02aVLw44uUIn0aIwBLgMQkd2B6UBfYIyIXOtjbMYY\no+bPh44ddYGzb79NepXGtWvhxht1uEe1atrAGDQooaVOTLbbeWfo0QMWLIAHHoCJE3UmVBZJpKFx\nNPCJ9/OFwHKgHtr4uN6nuIwxRktt9uihfwl+/LHOIZ03D846K2mHfP99Hez5+OO6eOvUqXDYYUk7\nnMkWu+6qY4cWLoT99w87mkAlMr21GrDG+7kl8KZzbquITEMbHMYYUzFr18JDD+kUj8qV9Ru/Z0/Y\nZZekHfKPP6BfP12bpHlzGD9e6zQZ46vK2VdVIpEejQVAWxH5J9AKmOht3wtY7VdgJv106dIl7BCy\nTsbmfMUKePBBuOYavcbdt29SGxlvvw2HHKJLWzz+uF5OL6mRkbE5T2FZlfONG7WmfQZJpKFxJ/AA\nsAiY7pyb6m1vCcz0KS6Thlq2tPHAQcvYnNerp3Uw7r8f9tgjaYf59Vcd+nHuuXq5ZPZsbdvsVMpv\nxozNeQrLqpxffz20agWzZoUdiW8SKkEuInWAfwBfO+e2etuOA1Y750KfMGwlyI0xpXFOey+uv15L\nif/nP3DppbY+iUkB//uf9uAVFMDVV8Odd8KeewYagt8lyMvVoyEiVURkM7Cnc25mUSMDwDn3eSo0\nMowxpjQ//aSLb3bsCGecoUu5d+pkjQyTIs49V3sz7r9flwFu1Eiv523ZEnZkCStXQ8M5twlYDFRK\nTjjGmKwwdiz8+9+BHtI5HejZpAl88QW89Ra8+irsvXegYRhTtqpVdX71/Pla/KtHDzjuOC3skoYS\nGaNxD3CviCTvwqlJS1MCKqJktkm7nC9cCJGIdilMn66LhwSgoADOPFOXn7jgAu3FaNs2sddKu5xn\ngKzN+d//Ds88o3OsAQYODDeeBCXS0OgFNAeWish3IpJX/OZzfCaNDB06NOwQsk7a5HzjRl3grEkT\nmDkT/vtfeOedpE/127JFq0Afdpi2cSZOhGefhdq1E3/NtMl5Bsn6nJ9wAnz+uV5KSUOJ/C9Ps9Vt\nTFBGjx4ddghZJy1yPm0adO8Oc+dqd/CAAbDbbkk/7OzZWu3588+1EvQ99/hz2LTIeYaxnAOVKsHf\n/hZ2FAkpd0PDORfshVWTNqrZKlOBS/mcP/64Fto65hiYMQOOOCLph9y4Uet73XUXHHggfPIJnHSS\nf6+f8jnPQJbz9JYSq7eKyCkiMlZEfhKRrSISiXr8OW978du4sOI1xsSpRQut8Dl1aiCNjC+/hGOP\n1XGm/frBV1/528gwJmWtWaP/AVJQIqu3VhKRm0TkcxH5WURWFr8lGEd14CugB1BSYY93gb2BOt6t\nQ4LHMsYEpWFDXd+hUnInqq1bBzffDMcfr8W2vvhCL5UksZioMallxAj9D9C/f9IXHSyvRHo0BgI3\nAq8CtYCHgDeBrcCgRIJwzo13zg1wzo0BSprNvsE596tz7hfv9kcixzLJ069fv7BDyDqWc11r7Ygj\ntOjW3XfrmIyjjkre8SznwbOcx6FfP21dP/KIrnL86adhR/SXRBoalwLdnXMPApuBHOfclWhp8hP8\nDC7KaSKyXETmichjNr029dStWzfsELJONud8zRod/nHqqToL8Kuv4NZboUqV5B43m3MeFst5HCpX\nhltu0Vlde+wBp5yiFUbXrw87svKXIBeRtUBj59xiEVkGtHHO5YnIgcBM51ytCgUkshVo65wbW2zb\nxUAhsBCoDwxGV5Bt5mK8AStBbkxA3nlHv+Fvuy3Qw44fr9WZf/sNBg/WekZJvjpjTPrYsgWGDdP/\nlwceCC+9BFpSPC6hliD3/IiucwJQgC6mBnAssKGiAcXinHvNOfe2c2621wD5P+A44LRkHM8YU4Y/\n/9Rv+v/7Px3oGVB55N9+g8svh3PO0crMs2bp1FVrZBhTTKVK2psxYwbsuqteVglRIg2Nt4AzvZ+H\nA3eJyHzgReBZvwIrjXNuIbACaFDafq1btyYSiWx3a9asGbm525cCmThxIpFIZIfn9+zZk5EjR263\nLS8vj0gkwooVK7bbPnDgQIYMGbLdtsWLFxOJRJg3b/slYIYPH77DNcfCwkIikcgOFfBycnJiLpHc\nvn17ex/2PsJ5H1OnwpFH0vPZZxnZqZMuAuV90yfzfRxzTIQGDaYwdqwW3Zo4EaZOtc/D3oe9jxLf\nxzff0OXQQ3UFwRLeR05Ozl/fjXXq1CESidCnT58d3k9FJLR663YvINIMaAbMd879r8IBxbh0EmOf\n/YAfgPOcc2/HeNwunYRg3rx5HHzwwWGHkVUCzfnmzbqS5D336LoLL76os0qSbNky6NUL3nxTy4Y/\n9hj84x9lPy9Z7DwPnuU8WKlw6WQ7zrmpzrmHKtLIEJHqInKEiBzpbTrQu/9P77GhInK8iNQTkTPR\n6qT5wISKxm/8079//7BDyDqB5XzRImjeXMuIDxyoVbCS3MhwDp5/XquWT5kCr72mjY0wGxlg53kY\nLOfpLa7KoNEFtEpTWk9EKY4BPkBraDjgQW/7C2htjcOBy4DdgaVoA2OAt5qsSREjRowIO4SsE1jO\nCwvhjz90LumJJyb9cIsW6RCQiROhc2ddryRVqi/beR48y3kSOQdSUlUJf8Rbgjze9U0cCSwh75z7\niNJ7V84u72ua4NkUtOAFlvMmTeDbb7UaVhJt3aqXRm65RRc+e+cdaN06qYcsNzvPg2c5T6KbbtIB\no4MGJW2Rw7h+azjndorzZmO/jclUSW5kfPedXp257jrtxZg9O/UaGcZknL//He67D844A378MSmH\nSIm1Towx2WvTJv09d8QRsHw5fPihrsVWs2bYkRmTBW65Rf/Tff+9VhQd5/8yYnE1NETk+nhvvkdo\n0kb09CyTfL7mPKBaGMV99ZUuz3DbbXD99fDNN1rpM5XZeR48y3mSnXyy/mc84QRo00br+fso3gsy\n8U6qdcAjCcZi0lxhYWHYIWQd33I+ezZccgk88UQgy52uX6/rkgwZAo0bw/TpupJ8OrDzPHiW8wDs\nuSeMHasjr2++2deXrnAdjVRkdTSMKYecHLjySqhfH15/HQ46KKmHmzoVunaFggK4/Xbtua1aNamH\nNMaUQ96zz9K0WzdIhToa4qloEMaYEGzcCL17Q8eO0K4dTJuW1EbG2rW6YvxJJ+n4i5kzYcAAa2QY\nk3KOPLLsfcohoYaGiFwmIt8C64B1IvKNiHT2NTJjTPIsXQqnn66jLkeM0Cqf1aol7XCTJsFhh8FT\nT8GDD8Jnn8EhhyTtcMaYFFLuhoaI3Ag8DowDLvZu44EnRMTfAukmrUTX7zfJl1DOp0yBo4+GH37Q\nAlw9eyatYM/vv0O3bnDWWbD//lqKo0+f9F4Ezc7z4FnO01siPRrXAdc65252zo31bv3RCp426ySL\nde3aNewQsk5COZ86FQ4+GPLydJR5kuTmap2v//5XezImT9ZhIOnOzvPgWc7TWyINjX8An8XY/hnb\nlo83WWjQoEFhh5B1Esr5TTfptYy99vI9HtBaGBdfDOefrzNJ5syB7t2TXuU4MHaeB89ynt4SaWgs\nQC+XRGsPzK9YOCad2Qyf4CWUc5GklBp2Dl56SXsxPvgAXnkFxoyBfff1/VChsvM8eJbz9JbIb5uB\nwKsi0hz41Nt2EnAmsRsgxpgMt3gxXHMNvPuuTmIZNkwrGxtjTLl7NJxzbwDHAyuAtt5tBXCcc+4t\nf8MzxqSyrVt14sohh2hVz7Fj4eWXrZFhjNkmoemtzrkZzrlOzrmm3q2Tc26m38GZ9DJy5MiwQ8g6\nMXPunFb4/PPPpB57/nydIdujB3TooMVFzz03qYdMCXaeB89ynt7iXeukZry3ZAdsUldeXoULyJly\n2iHn69bBpZfCtdcmZXEkgM2b4f774fDDdbHHyZN1VkmtWkk5XMqx8zx4lvP0FlcJchHZiq5jUqZU\nWCreSpCbbJCfn09BQQENGjSgYcOG8MsvcN55ujjSCy/o1A+fffutlg/Py9Mqn3feCdWr+34YY0yI\n8vLyaNq0KfhUgjzewaCnF/t5f+A+4HlgqretGXA5cGtFAzLGlG7lypV07NiZCRO29Vh0O6k5T/74\nA5XWr9ciXMce6+sxN2yAe+/VW6NGWtnz+ON9PYQxJkPF1dBwzn1U9LOIDABudM7lFNtlrFeS/Crg\nBX9DNMYU17FjZyZNmgaMAprTnCe5/9N7WbJbdfafNQvq1fP1eNOna3XP776DW2/VJd133tnXQxhj\nMlgig0GbAV/G2P4lcFzFwjHGlCY/P58JE8axZcsjwKX8H1/xHkOZQROO/PNP5m/c6NuxCguhb184\n8UTYZReYMUMvlVgjwxhTHok0NJYA3WNsv9J7zGSpSCQSdggZr6CgwPupOQDvMZxh3EBrxvAHsGDB\nAl+O88EHugjaY4/Bfffpwq6HH+7LS6c9O8+DZzlPb4kU7OoDvCEi5wDTvW3HAQ2BC/wKzKSfXr16\nhR1Cxqv/12IhHwOXsoGbuJmW6GUUaNCgQYVe/48/oH9/nUVyyilagKtRowq9ZMax8zx4lvP0lkjB\nrnFoo2IssId3+x/QyHvMZKmWLVuGHULGa9SoEa1ataZSpevRxkVjYBSVKvWmVavWOvskQW+/rYW3\nXnlFezI+/NAaGbHYeR48y3l6S2jBA+fcj8BtPsdijIlDTs4oOnToxIQJnf/a1qJFa3JyRiX0eitW\nQO/e2sA4+2x48kmoW9evaI0x2c7/lZWMMUlVu3Ztxo9/h/nz57NgwYJtdTTKyTl49VW47jotJf7C\nC9C5c+assmqMSQ0JlSA3Jpbc3NywQ8hMs2bpIiJRGjZsyIYNGxJqZPz0E7Rtq6XDTztNl3K/7DJr\nZMTDzvPgWc7TmzU0jG9ycnLK3smUz2ef6ajMe+/Vboco5c25c/DMMzoW4/PP4Y034PXXYe+9/Qo4\n89l5HjzLeXqLqwR5urES5CYjvPsuXHCBVvkcO7bCi4l8/z1cdZWuTXLFFfDQQ1C7tj+hGmMyh98l\nyK1Hw5hUlJMDkQi0aAHjx1eokbFlCwwbpnUxFiyACRPgueeskWGMCUa5GxoisreIvCQiS0Vks4hs\nKX5LRpDGZJXHH9cVWDt2hDffhF13Tfil5syBk0+GG2/UMuKzZoHNFDTGBCmRWSfPA3WBu4BlxLmq\nqzEmDvffrxWzevfWaxs7JdbpuGkTDBkCd90FBxyg66ydfLLPsRpjTBwS+S12MnCpc+5x51yuc25M\n8ZvfAZr00aVLl7BDSH+FhXDHHfDww3E1MmLlfMYMOOYYGDRI1yr56itrZPjJzvPgWc7TWyI9GksA\nmwRndmDV+3wwcGC5di+e83Xr4N//hgce0PEYn38ONhbaf3aeB89ynt7KPetERFoCfYGrnXOLkhFU\nRdmsE5NtPvkErrwSFi2CAQP06kuVKmFHZYxJR37POkmkR+NVoBpQICKFwKbiDzrn9qhoUMaY+KxZ\nA7feCo8+qsu5jxkDBx8cdlTGGLNNIg2NG3yPwhhTbhMmaF2MFSvgP/+Bnj2hUqWwozLGmO2Vu6Hh\nnHshGYGY9DdlyhROtlGHZdu4UUt07rxzQk9fuVKnq77wAjRtOoUPPzyZAw7wOUZTIjvPg2c5T29x\nzToRkZrFfy7tlrxQTaobOnRo2CGkvg0b4KKLtE5GAt54A5o0gdxcGDkS9tlnqDUyAmbnefAs5+kt\n3h6NVSLyD+fcL8DvxK6dId5267zNUqNHjw47hNS2fr2WFJ88Gd56q1xP/fln6NVLGxrnnQePPQb7\n7AOXXGI5D5qd58GznKe3eBsaZwArvZ9PT1IsJs1Vq1Yt7BBS14YN0K4dfPCBrlsS53Q95+DFF6FP\nH6hcWZd1v+iibausWs6DZzkPnuU8vcXV0HDOfRTrZ2NMHDZs0J6MDz6A//1P1y+Jww8/wNVX66DP\nTp20hteeeyY5VmOM8ZktqmZMMm3cqF0Qkybp3NM4Ghlbt+p01UMPhdmz4e234aWXrJFhjElP1tAw\nvunXr1/YIaSenj1h4kRtZMRxueS77+DUU3U8RqdO2tBo06bk/S3nwbOcB89ynt6soWF8U7du3bBD\nSD19+2ojo1WrUnfbvFkXQTviCFi2TK+yPP441CxjHpflPHiW8+BZztNbuUuQpwMrQW7SyddfQ9eu\nuvjZjTfqeiU29s0YExa/S5An1KMhIpVFpIWIXC0iNbxt+4jIbhUNyJhssWGDLtR6zDE6lGPqVF0l\n3hoZxphMUu7KoCJSDxgP1AV2Bt4D1gA3e/ev8TNAYzLR1KnQrRssWAC3367rlVStGnZUxhjjv0R6\nNP4DfAnUBtYV2/4WcKYfQZn0NG/evLBDSHlr18INN8BJJ0GNGpCXpyvDJ9rIsJwHz3IePMt5ekuk\noXEKcLdzbmPU9kXAvhWOyKSt/v37hx1COJyDJUvK3G3SJJ2y+tRT8MAD8Nlner8isjbnIbKcB89y\nnt4SaWjsROwy4/uhl1BMlhoxYkTYIYRj8GBtMSxbFvPh33/XyyRnnQX77w/ffKODPv1YaTVrcx4i\ny3nwLOfpLZGGxkS2XyreeYNA/w2M8yUqk5aycgrao4/CbbfBTTfBP/6xw8NjxugiaP/9Lzz5pC5z\n0qCBf4fPypyHzHIePMt5ekukodEXOElE5gC7AK+w7bLJzf6FZkyKe+klrax14406orOYX36B9u2h\nbVto2lQLb111FexklWuMMVmm3L/2nHM/AkcA9wAPAzOBW4CjvNVdy01EThGRsSLyk4hsFZFIjH3u\nFJGlIlIoIu+JiI9/FxpTTrm50KWLXhN54IG/VjlzDl5+WXsxJk/Wn8eOhf32CzleY4wJSUJ/Xznn\nNjvnXnbO9XfO9XDOPeOcW1f2M0tUHfgK6EGMJehF5GagF3AVcBywFpggIjYhMIUMGTIk7BCCMXmy\ndle0a6fXQ7xGxpIlcO65Wjr8rLNg7lzo2HHbSqvJkDU5TyGW8+BZztNbInU0bgV+ds49F7W9K/B3\n51y5zwjn3Hi0NgciMX8t9wbucs697e1zGbAcaAu8Vt7jmeQoLCwMO4TkmzcPzjsPzjgDRo2CSpXY\nuhWefhr69dMpq2PGQGSHPrnkyIqcpxjLefAs5+mt3CXIRWQR0N45Nz1q+/HAaOfcARUKSGQr0NY5\nN9a7fwBQABzpnPum2H4fAjOdc31ivIaVIDfJsWULDBsG114L1aqxYAF07w4ffghXXqmVPXffPewg\njTEmcalQgrwOEGssxq/AjsPuK64OejlledT25d5jxgSnUiXo25ctO1fjgQfgsMPghx+0RsbTT1sj\nwxhjoiXS0FgCnBRj+0nA0oqFY0zqmzULmjWD/v3hmmvg22/hTKuJa4wxMSXS0HgaGCYiXUSknnfr\nis5Aedrf8AD4GRBg76jte3uPlah169ZEIpHtbs2aNSM3N3e7/SZOnEgkxkX1nj17MnLkyO225eXl\nEYlEWLFixXbbBw4cuMOApcWLFxOJRHYonzt8+HD69eu33bbCwkIikQhTpkzZbntOTg5dunTZIbb2\n7dun3PtYsWJFRrwPiP15bNwIF1yQw+GHd+HPP7Wy58MPQ/Xq4b2P4vtn6nmVau9jwIABGfE+0unz\nmDx5cka8j1T8PHJycv76bqxTpw6RSIQ+fXYYkVAxzrly3dAv/SHoOidbvNtaYEB5X6uE198KRKK2\nLQX6FLtf0zv+RSW8xtGAmzFjhjPBOffcc8MOIWk+/9y5Qw91rnJl526/3bn168OOSGVyzlOV5Tx4\nlvNgzZgxw6FDFo52Pnyvl3vWiXPOATeLyF1AY+8Lf75zbkOijR0RqQ408BoxAAeKyBHASufcEmAY\ncLuILECLg90F/AiMSfSYxn+DBg0KOwT/eIOkC9cJAwZoz8WRR8KXX8IRR4QcWzEZlfM0YTkPnuU8\nvZW7oVHEOfcn8IVPcRwDfIC2oBzwoLf9BaCrc26oiFQDngR2Bz4BznE7LuxmQpRRM3wGDmTpzJ85\nde6TLPlRuPde6NsXKif8PyY5MirnacJyHjzLeXqL69emiLwJXOGcW+39XCLnXLvyBuGc+4gyxos4\n5wYBg8r72saU17phT7LrXXcxjCHUOVl4+x046KCwozLGmPQU799nf7CtYucfSYrFmNB9OWAsR93V\ng8erXEe9h/pxXw9bn8QYYyoirl+hzrkuzrk1XtXOgUAPb9sOt+SGa1JZ9AjrdLJiBQw8expN7rqE\nqXu1pfW8h+nZS1K+kZHOOU9XlvPgWc7TW3l/jQqwALAloswO8vIqXEAucM7Ba69Bm4MWcN3Ec1nT\nqCknLRxFvQMrhR1aXNIx5+nOch48y3l6S6QE+Wygm3NuWnJCqjgrQW7isXQp9OgBU8as4Nvqzfh7\nnZ2oPP0z+Nvfdtg3Pz+fgoICGjRoQMOGDUOI1hhjgpEKJchvAe4XkUMrenBjwuAcPPusLuU+bRq8\nNvh7/rFfJSpPfHeHRsbKlSs5++w2HHTQQbRu3ZpGjRpx9tltWLVqVUjRG2NMekmkofEiulT71yKy\nTkRWFr/5HJ8xvlq4EFq2hG7doG1bmDMHzrjlOJg9Gw48cIf9O3bszKRJ04BRwGJgFJMmTaNDh05B\nh26MMWkpkaoAfdg2A8WYtLBlCzz6KNx6K+y5J4wfD61aFduh0o5jMvLz85kwYRzayLjU23opW7Y4\nJkzozPz58+0yijHGlKHcPRrOueedcy+UdEtGkCY9xKr3nwrmzoVTToHevaFLF10UbbtGRgkKCgq8\nn5pHPXIqAAsWLPA1zkSkas4zmeU8eJbz9BZ3Q0NEdhKR/iLyqYh8ISL3iciuyQzOpJdevXqFHcJ2\nNm2Ce+7R0uG//QYffwwjRkCNGvE9v379+t5PH0c98hEADRo08C3WRKVazrOB5Tx4lvP0Vp4ejduA\ne4E1wE9Ab+DRZARl0lPLli3DDuEvM2fCccfBwIFw443w1Vfaq1EejRo1olWr1lSqdD16+WQJMIpK\nlXrTqlXrlLhskko5zxaW8+BZztNbeRoal6GFus52zrUFzgUuFZEUL2lkssn69fCvf8Gxx+rskunT\nYfBg2HVXtCrXd9+V6/VyckbRosUJQGegLtCZFi1OICdnVBKiN8aYzFOewaB1gXeL7jjnJomIA/ZB\nV1I1JlRTpuhskkWLYNAguPlmqFLFe3DDBjj/fPj1V51hEmPwZyy1a9dm/Ph3mD9/PgsWLLA6GsYY\nU07l6Y2oDKyP2rYJqBJjX5OFcnNzQznun3/CdddB8+awxx562eT224s1MpyD7t3hiy/guefibmQU\n17BhQ84555yUa2SElfNsZjkPnuU8vZWnoSHA8yLyZtEN2AV4ImqbyVI5OTmBH3PiRDj0UC3A9fDD\n2qvRpEnUToMHw0svaSOjWbPAY0ymMHKe7SznwbOcp7e4S5CLyHPx7JcKC6tZCfLMt2qVDvJ8/nk4\n80x46qmY9bbgrbegXTsdFTpoUMBRGmNM+vG7BHncYzRSoQFhDMCbb0LPnrBuHTz9tI7LEImx4zff\nQOfOcOGFMGBA4HEaY4xJrAS5MaFYvhwuugguuECnrs6ZA1deWUIjY8UKiESgUSPt9kj19d6NMSZD\n2W9fk/Kc0yEWTZrARx/B6NGQmwv77FPKk2rWhIsvhjFjoHr1wGI1xhizPWtoGN906eL/1bXFi6F1\na2n46EgAABqtSURBVLjsMjj7bO3FaN++hF6M4qpWhaFD4Z//9D2mVJKMnJvSWc6DZzlPb9bQML7x\ns3rf1q3w2GNwyCHw7bfw9tvw8su6IJrZxiomBs9yHjzLeXqLe9ZJOrFZJ+ktP1/HXnzyCVx9NQwZ\nArVqhR2VMcZkB79nnViPhkkZmzfr1Y4jjoClS+H99+GJJ6yRYYwx6cwaGiYlfP01HH883Hor9Oql\nM1NPPz3sqIwxxlSUNTSMb6ZMmVLu52zYAHfcAcccAxs3wtSpcP/9UK1anC9QNCVl8+ZyHzsTJJJz\nUzGW8+BZztObNTSMb4YOHVqu/adOhaOO0jEYt90GM2ZofYxyGTxYp6Rk6S+i8ubcVJzlPHiW8/Rm\nDQ3jm9GjR8e139q1cMMNcNJJsNtukJen1cGrVi3nAceN09XT7rgDTjutvOFmhHhzbvxjOQ+e5Ty9\nlWeZeGNKVS2O6x2TJ+tCqsuW6SWSG25IaDFVKCiASy+FNm2yeg2TeHJu/GU5D57lPL1Zj4YJxO+/\nawOjRQuoW1drY/Ttm2AjY+1aOP98Larx0ktWXtwYY1KY9WiYpBs7Fq69Ftas0emq3buX3jbIz8+n\noKCABg0a0LBhw+0fdA6uukp7NKZPh913T27wxhhjKsT+FDS+6dev33b3f/0VLrkEzjsPjjwSZs/W\nAlwlNTJWrlzJ2We34aCDDqJ169Y0atSIs89uw6pVq7btNHw4vPIKPPssHHpoEt9NeojOuUk+y3nw\nLOfpzRoaxjd169YFtNPhlVegcWOYNAlGjdIS4mUtO9KxY2cmTZoGjAIWA6OYNGkaHTp02rbTaadp\nVa/27ZP0LtJLUc5NcCznwbOcpzcrQW589eOPcM018M472hZ45BHYa6+yn5efn89BBx2ENjIuLfbI\nKKAz+fn5O15GMcYY4zsrQW5S0tat8NRTughaXp4u4z56dHyNDICCggLvp+ZRj5wKwIIFC3yL1Rhj\nTHCsoWEqrKBAZ5NcfTVcdJEu5X7eeeV7jfr163s/fRz1yEcANGjQoMJxGmOMCZ41NEzCtmyBhx6C\nww6DhQth5Mh5PPNMYhNBGjVqRKtWralU6Xr0cskSYBSVKvWmVavWdtmkBPPmzQs7hKxjOQ+e5Ty9\nWUPDJGTWLDjxRLjpJp1tOmsW5Ob2r9Br5uSMokWLE4DOQF2gMy1anEBOzig/Qs5I/ftXLOem/Czn\nwbOcpzero2HKZeNGuO8+uPtuqF9flxg58UR9bMSIERV67dq1azN+/DvMnz+fhbNmcbBz1G3Xzoeo\nM1dFc27Kz3IePMt5erMeDRO3L77QVVbvugv694eZM7c1MsC/KWgNGzSg5RtvUPfKK7XKlymRTfsL\nnuU8eJbz9GYNDVOmdeu0YXHCCVClijY47r4bdtklSQd88kl4+WV47DGoUSNJBzHGGBMEu3RiSvXx\nx9CtGyxZAvfco2MyKifzrMnLg969oUcPLStqjDEmrVmPholp9Wpdn+TUU2HvveHrr+GWW0pvZAwZ\nMqRiB/3jD50fe9hhOp3FlKnCOTflZjkPnuU8vVmPhtnBuHFa3XPlSl1apEeP+BZILSwsTPygzmnX\nyW+/wXvvwc47J/5aWaRCOTcJsZwHz3Ke3qwEufnLb7/BDTfo2iQtW2qlz3r1Ajr48OFw/fXw5pu6\nBLwxxphQ+F2C3Ho0DM7B669Dr16waRM89xxcfjmIBBjE+vXQt681MowxJsNYQyPLLVuml0Zyc6Fd\nO3j0UahTJ4RAbBloY4zJSDYYNEs5pz0XTZrA1Knw3//CG29UrJGxYsUK/wI0cbGcB89yHjzLeXqz\nhkYWWrQIWrWCrl0hEtFF0C64oOKv27Vr14q/iCkXy3nwLOfBs5ynN2toZJGtW+GRR+DQQ2HePJ1d\n8sILsMce/rz+oEGD/HkhEzfLefAs58GznKc3a2hkiblz4ZRTtBbWFVfA7Nlwzjn+HsNm+ATPch48\ny3nwLOfpzRoaGW7TJrj3XjjySPj1V630OWJEyJW9N24M8eDGGGOCZA2NDDZzJhx3HNxxh9bH+Ppr\n7dUI1Ysv6spsq1eHHIgxxpggpEVDQ0QGisjWqNucsONKVevXw7/+Bcceq+Mypk+HIUNg112Te9yR\nI0eWvkN+vs6lPfpoqFkzucFkiTJzbnxnOQ+e5Ty9pUVDwzML2Buo491ODjec1PTZZ3DUUfDggzBo\n0Lal3YOQl1dKAbkNG6BDB9h3X712Y3xRas5NUljOg2c5T2/pVLBrs3Pu17CDSFV//qm9GCNG6OWS\nvDw45JBgY3j00UdLfvC22+Dbb2HaNNhtt+CCynCl5twkheU8eJbz9JZOPRoNReQnESkQkVEi8s+w\nA0oV772nU1afeUZ7Mj79NPhGRqkmTNDABg/WyybGGGOyRro0NKYBVwCtgGuAA4CPRaR6mEGFbdUq\nLbrVsiXUrw+zZkGfPlCpUtiRFfPLL7pwSsuWGpwxxpiskhaXTpxzE4rdnSUinwM/ABcDz4UTVbje\nekvHVRYW6iqrV14Z8CJo8RoyROudv/BCfGvNG2OMyShp+ZvfOfcHkA80KG2/1q1bE4lEtrs1a9aM\n3Nzc7fabOHEikUhkh+f37Nlzh9HOeXl5RCKRHWrvDxw4kCFDhmy3bfHixUQiEebNm7fd9uHDh9Mv\nahGxwsJCIpEIU6ZM2W57Tk4OXbp0+ev+8uVw8cXQrl179tsvlzlzoHt3bWSE/T4ikciO72PwYHj/\nfXI++GC791Gkffv2af15hP0+ir9+Or+P4lL9fTRp0iQj3kc6fR7NmzfPiPeRip9HTk7OX9+NderU\nIRKJ0Mfn3mdxzvn6gkEQkd2AxcAA59wOUxhE5GhgxowZMzKmopxz8PLLWtlzp51g+HBo3z61ejEm\nTpxIy5Ytww7j/9u79yApqzOP499nFWUR0cSwuIb1gsQSVFTQVYQYJZSorOAtEZSgYoJG0U0kom6x\n3KSigxqMWlrEC0azIN4Xo4BBrRCMgs54QyCwCCJy8QKCxYByOfvHeSdp27ky3ef0+/bvU9VVTPf7\nnn764Z15nz7vec8pK8p5eMp5eMp5WFVVVXTr1g2gm3Ou2bf8pKJHw8xuNbOTzewgMzsJeBrYBkyN\nHFoQK1dC377wk5/4xdAWLoQBA0qryAD0hyAC5Tw85Tw85TzdUjFGA2gPTAH2Az4B5gInOuc+ixpV\nke3cCZMmwYgRsM8+MH06nHVW7KhEREQaLxWFhnNuYOwYQlu61A/wnDMHhg6FCRN8sSEiIpImqbh0\nUk62b4dbb4UuXWDVKnjxRd+rkYYiI3+QlBSfch6ech6ecp5uKjRKyDvvQPfucMMN/tbVd9+FXr1i\nR9VIzjH15pv9crESzNSpZTFMqaQo5+Ep5+mmQqMEfPkljBoF3brBli1+vZLbb4dWrWJH1gSTJjFt\n/nz4y19iR1JWpk2bFjuEsqOch6ecp1sqxmhk2bx5fnbPJUv8ciA33gh77hk7qiZasgSuvRauuCJF\nXTAiIhKCejQiqa725+bu3X3PRWWlX201dUXG9u3+vtv27eG222JHIyIiJUY9GhG8/LK/o2T1an83\nyS9+Abun9X/i17/2VdIrr8BeZb30jIiI1EI9GgFt3AiXX+6vLrRv7wd//upXKS4yXn8dxo3z13xO\nOKHWqW6luJTz8JTz8JTzdEvrKS51/vhHP4Rh0ya4914/N0aq1xirroZBg+DYY2HkSECz98WgnIen\nnIennKdbmk91qfDJJ3DhhX5Gzy5d4L33fMGR6iIDYO1aaNMGHnkEWrQAYODAsptXLTrlPDzlPDzl\nPN3Uo1EkzsG0aXD11X4q8UcegYsuKr31SXZZhw4wf36GPpCIiBRD2r9Xl6SPPoL+/WHgQDj1VL8I\n2qBBGTwnZ+4DiYhIoanQKCDn4L77oHNnP07yqafgscegXbvYkYUxd+7c2CGUHeU8POU8POU83VRo\nFMj770Pv3n6Q53nn+V6Mc86JHVVYEyZMiB1C2VHOw1POw1PO002FRjPt2AF33AFHHQXLlsGsWfDg\ng/Ctb8WOLLxHH300dghlRzkPTzkPTzlPNxUazbBwIfTs6Wf4vOwyWLAAyvkurFapWpwlG5Tz8JTz\n8JTzdFOhsQu++gpuuslPIbFhg19H7M47oXXr2JEVyZQpUFHhB6GIiIg0gQqNJnrjDTj+eBg7FoYP\nh7fegh49YkdVRKtW/WPNet1lIiIiTaRCo5G2bIERI+CEE/xkW6+/7pf5aNkydmRF5Bz87Gd+DZO7\n7mpw8+uuuy5AUJJLOQ9POQ9POU83TdjVCHPm+EXQVq6E8eP9+iTJZJjZNnkyzJwJzz3XqNGtBx54\nYICgJJdyHp5yHp5ynm7mMnjd3cy6ApWVlZV07dp1l9vZtAluuMGvTdKjB9x/Pxx+eOHiLGkffghH\nHgnnnusLDhERKQtVVVV069YNoJtzrqq57alHow4zZviVVtev91cNrrwyA+uTNFbNJZO994aJE2NH\nIyIiKVYup85G++wzGDwYzjzT914sWADDhpVRkQF+IpBZs/w0p/vuGzsaERFJsXI6fdbLOXjiCT99\n+LPP/uNce/DBsSOLoFMnGDUKzjijSbstXry4SAFJXZTz8JTz8JTzdFOhAaxZ46cN/9GP/FiMhQvh\n0kvL+G7Ok07y9+820YgRI4oQjNRHOQ9POQ9POU+3sh6j4Rw89JCf2XOPPeDxx33BUbYFRjPdfffd\nsUMoO8p5eMp5eMp5upVtj8aKFXD66TBkCJx1lu/FOP98FRnNoVvQwlPOw1POw1PO063sCo2dO/1d\nJEceCYsWwfPPw8MPw377xY5MREQke8qq0Fi8GE4+Ga65xt9ZsmBBk8c7ioiISBOURaGxbRvcfDMc\ncwysWwd//jPccw+0aRM7smypqKiIHULZUc7DU87DU87TLfOFxptv+vVJRo70PRnvvON7NSRxySV+\nytMCqK6uLkg70njKeXjKeXjKebplegryIUMq+f3vu9K5s58X47jjYkdWYp580o+AnToVBgyIHY2I\niJSAQk9BnukejYcf9vNOvfGGioxvWL8erroK+veHCy6IHY2IiGRUpufRmDLFT8IltRg+HLZu9YNV\ndE+viIgUSaZ7NA49NHYEJWr2bD9T2W23wQEHFKzZTz/9tGBtSeMo5+Ep5+Ep5+mW6UJDarF5Mwwd\nCqeeCpddVtCmhwwZUtD2pGHKeXjKeXjKebpl+tKJ1GLUKL+4ywsvFPySyZgxYwranjRMOQ9POQ9P\nOU83FRrlpm9fOOII6Nix4E137dq14G1K/ZTz8JTz8JTzdFOhUW569YodgYiIlBGN0RAREZGiUaEh\nBfPAAw/EDqHsKOfhKefhKefppkJDCqaqqtkTyEkTKefhKefhKefplukpyCsrKzWISEREpAk0BbmI\niIikhgqNLLviCj8DqIiISCQqNLJq5kyYNAl22y12JCIiUsZUaGTR5s3w859D794waFCwt+3Xr1+w\n9xJPOQ9POQ9POU83TdiVRWPGwNq1fvG0gCuzDhs2LNh7iaech6ech6ecp5sKjax5+22YOBHGjw++\nfO1pp50W9P1EOY9BOQ9POU83XTrJkh074PLL4fDDYfjw2NGIiIioRyNTfvc7mDcP5s6FFi1iRyMi\nIqIejUzp0MEvA9+jR5S3f+aZZ6K8bzlTzsNTzsNTztMtVYWGmV1lZsvNbIuZvWZmx8eOqaT06QNj\nx0Z7+4qKimjvXa6U8/CU8/CU83RLTaFhZhcAtwOjgWOBt4FZZvadqIHJ37Vt2zZ2CGVHOQ9POQ9P\nOU+31BQawC+BSc65h51zi4ErgGpgSNywREREpC6pKDTMrAXQDXix5jnnV4ObDXSPFZeIiIjULxWF\nBvAdYDdgXd7z64D9w4cjIiIijZHV21tbAixatCh2HGVl/vz5VFU1e0VhaQLlPDzlPDzlPKycc2fL\nQrRn/gpEaUsunVQD5znnpuc8/xCwj3PunLztLwT+J2iQIiIi2XKRc25KcxtJRY+Gc26bmVUCPwSm\nA5iZJT/fWcsus4CLgBXA1kBhioiIZEFL4GD8ubTZUtGjAWBmPwYewt9tMh9/F8r5wOHOuU8ihiYi\nIiJ1SEWPBoBz7rFkzoxxQDvgLaCPigwREZHSlZoeDREREUmftNzeKiIiIimkQkNERESKJpOFhhZf\nC8fMRpvZzrzHwthxZYmZfd/MppvZR0l++9WyzTgzW21m1Wb2JzPrGCPWrGgo52Y2uZbj/vlY8aad\nmd1oZvPNbJOZrTOzp83ssFq203FeII3JeaGO88wVGlp8LYoF+AG6+yePnnHDyZy98IOfrwS+MajK\nzK4HhgFDgX8HNuOP+T1CBpkx9eY8MYOvH/cDw4SWSd8H7gJOAHoDLYAXzOyfazbQcV5wDeY80ezj\nPHODQc3sNWCec+4/k58N+BC40zk3IWpwGWRmo4H+zrmusWMpB2a2Ezg7b+K61cCtzrmJyc9t8NPz\nX+yceyxOpNlRR84n4ycLPDdeZNmVfDH8GDjZOTc3eU7HeRHVkfOCHOeZ6tHQ4mvRfC/pYl5mZn8w\ns3+LHVC5MLND8N8yco/5TcA8dMwX2ylJl/NiM7vHzL4dO6AM2Rffk7QedJwH8rWc52j2cZ6pQgMt\nvhbDa8AlQB/8ZGqHAHPMbK+YQZWR/fF/HHTMhzUDGAz0AkYAPwCeT3pQpRmSHN4BzHXO1Yz30nFe\nRHXkHAp0nKdmwi4pTc653ClqF5jZfOAD4MfA5DhRiRRXXlf9e2b2LrAMOAV4OUpQ2XEP0BnoETuQ\nMlJrzgt1nGetR+NTYAd+4EqudsDa8OGUH+fcRmAJoNHgYawFDB3zUTnnluP//ui4bwYzuxs4EzjF\nObcm5yUd50VST86/YVeP80wVGs65bUDN4mvA1xZf+2usuMqJmbXGH4T1HrBSGMkv/lq+fsy3wY8k\n1zEfiJm1B/ZDx/0uS054/YFTnXMrc1/TcV4c9eW8ju136TjP4qWT3wAPJau91iy+1gq/IJsUmJnd\nCjyLv1zyXWAssA2YGjOuLEnGu3TEf6MD6GBmRwPrnXMf4q+tjjSz/8OvWHwTsAr43wjhZkJ9OU8e\no4En8Se/jkAFvievIKtdlhszuwd/22Q/YLOZ1fRcbHTO1azAreO8gBrKefI7UJjj3DmXuQf+3vcV\nwBbgVeC42DFl9YEvKFYluV4JTAEOiR1Xlh74AVg78ZcFcx8P5mwzBlgNVCd/BDrGjjvNj/pyjl9C\ne2byx3cr8D5wL9A2dtxpfdSR6x3A4LztdJwHynkhj/PMzaMhIiIipSNTYzRERESktKjQEBERkaJR\noSEiIiJFo0JDREREikaFhoiIiBSNCg0REREpGhUaIiIiUjQqNERERKRoVGiIlDgz22lm/WLHUWxm\ndrGZrY/dRl57ByX571KoNkXKjQoNkQjMbHJyAtthZl+Z2Voze8HMLk0WAsy1PzAjRpy5zGy0mb1Z\nxLd4FDisBNrIp+mTRZpBhYZIPDPwRcRBwOnAS8BvgWfN7O+/m865j51fmbgUNPuka2Ytam3YuS+d\nc582p+1CtFGL/MJPRJpAhYZIPF865z5xzq1xzr3lnLsFv2TzmcAlNRvlXzoxs1vM7G9mttnMlpnZ\nODPbLef10Wb2ZtI78oGZfWFmd5vZP5nZCDNbY2brzOy/coMxs33M7H4z+9jMNprZizWXDMzsYvxK\njkfn9MQMrmO/2bmXGnLiuczM3scvwPcNyWWPDbXsN8jMlpvZ52Y2NVlVsla70oZ5I8xsqZltNbMV\nZnZjXtOHmtlLSc7fMrMT8963p5nNMbPqJOe/NbNWOa9faWZLzGxL0nv1WF2fQSRrVGiIlBDn3MvA\n28C59Wy2CRgMdAKuAX4K/DJvm0PxvSR9gAHJNs8BBwAnA9cD483s+Jx9ngD2S/bpClQCs81sX2Aa\ncDvwHtAO+Nfkudr2q8rZr0bH5DOdAxxTXwpq+Rw1xVdf/KqqN9Sz/660cQswAhiLz+kF+BUrc40H\nJgBH45fJnlLT62Rmh+J7px4Hjkz27wHclbx+HL6naiT+sk4fYE4Dn0EkO2IvVauHHuX4ACYDT9Xx\n2lRgQc7PO4F+9bQ1HJif8/No4AugVc5zM4BlefstAkYk/+4JbABa5G2zFPhpTrtVea/3aOR+W4Fv\nN5CTi4H1DXyOCuCvhWoDaI3vYbm0jvYOSvJ/Sc5znfDLaR+W/HwfcG/efj2B7cAe+OJqA7BX7ONO\nDz1iPHZvUlUiIiEY9YyFMLMLgKvx39RbA7sDG/M2W+Gcq875eR3+xEfec/+S/LsLsDewPm8sasvk\nfepydCP3+8A5tyt3g+R/jjU5MReijU74YuClBtp4N29/S9pYgs/BUWY2KGebmmQcAvwJWAksN7OZ\nwEzgaedcrZeQRLJGhYZI6ekELK/tBTPrDvwB+G/gBXyBMRC4Nm/T/MGjro7nai6ftgZW4y8r5A9+\n/LyeWBu73+Z62qhPfTEXoo3Gnuxz26gpAnNzNwl/eSQ/Byudc9vN7FjgFOA0/CWaMWZ2nHNuUyPf\nXyS1VGiIlBAz6wUchR8PUZvu+G/ot+Tsc3AB3roKfwfMDufcyjq2+QrYLe+5xuxXypbiL+v8EHiw\njm0autOmCujsnKu1OARwzu3E95q8ZGbj8EVYL+CZJkcskjIqNETi2dPM2uFP3u2AM/CDFKcDj9Sx\nz1LgwOTyyevAfwBnNzcQ59xsM3sVeMbMrsdfEvgufgDlU865KmAFcIiZHQ2sAr5o5H4lyzn3pZlV\nABPMbBvwCtAWOMI5V1N4NHR7awXwqpndBdyP7705AujtnLvazPoCHfADQDfgB6Qa8LeCfyCREqS7\nTkTiOR1/2WE5frDmD4BhzrmznXO536L//m/n3LPARPwdDW8CJwLjdvH987+pn4k/GT6IPwlOAQ7E\nj+UAeBI/vuBl4GP83SyN2a+kOefG4XuQxgIL8ZN+tc3dpLbdcvZ/F/9/9z18HqqAMcBHySaf4++4\neTFpfygwwDm3qJCfQ6RU2df/nomIiIgUjno0REREpGhUaIiIiEjRqNAQERGRolGhISIiIkWjQkNE\nRESKRoWGiIiIFI0KDRERESkaFRoiIiJSNCo0REREpGhUaIiIiEjRqNAQERGRolGhISIiIkXz/4EF\nXhYV1DrOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb12563ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx_quadratic = quadratic_featurizer.transform(xx.reshape(xx.\n",
    "shape[0], 1))\n",
    "plt.plot(xx, yy)\n",
    "plt.plot(xx, regressor_quadratic.predict(xx_quadratic), c='r',linestyle='--')\n",
    "plt.title('Pizza price regressed on diameter')\n",
    "plt.xlabel('Diameter in inches')\n",
    "plt.ylabel('Price in dollars')\n",
    "plt.axis([0, 25, 0, 25])\n",
    "plt.grid(True)\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAGHCAYAAAD2qfsmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXhYV1DrOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb125547c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx_quadratic = quadratic_featurizer.transform(xx.reshape(xx.\n",
    "shape[0], 1))\n",
    "plt.plot(xx, yy)\n",
    "plt.plot(xx, regressor_quadratic.predict(xx_quadratic), c='r',linestyle='--')\n",
    "plt.title('Pizza price regressed on diameter')\n",
    "plt.xlabel('Diameter in inches')\n",
    "plt.ylabel('Price in dollars')\n",
    "plt.axis([0, 25, 0, 25])\n",
    "plt.grid(True)\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>799.595960</td>\n",
       "      <td>0.017412</td>\n",
       "      <td>0.028539</td>\n",
       "      <td>0.031988</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>0.038579</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036501</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.015944</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.019420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>452.477568</td>\n",
       "      <td>0.019739</td>\n",
       "      <td>0.038855</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.028411</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.052030</td>\n",
       "      <td>0.017511</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063403</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.060151</td>\n",
       "      <td>0.011415</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.022768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>415.250000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>802.500000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1195.500000</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.056153</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1584.000000</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.205080</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.310550</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429690</td>\n",
       "      <td>0.202150</td>\n",
       "      <td>0.172850</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.578130</td>\n",
       "      <td>0.151370</td>\n",
       "      <td>0.375980</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     margin1     margin2     margin3     margin4  \\\n",
       "count   990.000000  990.000000  990.000000  990.000000  990.000000   \n",
       "mean    799.595960    0.017412    0.028539    0.031988    0.023280   \n",
       "std     452.477568    0.019739    0.038855    0.025847    0.028411   \n",
       "min       1.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     415.250000    0.001953    0.001953    0.013672    0.005859   \n",
       "50%     802.500000    0.009766    0.011719    0.025391    0.013672   \n",
       "75%    1195.500000    0.025391    0.041016    0.044922    0.029297   \n",
       "max    1584.000000    0.087891    0.205080    0.156250    0.169920   \n",
       "\n",
       "          margin5     margin6     margin7     margin8     margin9     ...      \\\n",
       "count  990.000000  990.000000  990.000000  990.000000  990.000000     ...       \n",
       "mean     0.014264    0.038579    0.019202    0.001083    0.007167     ...       \n",
       "std      0.018390    0.052030    0.017511    0.002743    0.008933     ...       \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000     ...       \n",
       "25%      0.001953    0.000000    0.005859    0.000000    0.001953     ...       \n",
       "50%      0.007812    0.015625    0.015625    0.000000    0.005859     ...       \n",
       "75%      0.017578    0.056153    0.029297    0.000000    0.007812     ...       \n",
       "max      0.111330    0.310550    0.091797    0.031250    0.076172     ...       \n",
       "\n",
       "        texture55   texture56   texture57   texture58   texture59   texture60  \\\n",
       "count  990.000000  990.000000  990.000000  990.000000  990.000000  990.000000   \n",
       "mean     0.036501    0.005024    0.015944    0.011586    0.016108    0.014017   \n",
       "std      0.063403    0.019321    0.023214    0.025040    0.015335    0.060151   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000977    0.000000    0.004883    0.000000   \n",
       "50%      0.004883    0.000000    0.005859    0.000977    0.012695    0.000000   \n",
       "75%      0.043701    0.000000    0.022217    0.009766    0.021484    0.000000   \n",
       "max      0.429690    0.202150    0.172850    0.200200    0.106450    0.578130   \n",
       "\n",
       "        texture61   texture62   texture63   texture64  \n",
       "count  990.000000  990.000000  990.000000  990.000000  \n",
       "mean     0.002688    0.020291    0.008989    0.019420  \n",
       "std      0.011415    0.039040    0.013791    0.022768  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    0.000977  \n",
       "50%      0.000000    0.003906    0.002930    0.011719  \n",
       "75%      0.000000    0.023438    0.012695    0.029297  \n",
       "max      0.151370    0.375980    0.086914    0.141600  \n",
       "\n",
       "[8 rows x 193 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                species   margin1   margin2   margin3   margin4  \\\n",
       "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "\n",
       "    margin5   margin6   margin7  margin8    ...      texture55  texture56  \\\n",
       "0  0.011719  0.009766  0.027344      0.0    ...       0.007812   0.000000   \n",
       "1  0.025391  0.001953  0.019531      0.0    ...       0.000977   0.000000   \n",
       "2  0.003906  0.005859  0.068359      0.0    ...       0.154300   0.000000   \n",
       "3  0.021484  0.019531  0.023438      0.0    ...       0.000000   0.000977   \n",
       "4  0.013672  0.015625  0.005859      0.0    ...       0.096680   0.000000   \n",
       "\n",
       "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "   texture63  texture64  \n",
       "0   0.000000   0.025391  \n",
       "1   0.039062   0.022461  \n",
       "2   0.020508   0.002930  \n",
       "3   0.000000   0.047852  \n",
       "4   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = preprocess_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  species   margin1   margin2   margin3   margin4   margin5   margin6  \\\n",
       "0   1        3  0.007812  0.023438  0.023438  0.003906  0.011719  0.009766   \n",
       "1   2       49  0.005859  0.000000  0.031250  0.015625  0.025391  0.001953   \n",
       "2   3       65  0.005859  0.009766  0.019531  0.007812  0.003906  0.005859   \n",
       "3   5       94  0.000000  0.003906  0.023438  0.005859  0.021484  0.019531   \n",
       "4   6       84  0.005859  0.003906  0.048828  0.009766  0.013672  0.015625   \n",
       "\n",
       "    margin7  margin8    ...      texture55  texture56  texture57  texture58  \\\n",
       "0  0.027344      0.0    ...       0.007812   0.000000   0.002930   0.002930   \n",
       "1  0.019531      0.0    ...       0.000977   0.000000   0.000000   0.000977   \n",
       "2  0.068359      0.0    ...       0.154300   0.000000   0.005859   0.000977   \n",
       "3  0.023438      0.0    ...       0.000000   0.000977   0.000000   0.000000   \n",
       "4  0.005859      0.0    ...       0.096680   0.000000   0.021484   0.000000   \n",
       "\n",
       "   texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "0   0.035156        0.0        0.0   0.004883   0.000000   0.025391  \n",
       "1   0.023438        0.0        0.0   0.000977   0.039062   0.022461  \n",
       "2   0.007812        0.0        0.0   0.000000   0.020508   0.002930  \n",
       "3   0.020508        0.0        0.0   0.017578   0.000000   0.047852  \n",
       "4   0.000000        0.0        0.0   0.000000   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.192380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>98</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>74</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22</td>\n",
       "      <td>58</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>75</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.333010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>83</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>84</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>66</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43</td>\n",
       "      <td>73</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>1541</td>\n",
       "      <td>85</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.055664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>1543</td>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>1544</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>1545</td>\n",
       "      <td>45</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.121090</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>1547</td>\n",
       "      <td>48</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>1548</td>\n",
       "      <td>86</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>1549</td>\n",
       "      <td>81</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>1550</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>1551</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>1552</td>\n",
       "      <td>77</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147460</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1554</td>\n",
       "      <td>56</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>1555</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>1556</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>1557</td>\n",
       "      <td>85</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>1559</td>\n",
       "      <td>70</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>1561</td>\n",
       "      <td>88</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1562</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>1563</td>\n",
       "      <td>75</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>1566</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1568</td>\n",
       "      <td>86</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>1569</td>\n",
       "      <td>81</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>1570</td>\n",
       "      <td>97</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>1571</td>\n",
       "      <td>70</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1572</td>\n",
       "      <td>72</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>1574</td>\n",
       "      <td>34</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1575</td>\n",
       "      <td>40</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1578</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1581</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1582</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1584</td>\n",
       "      <td>50</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  species   margin1   margin2   margin3   margin4   margin5  \\\n",
       "0       1        3  0.007812  0.023438  0.023438  0.003906  0.011719   \n",
       "1       2       49  0.005859  0.000000  0.031250  0.015625  0.025391   \n",
       "2       3       65  0.005859  0.009766  0.019531  0.007812  0.003906   \n",
       "3       5       94  0.000000  0.003906  0.023438  0.005859  0.021484   \n",
       "4       6       84  0.005859  0.003906  0.048828  0.009766  0.013672   \n",
       "5       8       40  0.070312  0.093750  0.033203  0.001953  0.000000   \n",
       "6      10       54  0.021484  0.031250  0.017578  0.009766  0.001953   \n",
       "7      11       78  0.000000  0.000000  0.037109  0.050781  0.003906   \n",
       "8      14       53  0.005859  0.001953  0.033203  0.015625  0.001953   \n",
       "9      15       89  0.000000  0.000000  0.009766  0.037109  0.072266   \n",
       "10     17       98  0.019531  0.031250  0.001953  0.005859  0.003906   \n",
       "11     18       16  0.001953  0.001953  0.023438  0.025391  0.076172   \n",
       "12     20       74  0.015625  0.011719  0.041016  0.003906  0.023438   \n",
       "13     21       50  0.011719  0.011719  0.054688  0.017578  0.007812   \n",
       "14     22       58  0.011719  0.007812  0.111330  0.027344  0.023438   \n",
       "15     25       31  0.027344  0.025391  0.066406  0.007812  0.003906   \n",
       "16     26       43  0.009766  0.062500  0.033203  0.029297  0.011719   \n",
       "17     27        4  0.000000  0.000000  0.001953  0.029297  0.111330   \n",
       "18     29       75  0.001953  0.000000  0.015625  0.031250  0.011719   \n",
       "19     30       44  0.005859  0.027344  0.017578  0.041016  0.007812   \n",
       "20     31       83  0.019531  0.019531  0.064453  0.025391  0.007812   \n",
       "21     32       84  0.019531  0.029297  0.080078  0.033203  0.025391   \n",
       "22     34       13  0.000000  0.005859  0.023438  0.001953  0.013672   \n",
       "23     35       66  0.029297  0.033203  0.033203  0.021484  0.001953   \n",
       "24     37       15  0.001953  0.023438  0.005859  0.029297  0.000000   \n",
       "25     38        6  0.015625  0.031250  0.101560  0.015625  0.005859   \n",
       "26     40       73  0.027344  0.009766  0.021484  0.042969  0.017578   \n",
       "27     42       22  0.085938  0.095703  0.007812  0.003906  0.000000   \n",
       "28     43       73  0.001953  0.003906  0.017578  0.044922  0.041016   \n",
       "29     45       31  0.019531  0.041016  0.093750  0.005859  0.011719   \n",
       "..    ...      ...       ...       ...       ...       ...       ...   \n",
       "960  1541       85  0.003906  0.000000  0.013672  0.029297  0.007812   \n",
       "961  1543       89  0.000000  0.000000  0.007812  0.015625  0.072266   \n",
       "962  1544       94  0.000000  0.017578  0.015625  0.011719  0.037109   \n",
       "963  1545       45  0.009766  0.011719  0.121090  0.003906  0.009766   \n",
       "964  1547       48  0.001953  0.001953  0.007812  0.019531  0.037109   \n",
       "965  1548       86  0.003906  0.003906  0.033203  0.037109  0.003906   \n",
       "966  1549       81  0.011719  0.005859  0.048828  0.009766  0.003906   \n",
       "967  1550       14  0.000000  0.000000  0.005859  0.013672  0.031250   \n",
       "968  1551        4  0.000000  0.000000  0.003906  0.039062  0.082031   \n",
       "969  1552       77  0.025391  0.042969  0.025391  0.033203  0.001953   \n",
       "970  1554       56  0.011719  0.013672  0.021484  0.031250  0.017578   \n",
       "971  1555       82  0.000000  0.005859  0.072266  0.076172  0.007812   \n",
       "972  1556        2  0.029297  0.005859  0.056641  0.031250  0.000000   \n",
       "973  1557       85  0.011719  0.005859  0.001953  0.003906  0.011719   \n",
       "974  1559       70  0.037109  0.056641  0.019531  0.021484  0.001953   \n",
       "975  1561       88  0.074219  0.093750  0.007812  0.013672  0.000000   \n",
       "976  1562        0  0.000000  0.000000  0.013672  0.015625  0.048828   \n",
       "977  1563       75  0.001953  0.000000  0.041016  0.015625  0.015625   \n",
       "978  1566       14  0.000000  0.000000  0.007812  0.007812  0.039062   \n",
       "979  1568       86  0.009766  0.003906  0.025391  0.017578  0.005859   \n",
       "980  1569       81  0.031250  0.009766  0.078125  0.017578  0.003906   \n",
       "981  1570       97  0.027344  0.044922  0.023438  0.017578  0.001953   \n",
       "982  1571       70  0.017578  0.074219  0.015625  0.013672  0.000000   \n",
       "983  1572       72  0.023438  0.076172  0.015625  0.019531  0.000000   \n",
       "984  1574       34  0.080078  0.048828  0.023438  0.007812  0.000000   \n",
       "985  1575       40  0.060547  0.119140  0.007812  0.003906  0.000000   \n",
       "986  1578        5  0.001953  0.003906  0.021484  0.107420  0.001953   \n",
       "987  1581       11  0.001953  0.003906  0.000000  0.021484  0.078125   \n",
       "988  1582       78  0.000000  0.000000  0.046875  0.056641  0.009766   \n",
       "989  1584       50  0.023438  0.019531  0.031250  0.015625  0.005859   \n",
       "\n",
       "      margin6   margin7   margin8    ...      texture55  texture56  texture57  \\\n",
       "0    0.009766  0.027344  0.000000    ...       0.007812   0.000000   0.002930   \n",
       "1    0.001953  0.019531  0.000000    ...       0.000977   0.000000   0.000000   \n",
       "2    0.005859  0.068359  0.000000    ...       0.154300   0.000000   0.005859   \n",
       "3    0.019531  0.023438  0.000000    ...       0.000000   0.000977   0.000000   \n",
       "4    0.015625  0.005859  0.000000    ...       0.096680   0.000000   0.021484   \n",
       "5    0.152340  0.007812  0.000000    ...       0.145510   0.000000   0.041992   \n",
       "6    0.042969  0.039062  0.000000    ...       0.085938   0.000000   0.040039   \n",
       "7    0.000000  0.003906  0.000000    ...       0.038086   0.025391   0.009766   \n",
       "8    0.000000  0.023438  0.000000    ...       0.000000   0.000000   0.008789   \n",
       "9    0.000000  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "10   0.013672  0.033203  0.000000    ...       0.009766   0.000000   0.000000   \n",
       "11   0.000000  0.029297  0.000000    ...       0.013672   0.003906   0.014648   \n",
       "12   0.015625  0.019531  0.000000    ...       0.003906   0.000000   0.086914   \n",
       "13   0.009766  0.011719  0.007812    ...       0.002930   0.000000   0.015625   \n",
       "14   0.039062  0.013672  0.000000    ...       0.000977   0.001953   0.000000   \n",
       "15   0.052734  0.031250  0.000000    ...       0.008789   0.000000   0.004883   \n",
       "16   0.044922  0.005859  0.000000    ...       0.000000   0.000000   0.004883   \n",
       "17   0.000000  0.000000  0.000000    ...       0.012695   0.000000   0.079102   \n",
       "18   0.000000  0.021484  0.001953    ...       0.000000   0.000000   0.001953   \n",
       "19   0.017578  0.046875  0.001953    ...       0.000000   0.000977   0.000000   \n",
       "20   0.005859  0.023438  0.000000    ...       0.000000   0.000977   0.001953   \n",
       "21   0.023438  0.009766  0.005859    ...       0.021484   0.000000   0.057617   \n",
       "22   0.001953  0.015625  0.000000    ...       0.000000   0.019531   0.000000   \n",
       "23   0.068359  0.037109  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "24   0.000000  0.000000  0.000000    ...       0.103520   0.009766   0.000000   \n",
       "25   0.017578  0.000000  0.000000    ...       0.029297   0.033203   0.007812   \n",
       "26   0.009766  0.037109  0.031250    ...       0.000000   0.000000   0.003906   \n",
       "27   0.148440  0.007812  0.000000    ...       0.018555   0.052734   0.005859   \n",
       "28   0.011719  0.039062  0.013672    ...       0.000000   0.000000   0.000000   \n",
       "29   0.037109  0.035156  0.000000    ...       0.076172   0.000000   0.049805   \n",
       "..        ...       ...       ...    ...            ...        ...        ...   \n",
       "960  0.005859  0.019531  0.000000    ...       0.000000   0.000000   0.013672   \n",
       "961  0.000000  0.011719  0.003906    ...       0.000000   0.000000   0.000000   \n",
       "962  0.003906  0.019531  0.000000    ...       0.000000   0.002930   0.000000   \n",
       "963  0.009766  0.007812  0.005859    ...       0.213870   0.000000   0.041016   \n",
       "964  0.000000  0.013672  0.000000    ...       0.000000   0.000000   0.003906   \n",
       "965  0.003906  0.031250  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "966  0.046875  0.050781  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "967  0.000000  0.011719  0.000000    ...       0.000000   0.020508   0.002930   \n",
       "968  0.000000  0.000000  0.000000    ...       0.007812   0.000000   0.144530   \n",
       "969  0.033203  0.011719  0.000000    ...       0.000000   0.147460   0.000977   \n",
       "970  0.003906  0.025391  0.003906    ...       0.000000   0.000000   0.002930   \n",
       "971  0.000000  0.000000  0.000000    ...       0.040039   0.000977   0.006836   \n",
       "972  0.021484  0.031250  0.003906    ...       0.109380   0.000000   0.013672   \n",
       "973  0.011719  0.031250  0.000000    ...       0.066406   0.000000   0.096680   \n",
       "974  0.068359  0.015625  0.000000    ...       0.046875   0.000000   0.009766   \n",
       "975  0.119140  0.003906  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "976  0.000000  0.033203  0.000000    ...       0.094727   0.000000   0.016602   \n",
       "977  0.000000  0.009766  0.000000    ...       0.000000   0.000000   0.017578   \n",
       "978  0.000000  0.001953  0.005859    ...       0.000000   0.056641   0.000000   \n",
       "979  0.003906  0.035156  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "980  0.058594  0.048828  0.000000    ...       0.000977   0.000000   0.024414   \n",
       "981  0.025391  0.083984  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "982  0.074219  0.005859  0.000000    ...       0.025391   0.000000   0.003906   \n",
       "983  0.039062  0.011719  0.000000    ...       0.053711   0.000000   0.002930   \n",
       "984  0.091797  0.009766  0.000000    ...       0.003906   0.003906   0.041016   \n",
       "985  0.148440  0.017578  0.000000    ...       0.242190   0.000000   0.034180   \n",
       "986  0.000000  0.000000  0.000000    ...       0.170900   0.000000   0.018555   \n",
       "987  0.003906  0.007812  0.000000    ...       0.004883   0.000977   0.004883   \n",
       "988  0.000000  0.000000  0.000000    ...       0.083008   0.030273   0.000977   \n",
       "989  0.019531  0.035156  0.000000    ...       0.000000   0.000000   0.002930   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.002930   0.035156   0.000000   0.000000   0.004883   0.000000   \n",
       "1     0.000977   0.023438   0.000000   0.000000   0.000977   0.039062   \n",
       "2     0.000977   0.007812   0.000000   0.000000   0.000000   0.020508   \n",
       "3     0.000000   0.020508   0.000000   0.000000   0.017578   0.000000   \n",
       "4     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5     0.000000   0.005859   0.000000   0.000000   0.000000   0.001953   \n",
       "6     0.000000   0.009766   0.000000   0.000000   0.000000   0.039062   \n",
       "7     0.002930   0.021484   0.000000   0.037109   0.006836   0.002930   \n",
       "8     0.000000   0.017578   0.000000   0.000000   0.000977   0.033203   \n",
       "9     0.070312   0.013672   0.192380   0.000000   0.074219   0.000000   \n",
       "10    0.002930   0.024414   0.000000   0.000000   0.006836   0.000000   \n",
       "11    0.003906   0.036133   0.000000   0.000000   0.012695   0.005859   \n",
       "12    0.000000   0.013672   0.000000   0.000000   0.000000   0.011719   \n",
       "13    0.000000   0.015625   0.000000   0.000000   0.000000   0.018555   \n",
       "14    0.034180   0.007812   0.038086   0.000000   0.089844   0.023438   \n",
       "15    0.004883   0.009766   0.001953   0.000000   0.003906   0.000000   \n",
       "16    0.000000   0.068359   0.000000   0.000000   0.000000   0.036133   \n",
       "17    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "18    0.005859   0.049805   0.000000   0.000000   0.006836   0.000000   \n",
       "19    0.020508   0.001953   0.333010   0.000000   0.169920   0.000000   \n",
       "20    0.016602   0.023438   0.004883   0.000000   0.003906   0.013672   \n",
       "21    0.000000   0.002930   0.000000   0.000000   0.000000   0.000000   \n",
       "22    0.031250   0.005859   0.054688   0.000000   0.103520   0.000977   \n",
       "23    0.044922   0.003906   0.004883   0.000000   0.069336   0.000000   \n",
       "24    0.002930   0.002930   0.077148   0.000000   0.052734   0.000000   \n",
       "25    0.006836   0.014648   0.000000   0.000000   0.020508   0.000000   \n",
       "26    0.000977   0.009766   0.001953   0.000000   0.001953   0.023438   \n",
       "27    0.000977   0.019531   0.000000   0.011719   0.041992   0.004883   \n",
       "28    0.000977   0.007812   0.000000   0.000000   0.007812   0.024414   \n",
       "29    0.000000   0.000977   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "960   0.000000   0.019531   0.000000   0.000000   0.003906   0.029297   \n",
       "961   0.041016   0.057617   0.011719   0.000000   0.010742   0.000000   \n",
       "962   0.015625   0.029297   0.000000   0.000000   0.030273   0.000000   \n",
       "963   0.000000   0.003906   0.000000   0.000000   0.000000   0.000000   \n",
       "964   0.000977   0.039062   0.000000   0.000000   0.000000   0.022461   \n",
       "965   0.011719   0.028320   0.000000   0.000000   0.011719   0.008789   \n",
       "966   0.039062   0.000977   0.008789   0.000000   0.032227   0.000000   \n",
       "967   0.009766   0.026367   0.000000   0.000000   0.009766   0.011719   \n",
       "968   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "969   0.018555   0.008789   0.009766   0.000000   0.078125   0.022461   \n",
       "970   0.000000   0.019531   0.000000   0.000000   0.001953   0.010742   \n",
       "971   0.002930   0.022461   0.000000   0.082031   0.001953   0.005859   \n",
       "972   0.004883   0.018555   0.000000   0.000000   0.001953   0.000977   \n",
       "973   0.000000   0.006836   0.000000   0.000000   0.000000   0.030273   \n",
       "974   0.000000   0.010742   0.000000   0.000977   0.005859   0.011719   \n",
       "975   0.005859   0.007812   0.000000   0.000000   0.030273   0.012695   \n",
       "976   0.000000   0.018555   0.000000   0.000000   0.000977   0.028320   \n",
       "977   0.011719   0.070312   0.000000   0.000000   0.000977   0.000000   \n",
       "978   0.035156   0.011719   0.024414   0.000000   0.066406   0.013672   \n",
       "979   0.008789   0.016602   0.000000   0.000000   0.024414   0.037109   \n",
       "980   0.006836   0.017578   0.000977   0.000000   0.016602   0.000000   \n",
       "981   0.036133   0.082031   0.000000   0.000000   0.026367   0.008789   \n",
       "982   0.000000   0.009766   0.000000   0.012695   0.000000   0.009766   \n",
       "983   0.000977   0.005859   0.000000   0.081055   0.000000   0.003906   \n",
       "984   0.006836   0.017578   0.000000   0.000000   0.003906   0.000977   \n",
       "985   0.000000   0.010742   0.000000   0.000000   0.000000   0.000000   \n",
       "986   0.000000   0.011719   0.000000   0.000000   0.000977   0.000000   \n",
       "987   0.027344   0.016602   0.007812   0.000000   0.027344   0.000000   \n",
       "988   0.002930   0.014648   0.000000   0.041992   0.000000   0.001953   \n",
       "989   0.000000   0.012695   0.000000   0.000000   0.023438   0.025391   \n",
       "\n",
       "     texture64  \n",
       "0     0.025391  \n",
       "1     0.022461  \n",
       "2     0.002930  \n",
       "3     0.047852  \n",
       "4     0.031250  \n",
       "5     0.013672  \n",
       "6     0.003906  \n",
       "7     0.036133  \n",
       "8     0.074219  \n",
       "9     0.000000  \n",
       "10    0.004883  \n",
       "11    0.000000  \n",
       "12    0.002930  \n",
       "13    0.022461  \n",
       "14    0.001953  \n",
       "15    0.036133  \n",
       "16    0.000977  \n",
       "17    0.019531  \n",
       "18    0.001953  \n",
       "19    0.000000  \n",
       "20    0.004883  \n",
       "21    0.035156  \n",
       "22    0.000000  \n",
       "23    0.000000  \n",
       "24    0.000000  \n",
       "25    0.035156  \n",
       "26    0.007812  \n",
       "27    0.020508  \n",
       "28    0.015625  \n",
       "29    0.001953  \n",
       "..         ...  \n",
       "960   0.055664  \n",
       "961   0.002930  \n",
       "962   0.012695  \n",
       "963   0.014648  \n",
       "964   0.033203  \n",
       "965   0.000000  \n",
       "966   0.000977  \n",
       "967   0.041016  \n",
       "968   0.010742  \n",
       "969   0.000000  \n",
       "970   0.067383  \n",
       "971   0.029297  \n",
       "972   0.004883  \n",
       "973   0.011719  \n",
       "974   0.002930  \n",
       "975   0.000977  \n",
       "976   0.000000  \n",
       "977   0.012695  \n",
       "978   0.000000  \n",
       "979   0.003906  \n",
       "980   0.041016  \n",
       "981   0.000000  \n",
       "982   0.000000  \n",
       "983   0.019531  \n",
       "984   0.032227  \n",
       "985   0.018555  \n",
       "986   0.021484  \n",
       "987   0.001953  \n",
       "988   0.002930  \n",
       "989   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = preprocess_df(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.192380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>98</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>74</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22</td>\n",
       "      <td>58</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>75</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.333010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>83</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>84</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>66</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43</td>\n",
       "      <td>73</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>1541</td>\n",
       "      <td>85</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.055664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>1543</td>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>1544</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>1545</td>\n",
       "      <td>45</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.121090</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>1547</td>\n",
       "      <td>48</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>1548</td>\n",
       "      <td>86</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>1549</td>\n",
       "      <td>81</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>1550</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>1551</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>1552</td>\n",
       "      <td>77</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147460</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1554</td>\n",
       "      <td>56</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>1555</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>1556</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>1557</td>\n",
       "      <td>85</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>1559</td>\n",
       "      <td>70</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>1561</td>\n",
       "      <td>88</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1562</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>1563</td>\n",
       "      <td>75</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>1566</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1568</td>\n",
       "      <td>86</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>1569</td>\n",
       "      <td>81</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>1570</td>\n",
       "      <td>97</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>1571</td>\n",
       "      <td>70</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1572</td>\n",
       "      <td>72</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>1574</td>\n",
       "      <td>34</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1575</td>\n",
       "      <td>40</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1578</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1581</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1582</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1584</td>\n",
       "      <td>50</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  species   margin1   margin2   margin3   margin4   margin5  \\\n",
       "0       1        3  0.007812  0.023438  0.023438  0.003906  0.011719   \n",
       "1       2       49  0.005859  0.000000  0.031250  0.015625  0.025391   \n",
       "2       3       65  0.005859  0.009766  0.019531  0.007812  0.003906   \n",
       "3       5       94  0.000000  0.003906  0.023438  0.005859  0.021484   \n",
       "4       6       84  0.005859  0.003906  0.048828  0.009766  0.013672   \n",
       "5       8       40  0.070312  0.093750  0.033203  0.001953  0.000000   \n",
       "6      10       54  0.021484  0.031250  0.017578  0.009766  0.001953   \n",
       "7      11       78  0.000000  0.000000  0.037109  0.050781  0.003906   \n",
       "8      14       53  0.005859  0.001953  0.033203  0.015625  0.001953   \n",
       "9      15       89  0.000000  0.000000  0.009766  0.037109  0.072266   \n",
       "10     17       98  0.019531  0.031250  0.001953  0.005859  0.003906   \n",
       "11     18       16  0.001953  0.001953  0.023438  0.025391  0.076172   \n",
       "12     20       74  0.015625  0.011719  0.041016  0.003906  0.023438   \n",
       "13     21       50  0.011719  0.011719  0.054688  0.017578  0.007812   \n",
       "14     22       58  0.011719  0.007812  0.111330  0.027344  0.023438   \n",
       "15     25       31  0.027344  0.025391  0.066406  0.007812  0.003906   \n",
       "16     26       43  0.009766  0.062500  0.033203  0.029297  0.011719   \n",
       "17     27        4  0.000000  0.000000  0.001953  0.029297  0.111330   \n",
       "18     29       75  0.001953  0.000000  0.015625  0.031250  0.011719   \n",
       "19     30       44  0.005859  0.027344  0.017578  0.041016  0.007812   \n",
       "20     31       83  0.019531  0.019531  0.064453  0.025391  0.007812   \n",
       "21     32       84  0.019531  0.029297  0.080078  0.033203  0.025391   \n",
       "22     34       13  0.000000  0.005859  0.023438  0.001953  0.013672   \n",
       "23     35       66  0.029297  0.033203  0.033203  0.021484  0.001953   \n",
       "24     37       15  0.001953  0.023438  0.005859  0.029297  0.000000   \n",
       "25     38        6  0.015625  0.031250  0.101560  0.015625  0.005859   \n",
       "26     40       73  0.027344  0.009766  0.021484  0.042969  0.017578   \n",
       "27     42       22  0.085938  0.095703  0.007812  0.003906  0.000000   \n",
       "28     43       73  0.001953  0.003906  0.017578  0.044922  0.041016   \n",
       "29     45       31  0.019531  0.041016  0.093750  0.005859  0.011719   \n",
       "..    ...      ...       ...       ...       ...       ...       ...   \n",
       "960  1541       85  0.003906  0.000000  0.013672  0.029297  0.007812   \n",
       "961  1543       89  0.000000  0.000000  0.007812  0.015625  0.072266   \n",
       "962  1544       94  0.000000  0.017578  0.015625  0.011719  0.037109   \n",
       "963  1545       45  0.009766  0.011719  0.121090  0.003906  0.009766   \n",
       "964  1547       48  0.001953  0.001953  0.007812  0.019531  0.037109   \n",
       "965  1548       86  0.003906  0.003906  0.033203  0.037109  0.003906   \n",
       "966  1549       81  0.011719  0.005859  0.048828  0.009766  0.003906   \n",
       "967  1550       14  0.000000  0.000000  0.005859  0.013672  0.031250   \n",
       "968  1551        4  0.000000  0.000000  0.003906  0.039062  0.082031   \n",
       "969  1552       77  0.025391  0.042969  0.025391  0.033203  0.001953   \n",
       "970  1554       56  0.011719  0.013672  0.021484  0.031250  0.017578   \n",
       "971  1555       82  0.000000  0.005859  0.072266  0.076172  0.007812   \n",
       "972  1556        2  0.029297  0.005859  0.056641  0.031250  0.000000   \n",
       "973  1557       85  0.011719  0.005859  0.001953  0.003906  0.011719   \n",
       "974  1559       70  0.037109  0.056641  0.019531  0.021484  0.001953   \n",
       "975  1561       88  0.074219  0.093750  0.007812  0.013672  0.000000   \n",
       "976  1562        0  0.000000  0.000000  0.013672  0.015625  0.048828   \n",
       "977  1563       75  0.001953  0.000000  0.041016  0.015625  0.015625   \n",
       "978  1566       14  0.000000  0.000000  0.007812  0.007812  0.039062   \n",
       "979  1568       86  0.009766  0.003906  0.025391  0.017578  0.005859   \n",
       "980  1569       81  0.031250  0.009766  0.078125  0.017578  0.003906   \n",
       "981  1570       97  0.027344  0.044922  0.023438  0.017578  0.001953   \n",
       "982  1571       70  0.017578  0.074219  0.015625  0.013672  0.000000   \n",
       "983  1572       72  0.023438  0.076172  0.015625  0.019531  0.000000   \n",
       "984  1574       34  0.080078  0.048828  0.023438  0.007812  0.000000   \n",
       "985  1575       40  0.060547  0.119140  0.007812  0.003906  0.000000   \n",
       "986  1578        5  0.001953  0.003906  0.021484  0.107420  0.001953   \n",
       "987  1581       11  0.001953  0.003906  0.000000  0.021484  0.078125   \n",
       "988  1582       78  0.000000  0.000000  0.046875  0.056641  0.009766   \n",
       "989  1584       50  0.023438  0.019531  0.031250  0.015625  0.005859   \n",
       "\n",
       "      margin6   margin7   margin8    ...      texture55  texture56  texture57  \\\n",
       "0    0.009766  0.027344  0.000000    ...       0.007812   0.000000   0.002930   \n",
       "1    0.001953  0.019531  0.000000    ...       0.000977   0.000000   0.000000   \n",
       "2    0.005859  0.068359  0.000000    ...       0.154300   0.000000   0.005859   \n",
       "3    0.019531  0.023438  0.000000    ...       0.000000   0.000977   0.000000   \n",
       "4    0.015625  0.005859  0.000000    ...       0.096680   0.000000   0.021484   \n",
       "5    0.152340  0.007812  0.000000    ...       0.145510   0.000000   0.041992   \n",
       "6    0.042969  0.039062  0.000000    ...       0.085938   0.000000   0.040039   \n",
       "7    0.000000  0.003906  0.000000    ...       0.038086   0.025391   0.009766   \n",
       "8    0.000000  0.023438  0.000000    ...       0.000000   0.000000   0.008789   \n",
       "9    0.000000  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "10   0.013672  0.033203  0.000000    ...       0.009766   0.000000   0.000000   \n",
       "11   0.000000  0.029297  0.000000    ...       0.013672   0.003906   0.014648   \n",
       "12   0.015625  0.019531  0.000000    ...       0.003906   0.000000   0.086914   \n",
       "13   0.009766  0.011719  0.007812    ...       0.002930   0.000000   0.015625   \n",
       "14   0.039062  0.013672  0.000000    ...       0.000977   0.001953   0.000000   \n",
       "15   0.052734  0.031250  0.000000    ...       0.008789   0.000000   0.004883   \n",
       "16   0.044922  0.005859  0.000000    ...       0.000000   0.000000   0.004883   \n",
       "17   0.000000  0.000000  0.000000    ...       0.012695   0.000000   0.079102   \n",
       "18   0.000000  0.021484  0.001953    ...       0.000000   0.000000   0.001953   \n",
       "19   0.017578  0.046875  0.001953    ...       0.000000   0.000977   0.000000   \n",
       "20   0.005859  0.023438  0.000000    ...       0.000000   0.000977   0.001953   \n",
       "21   0.023438  0.009766  0.005859    ...       0.021484   0.000000   0.057617   \n",
       "22   0.001953  0.015625  0.000000    ...       0.000000   0.019531   0.000000   \n",
       "23   0.068359  0.037109  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "24   0.000000  0.000000  0.000000    ...       0.103520   0.009766   0.000000   \n",
       "25   0.017578  0.000000  0.000000    ...       0.029297   0.033203   0.007812   \n",
       "26   0.009766  0.037109  0.031250    ...       0.000000   0.000000   0.003906   \n",
       "27   0.148440  0.007812  0.000000    ...       0.018555   0.052734   0.005859   \n",
       "28   0.011719  0.039062  0.013672    ...       0.000000   0.000000   0.000000   \n",
       "29   0.037109  0.035156  0.000000    ...       0.076172   0.000000   0.049805   \n",
       "..        ...       ...       ...    ...            ...        ...        ...   \n",
       "960  0.005859  0.019531  0.000000    ...       0.000000   0.000000   0.013672   \n",
       "961  0.000000  0.011719  0.003906    ...       0.000000   0.000000   0.000000   \n",
       "962  0.003906  0.019531  0.000000    ...       0.000000   0.002930   0.000000   \n",
       "963  0.009766  0.007812  0.005859    ...       0.213870   0.000000   0.041016   \n",
       "964  0.000000  0.013672  0.000000    ...       0.000000   0.000000   0.003906   \n",
       "965  0.003906  0.031250  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "966  0.046875  0.050781  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "967  0.000000  0.011719  0.000000    ...       0.000000   0.020508   0.002930   \n",
       "968  0.000000  0.000000  0.000000    ...       0.007812   0.000000   0.144530   \n",
       "969  0.033203  0.011719  0.000000    ...       0.000000   0.147460   0.000977   \n",
       "970  0.003906  0.025391  0.003906    ...       0.000000   0.000000   0.002930   \n",
       "971  0.000000  0.000000  0.000000    ...       0.040039   0.000977   0.006836   \n",
       "972  0.021484  0.031250  0.003906    ...       0.109380   0.000000   0.013672   \n",
       "973  0.011719  0.031250  0.000000    ...       0.066406   0.000000   0.096680   \n",
       "974  0.068359  0.015625  0.000000    ...       0.046875   0.000000   0.009766   \n",
       "975  0.119140  0.003906  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "976  0.000000  0.033203  0.000000    ...       0.094727   0.000000   0.016602   \n",
       "977  0.000000  0.009766  0.000000    ...       0.000000   0.000000   0.017578   \n",
       "978  0.000000  0.001953  0.005859    ...       0.000000   0.056641   0.000000   \n",
       "979  0.003906  0.035156  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "980  0.058594  0.048828  0.000000    ...       0.000977   0.000000   0.024414   \n",
       "981  0.025391  0.083984  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "982  0.074219  0.005859  0.000000    ...       0.025391   0.000000   0.003906   \n",
       "983  0.039062  0.011719  0.000000    ...       0.053711   0.000000   0.002930   \n",
       "984  0.091797  0.009766  0.000000    ...       0.003906   0.003906   0.041016   \n",
       "985  0.148440  0.017578  0.000000    ...       0.242190   0.000000   0.034180   \n",
       "986  0.000000  0.000000  0.000000    ...       0.170900   0.000000   0.018555   \n",
       "987  0.003906  0.007812  0.000000    ...       0.004883   0.000977   0.004883   \n",
       "988  0.000000  0.000000  0.000000    ...       0.083008   0.030273   0.000977   \n",
       "989  0.019531  0.035156  0.000000    ...       0.000000   0.000000   0.002930   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.002930   0.035156   0.000000   0.000000   0.004883   0.000000   \n",
       "1     0.000977   0.023438   0.000000   0.000000   0.000977   0.039062   \n",
       "2     0.000977   0.007812   0.000000   0.000000   0.000000   0.020508   \n",
       "3     0.000000   0.020508   0.000000   0.000000   0.017578   0.000000   \n",
       "4     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5     0.000000   0.005859   0.000000   0.000000   0.000000   0.001953   \n",
       "6     0.000000   0.009766   0.000000   0.000000   0.000000   0.039062   \n",
       "7     0.002930   0.021484   0.000000   0.037109   0.006836   0.002930   \n",
       "8     0.000000   0.017578   0.000000   0.000000   0.000977   0.033203   \n",
       "9     0.070312   0.013672   0.192380   0.000000   0.074219   0.000000   \n",
       "10    0.002930   0.024414   0.000000   0.000000   0.006836   0.000000   \n",
       "11    0.003906   0.036133   0.000000   0.000000   0.012695   0.005859   \n",
       "12    0.000000   0.013672   0.000000   0.000000   0.000000   0.011719   \n",
       "13    0.000000   0.015625   0.000000   0.000000   0.000000   0.018555   \n",
       "14    0.034180   0.007812   0.038086   0.000000   0.089844   0.023438   \n",
       "15    0.004883   0.009766   0.001953   0.000000   0.003906   0.000000   \n",
       "16    0.000000   0.068359   0.000000   0.000000   0.000000   0.036133   \n",
       "17    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "18    0.005859   0.049805   0.000000   0.000000   0.006836   0.000000   \n",
       "19    0.020508   0.001953   0.333010   0.000000   0.169920   0.000000   \n",
       "20    0.016602   0.023438   0.004883   0.000000   0.003906   0.013672   \n",
       "21    0.000000   0.002930   0.000000   0.000000   0.000000   0.000000   \n",
       "22    0.031250   0.005859   0.054688   0.000000   0.103520   0.000977   \n",
       "23    0.044922   0.003906   0.004883   0.000000   0.069336   0.000000   \n",
       "24    0.002930   0.002930   0.077148   0.000000   0.052734   0.000000   \n",
       "25    0.006836   0.014648   0.000000   0.000000   0.020508   0.000000   \n",
       "26    0.000977   0.009766   0.001953   0.000000   0.001953   0.023438   \n",
       "27    0.000977   0.019531   0.000000   0.011719   0.041992   0.004883   \n",
       "28    0.000977   0.007812   0.000000   0.000000   0.007812   0.024414   \n",
       "29    0.000000   0.000977   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "960   0.000000   0.019531   0.000000   0.000000   0.003906   0.029297   \n",
       "961   0.041016   0.057617   0.011719   0.000000   0.010742   0.000000   \n",
       "962   0.015625   0.029297   0.000000   0.000000   0.030273   0.000000   \n",
       "963   0.000000   0.003906   0.000000   0.000000   0.000000   0.000000   \n",
       "964   0.000977   0.039062   0.000000   0.000000   0.000000   0.022461   \n",
       "965   0.011719   0.028320   0.000000   0.000000   0.011719   0.008789   \n",
       "966   0.039062   0.000977   0.008789   0.000000   0.032227   0.000000   \n",
       "967   0.009766   0.026367   0.000000   0.000000   0.009766   0.011719   \n",
       "968   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "969   0.018555   0.008789   0.009766   0.000000   0.078125   0.022461   \n",
       "970   0.000000   0.019531   0.000000   0.000000   0.001953   0.010742   \n",
       "971   0.002930   0.022461   0.000000   0.082031   0.001953   0.005859   \n",
       "972   0.004883   0.018555   0.000000   0.000000   0.001953   0.000977   \n",
       "973   0.000000   0.006836   0.000000   0.000000   0.000000   0.030273   \n",
       "974   0.000000   0.010742   0.000000   0.000977   0.005859   0.011719   \n",
       "975   0.005859   0.007812   0.000000   0.000000   0.030273   0.012695   \n",
       "976   0.000000   0.018555   0.000000   0.000000   0.000977   0.028320   \n",
       "977   0.011719   0.070312   0.000000   0.000000   0.000977   0.000000   \n",
       "978   0.035156   0.011719   0.024414   0.000000   0.066406   0.013672   \n",
       "979   0.008789   0.016602   0.000000   0.000000   0.024414   0.037109   \n",
       "980   0.006836   0.017578   0.000977   0.000000   0.016602   0.000000   \n",
       "981   0.036133   0.082031   0.000000   0.000000   0.026367   0.008789   \n",
       "982   0.000000   0.009766   0.000000   0.012695   0.000000   0.009766   \n",
       "983   0.000977   0.005859   0.000000   0.081055   0.000000   0.003906   \n",
       "984   0.006836   0.017578   0.000000   0.000000   0.003906   0.000977   \n",
       "985   0.000000   0.010742   0.000000   0.000000   0.000000   0.000000   \n",
       "986   0.000000   0.011719   0.000000   0.000000   0.000977   0.000000   \n",
       "987   0.027344   0.016602   0.007812   0.000000   0.027344   0.000000   \n",
       "988   0.002930   0.014648   0.000000   0.041992   0.000000   0.001953   \n",
       "989   0.000000   0.012695   0.000000   0.000000   0.023438   0.025391   \n",
       "\n",
       "     texture64  \n",
       "0     0.025391  \n",
       "1     0.022461  \n",
       "2     0.002930  \n",
       "3     0.047852  \n",
       "4     0.031250  \n",
       "5     0.013672  \n",
       "6     0.003906  \n",
       "7     0.036133  \n",
       "8     0.074219  \n",
       "9     0.000000  \n",
       "10    0.004883  \n",
       "11    0.000000  \n",
       "12    0.002930  \n",
       "13    0.022461  \n",
       "14    0.001953  \n",
       "15    0.036133  \n",
       "16    0.000977  \n",
       "17    0.019531  \n",
       "18    0.001953  \n",
       "19    0.000000  \n",
       "20    0.004883  \n",
       "21    0.035156  \n",
       "22    0.000000  \n",
       "23    0.000000  \n",
       "24    0.000000  \n",
       "25    0.035156  \n",
       "26    0.007812  \n",
       "27    0.020508  \n",
       "28    0.015625  \n",
       "29    0.001953  \n",
       "..         ...  \n",
       "960   0.055664  \n",
       "961   0.002930  \n",
       "962   0.012695  \n",
       "963   0.014648  \n",
       "964   0.033203  \n",
       "965   0.000000  \n",
       "966   0.000977  \n",
       "967   0.041016  \n",
       "968   0.010742  \n",
       "969   0.000000  \n",
       "970   0.067383  \n",
       "971   0.029297  \n",
       "972   0.004883  \n",
       "973   0.011719  \n",
       "974   0.002930  \n",
       "975   0.000977  \n",
       "976   0.000000  \n",
       "977   0.012695  \n",
       "978   0.000000  \n",
       "979   0.003906  \n",
       "980   0.041016  \n",
       "981   0.000000  \n",
       "982   0.000000  \n",
       "983   0.019531  \n",
       "984   0.032227  \n",
       "985   0.018555  \n",
       "986   0.021484  \n",
       "987   0.001953  \n",
       "988   0.002930  \n",
       "989   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "train.drop(['id','species'],axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>margin10</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.192380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.333010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.055664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.121090</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147460</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "0    0.007812  0.023438  0.023438  0.003906  0.011719  0.009766  0.027344   \n",
       "1    0.005859  0.000000  0.031250  0.015625  0.025391  0.001953  0.019531   \n",
       "2    0.005859  0.009766  0.019531  0.007812  0.003906  0.005859  0.068359   \n",
       "3    0.000000  0.003906  0.023438  0.005859  0.021484  0.019531  0.023438   \n",
       "4    0.005859  0.003906  0.048828  0.009766  0.013672  0.015625  0.005859   \n",
       "5    0.070312  0.093750  0.033203  0.001953  0.000000  0.152340  0.007812   \n",
       "6    0.021484  0.031250  0.017578  0.009766  0.001953  0.042969  0.039062   \n",
       "7    0.000000  0.000000  0.037109  0.050781  0.003906  0.000000  0.003906   \n",
       "8    0.005859  0.001953  0.033203  0.015625  0.001953  0.000000  0.023438   \n",
       "9    0.000000  0.000000  0.009766  0.037109  0.072266  0.000000  0.000000   \n",
       "10   0.019531  0.031250  0.001953  0.005859  0.003906  0.013672  0.033203   \n",
       "11   0.001953  0.001953  0.023438  0.025391  0.076172  0.000000  0.029297   \n",
       "12   0.015625  0.011719  0.041016  0.003906  0.023438  0.015625  0.019531   \n",
       "13   0.011719  0.011719  0.054688  0.017578  0.007812  0.009766  0.011719   \n",
       "14   0.011719  0.007812  0.111330  0.027344  0.023438  0.039062  0.013672   \n",
       "15   0.027344  0.025391  0.066406  0.007812  0.003906  0.052734  0.031250   \n",
       "16   0.009766  0.062500  0.033203  0.029297  0.011719  0.044922  0.005859   \n",
       "17   0.000000  0.000000  0.001953  0.029297  0.111330  0.000000  0.000000   \n",
       "18   0.001953  0.000000  0.015625  0.031250  0.011719  0.000000  0.021484   \n",
       "19   0.005859  0.027344  0.017578  0.041016  0.007812  0.017578  0.046875   \n",
       "20   0.019531  0.019531  0.064453  0.025391  0.007812  0.005859  0.023438   \n",
       "21   0.019531  0.029297  0.080078  0.033203  0.025391  0.023438  0.009766   \n",
       "22   0.000000  0.005859  0.023438  0.001953  0.013672  0.001953  0.015625   \n",
       "23   0.029297  0.033203  0.033203  0.021484  0.001953  0.068359  0.037109   \n",
       "24   0.001953  0.023438  0.005859  0.029297  0.000000  0.000000  0.000000   \n",
       "25   0.015625  0.031250  0.101560  0.015625  0.005859  0.017578  0.000000   \n",
       "26   0.027344  0.009766  0.021484  0.042969  0.017578  0.009766  0.037109   \n",
       "27   0.085938  0.095703  0.007812  0.003906  0.000000  0.148440  0.007812   \n",
       "28   0.001953  0.003906  0.017578  0.044922  0.041016  0.011719  0.039062   \n",
       "29   0.019531  0.041016  0.093750  0.005859  0.011719  0.037109  0.035156   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "960  0.003906  0.000000  0.013672  0.029297  0.007812  0.005859  0.019531   \n",
       "961  0.000000  0.000000  0.007812  0.015625  0.072266  0.000000  0.011719   \n",
       "962  0.000000  0.017578  0.015625  0.011719  0.037109  0.003906  0.019531   \n",
       "963  0.009766  0.011719  0.121090  0.003906  0.009766  0.009766  0.007812   \n",
       "964  0.001953  0.001953  0.007812  0.019531  0.037109  0.000000  0.013672   \n",
       "965  0.003906  0.003906  0.033203  0.037109  0.003906  0.003906  0.031250   \n",
       "966  0.011719  0.005859  0.048828  0.009766  0.003906  0.046875  0.050781   \n",
       "967  0.000000  0.000000  0.005859  0.013672  0.031250  0.000000  0.011719   \n",
       "968  0.000000  0.000000  0.003906  0.039062  0.082031  0.000000  0.000000   \n",
       "969  0.025391  0.042969  0.025391  0.033203  0.001953  0.033203  0.011719   \n",
       "970  0.011719  0.013672  0.021484  0.031250  0.017578  0.003906  0.025391   \n",
       "971  0.000000  0.005859  0.072266  0.076172  0.007812  0.000000  0.000000   \n",
       "972  0.029297  0.005859  0.056641  0.031250  0.000000  0.021484  0.031250   \n",
       "973  0.011719  0.005859  0.001953  0.003906  0.011719  0.011719  0.031250   \n",
       "974  0.037109  0.056641  0.019531  0.021484  0.001953  0.068359  0.015625   \n",
       "975  0.074219  0.093750  0.007812  0.013672  0.000000  0.119140  0.003906   \n",
       "976  0.000000  0.000000  0.013672  0.015625  0.048828  0.000000  0.033203   \n",
       "977  0.001953  0.000000  0.041016  0.015625  0.015625  0.000000  0.009766   \n",
       "978  0.000000  0.000000  0.007812  0.007812  0.039062  0.000000  0.001953   \n",
       "979  0.009766  0.003906  0.025391  0.017578  0.005859  0.003906  0.035156   \n",
       "980  0.031250  0.009766  0.078125  0.017578  0.003906  0.058594  0.048828   \n",
       "981  0.027344  0.044922  0.023438  0.017578  0.001953  0.025391  0.083984   \n",
       "982  0.017578  0.074219  0.015625  0.013672  0.000000  0.074219  0.005859   \n",
       "983  0.023438  0.076172  0.015625  0.019531  0.000000  0.039062  0.011719   \n",
       "984  0.080078  0.048828  0.023438  0.007812  0.000000  0.091797  0.009766   \n",
       "985  0.060547  0.119140  0.007812  0.003906  0.000000  0.148440  0.017578   \n",
       "986  0.001953  0.003906  0.021484  0.107420  0.001953  0.000000  0.000000   \n",
       "987  0.001953  0.003906  0.000000  0.021484  0.078125  0.003906  0.007812   \n",
       "988  0.000000  0.000000  0.046875  0.056641  0.009766  0.000000  0.000000   \n",
       "989  0.023438  0.019531  0.031250  0.015625  0.005859  0.019531  0.035156   \n",
       "\n",
       "      margin8   margin9  margin10    ...      texture55  texture56  texture57  \\\n",
       "0    0.000000  0.001953  0.033203    ...       0.007812   0.000000   0.002930   \n",
       "1    0.000000  0.000000  0.007812    ...       0.000977   0.000000   0.000000   \n",
       "2    0.000000  0.000000  0.044922    ...       0.154300   0.000000   0.005859   \n",
       "3    0.000000  0.013672  0.017578    ...       0.000000   0.000977   0.000000   \n",
       "4    0.000000  0.000000  0.005859    ...       0.096680   0.000000   0.021484   \n",
       "5    0.000000  0.003906  0.027344    ...       0.145510   0.000000   0.041992   \n",
       "6    0.000000  0.003906  0.019531    ...       0.085938   0.000000   0.040039   \n",
       "7    0.000000  0.048828  0.003906    ...       0.038086   0.025391   0.009766   \n",
       "8    0.000000  0.000000  0.021484    ...       0.000000   0.000000   0.008789   \n",
       "9    0.000000  0.007812  0.001953    ...       0.000000   0.000000   0.000000   \n",
       "10   0.000000  0.011719  0.042969    ...       0.009766   0.000000   0.000000   \n",
       "11   0.000000  0.005859  0.015625    ...       0.013672   0.003906   0.014648   \n",
       "12   0.000000  0.009766  0.015625    ...       0.003906   0.000000   0.086914   \n",
       "13   0.007812  0.025391  0.015625    ...       0.002930   0.000000   0.015625   \n",
       "14   0.000000  0.001953  0.013672    ...       0.000977   0.001953   0.000000   \n",
       "15   0.000000  0.003906  0.039062    ...       0.008789   0.000000   0.004883   \n",
       "16   0.000000  0.009766  0.017578    ...       0.000000   0.000000   0.004883   \n",
       "17   0.000000  0.029297  0.000000    ...       0.012695   0.000000   0.079102   \n",
       "18   0.001953  0.005859  0.013672    ...       0.000000   0.000000   0.001953   \n",
       "19   0.001953  0.000000  0.044922    ...       0.000000   0.000977   0.000000   \n",
       "20   0.000000  0.005859  0.003906    ...       0.000000   0.000977   0.001953   \n",
       "21   0.005859  0.000000  0.009766    ...       0.021484   0.000000   0.057617   \n",
       "22   0.000000  0.003906  0.007812    ...       0.000000   0.019531   0.000000   \n",
       "23   0.000000  0.005859  0.080078    ...       0.000000   0.000000   0.000000   \n",
       "24   0.000000  0.011719  0.000000    ...       0.103520   0.009766   0.000000   \n",
       "25   0.000000  0.041016  0.005859    ...       0.029297   0.033203   0.007812   \n",
       "26   0.031250  0.005859  0.037109    ...       0.000000   0.000000   0.003906   \n",
       "27   0.000000  0.007812  0.027344    ...       0.018555   0.052734   0.005859   \n",
       "28   0.013672  0.011719  0.021484    ...       0.000000   0.000000   0.000000   \n",
       "29   0.000000  0.000000  0.048828    ...       0.076172   0.000000   0.049805   \n",
       "..        ...       ...       ...    ...            ...        ...        ...   \n",
       "960  0.000000  0.000000  0.019531    ...       0.000000   0.000000   0.013672   \n",
       "961  0.003906  0.003906  0.003906    ...       0.000000   0.000000   0.000000   \n",
       "962  0.000000  0.007812  0.005859    ...       0.000000   0.002930   0.000000   \n",
       "963  0.005859  0.000000  0.015625    ...       0.213870   0.000000   0.041016   \n",
       "964  0.000000  0.009766  0.003906    ...       0.000000   0.000000   0.003906   \n",
       "965  0.000000  0.000000  0.011719    ...       0.000000   0.000000   0.000000   \n",
       "966  0.000000  0.000000  0.048828    ...       0.000000   0.000000   0.000000   \n",
       "967  0.000000  0.001953  0.007812    ...       0.000000   0.020508   0.002930   \n",
       "968  0.000000  0.029297  0.000000    ...       0.007812   0.000000   0.144530   \n",
       "969  0.000000  0.001953  0.021484    ...       0.000000   0.147460   0.000977   \n",
       "970  0.003906  0.000000  0.015625    ...       0.000000   0.000000   0.002930   \n",
       "971  0.000000  0.005859  0.003906    ...       0.040039   0.000977   0.006836   \n",
       "972  0.003906  0.000000  0.031250    ...       0.109380   0.000000   0.013672   \n",
       "973  0.000000  0.000000  0.021484    ...       0.066406   0.000000   0.096680   \n",
       "974  0.000000  0.000000  0.007812    ...       0.046875   0.000000   0.009766   \n",
       "975  0.000000  0.000000  0.009766    ...       0.000000   0.000000   0.000000   \n",
       "976  0.000000  0.003906  0.013672    ...       0.094727   0.000000   0.016602   \n",
       "977  0.000000  0.003906  0.017578    ...       0.000000   0.000000   0.017578   \n",
       "978  0.005859  0.000000  0.003906    ...       0.000000   0.056641   0.000000   \n",
       "979  0.000000  0.000000  0.033203    ...       0.000000   0.000000   0.000000   \n",
       "980  0.000000  0.005859  0.050781    ...       0.000977   0.000000   0.024414   \n",
       "981  0.000000  0.000000  0.074219    ...       0.000000   0.000000   0.000000   \n",
       "982  0.000000  0.007812  0.001953    ...       0.025391   0.000000   0.003906   \n",
       "983  0.000000  0.005859  0.011719    ...       0.053711   0.000000   0.002930   \n",
       "984  0.000000  0.007812  0.025391    ...       0.003906   0.003906   0.041016   \n",
       "985  0.000000  0.001953  0.042969    ...       0.242190   0.000000   0.034180   \n",
       "986  0.000000  0.029297  0.003906    ...       0.170900   0.000000   0.018555   \n",
       "987  0.000000  0.003906  0.000000    ...       0.004883   0.000977   0.004883   \n",
       "988  0.000000  0.037109  0.001953    ...       0.083008   0.030273   0.000977   \n",
       "989  0.000000  0.003906  0.039062    ...       0.000000   0.000000   0.002930   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.002930   0.035156   0.000000   0.000000   0.004883   0.000000   \n",
       "1     0.000977   0.023438   0.000000   0.000000   0.000977   0.039062   \n",
       "2     0.000977   0.007812   0.000000   0.000000   0.000000   0.020508   \n",
       "3     0.000000   0.020508   0.000000   0.000000   0.017578   0.000000   \n",
       "4     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5     0.000000   0.005859   0.000000   0.000000   0.000000   0.001953   \n",
       "6     0.000000   0.009766   0.000000   0.000000   0.000000   0.039062   \n",
       "7     0.002930   0.021484   0.000000   0.037109   0.006836   0.002930   \n",
       "8     0.000000   0.017578   0.000000   0.000000   0.000977   0.033203   \n",
       "9     0.070312   0.013672   0.192380   0.000000   0.074219   0.000000   \n",
       "10    0.002930   0.024414   0.000000   0.000000   0.006836   0.000000   \n",
       "11    0.003906   0.036133   0.000000   0.000000   0.012695   0.005859   \n",
       "12    0.000000   0.013672   0.000000   0.000000   0.000000   0.011719   \n",
       "13    0.000000   0.015625   0.000000   0.000000   0.000000   0.018555   \n",
       "14    0.034180   0.007812   0.038086   0.000000   0.089844   0.023438   \n",
       "15    0.004883   0.009766   0.001953   0.000000   0.003906   0.000000   \n",
       "16    0.000000   0.068359   0.000000   0.000000   0.000000   0.036133   \n",
       "17    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "18    0.005859   0.049805   0.000000   0.000000   0.006836   0.000000   \n",
       "19    0.020508   0.001953   0.333010   0.000000   0.169920   0.000000   \n",
       "20    0.016602   0.023438   0.004883   0.000000   0.003906   0.013672   \n",
       "21    0.000000   0.002930   0.000000   0.000000   0.000000   0.000000   \n",
       "22    0.031250   0.005859   0.054688   0.000000   0.103520   0.000977   \n",
       "23    0.044922   0.003906   0.004883   0.000000   0.069336   0.000000   \n",
       "24    0.002930   0.002930   0.077148   0.000000   0.052734   0.000000   \n",
       "25    0.006836   0.014648   0.000000   0.000000   0.020508   0.000000   \n",
       "26    0.000977   0.009766   0.001953   0.000000   0.001953   0.023438   \n",
       "27    0.000977   0.019531   0.000000   0.011719   0.041992   0.004883   \n",
       "28    0.000977   0.007812   0.000000   0.000000   0.007812   0.024414   \n",
       "29    0.000000   0.000977   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "960   0.000000   0.019531   0.000000   0.000000   0.003906   0.029297   \n",
       "961   0.041016   0.057617   0.011719   0.000000   0.010742   0.000000   \n",
       "962   0.015625   0.029297   0.000000   0.000000   0.030273   0.000000   \n",
       "963   0.000000   0.003906   0.000000   0.000000   0.000000   0.000000   \n",
       "964   0.000977   0.039062   0.000000   0.000000   0.000000   0.022461   \n",
       "965   0.011719   0.028320   0.000000   0.000000   0.011719   0.008789   \n",
       "966   0.039062   0.000977   0.008789   0.000000   0.032227   0.000000   \n",
       "967   0.009766   0.026367   0.000000   0.000000   0.009766   0.011719   \n",
       "968   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "969   0.018555   0.008789   0.009766   0.000000   0.078125   0.022461   \n",
       "970   0.000000   0.019531   0.000000   0.000000   0.001953   0.010742   \n",
       "971   0.002930   0.022461   0.000000   0.082031   0.001953   0.005859   \n",
       "972   0.004883   0.018555   0.000000   0.000000   0.001953   0.000977   \n",
       "973   0.000000   0.006836   0.000000   0.000000   0.000000   0.030273   \n",
       "974   0.000000   0.010742   0.000000   0.000977   0.005859   0.011719   \n",
       "975   0.005859   0.007812   0.000000   0.000000   0.030273   0.012695   \n",
       "976   0.000000   0.018555   0.000000   0.000000   0.000977   0.028320   \n",
       "977   0.011719   0.070312   0.000000   0.000000   0.000977   0.000000   \n",
       "978   0.035156   0.011719   0.024414   0.000000   0.066406   0.013672   \n",
       "979   0.008789   0.016602   0.000000   0.000000   0.024414   0.037109   \n",
       "980   0.006836   0.017578   0.000977   0.000000   0.016602   0.000000   \n",
       "981   0.036133   0.082031   0.000000   0.000000   0.026367   0.008789   \n",
       "982   0.000000   0.009766   0.000000   0.012695   0.000000   0.009766   \n",
       "983   0.000977   0.005859   0.000000   0.081055   0.000000   0.003906   \n",
       "984   0.006836   0.017578   0.000000   0.000000   0.003906   0.000977   \n",
       "985   0.000000   0.010742   0.000000   0.000000   0.000000   0.000000   \n",
       "986   0.000000   0.011719   0.000000   0.000000   0.000977   0.000000   \n",
       "987   0.027344   0.016602   0.007812   0.000000   0.027344   0.000000   \n",
       "988   0.002930   0.014648   0.000000   0.041992   0.000000   0.001953   \n",
       "989   0.000000   0.012695   0.000000   0.000000   0.023438   0.025391   \n",
       "\n",
       "     texture64  \n",
       "0     0.025391  \n",
       "1     0.022461  \n",
       "2     0.002930  \n",
       "3     0.047852  \n",
       "4     0.031250  \n",
       "5     0.013672  \n",
       "6     0.003906  \n",
       "7     0.036133  \n",
       "8     0.074219  \n",
       "9     0.000000  \n",
       "10    0.004883  \n",
       "11    0.000000  \n",
       "12    0.002930  \n",
       "13    0.022461  \n",
       "14    0.001953  \n",
       "15    0.036133  \n",
       "16    0.000977  \n",
       "17    0.019531  \n",
       "18    0.001953  \n",
       "19    0.000000  \n",
       "20    0.004883  \n",
       "21    0.035156  \n",
       "22    0.000000  \n",
       "23    0.000000  \n",
       "24    0.000000  \n",
       "25    0.035156  \n",
       "26    0.007812  \n",
       "27    0.020508  \n",
       "28    0.015625  \n",
       "29    0.001953  \n",
       "..         ...  \n",
       "960   0.055664  \n",
       "961   0.002930  \n",
       "962   0.012695  \n",
       "963   0.014648  \n",
       "964   0.033203  \n",
       "965   0.000000  \n",
       "966   0.000977  \n",
       "967   0.041016  \n",
       "968   0.010742  \n",
       "969   0.000000  \n",
       "970   0.067383  \n",
       "971   0.029297  \n",
       "972   0.004883  \n",
       "973   0.011719  \n",
       "974   0.002930  \n",
       "975   0.000977  \n",
       "976   0.000000  \n",
       "977   0.012695  \n",
       "978   0.000000  \n",
       "979   0.003906  \n",
       "980   0.041016  \n",
       "981   0.000000  \n",
       "982   0.000000  \n",
       "983   0.019531  \n",
       "984   0.032227  \n",
       "985   0.018555  \n",
       "986   0.021484  \n",
       "987   0.001953  \n",
       "988   0.002930  \n",
       "989   0.022461  \n",
       "\n",
       "[990 rows x 192 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y_train = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X_train = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 49, 65, 94, 84, 40, 54, 78, 53, 89, 98, 16, 74, 50, 58, 31, 43,\n",
       "        4, 75, 44, 83, 84, 13, 66, 15,  6, 73, 22, 73, 31, 36, 27, 94, 88,\n",
       "       12, 28, 21, 25, 20, 60, 84, 65, 69, 58, 23, 76, 18, 52, 54,  9, 48,\n",
       "       47, 64, 81, 83, 36, 58, 21, 81, 20, 62, 88, 34, 92, 79, 82, 20, 32,\n",
       "        4, 84, 36, 35, 72, 60, 71, 72, 52, 50, 54, 11, 51, 18, 47,  5,  8,\n",
       "       37, 97, 20, 33,  1, 59,  1, 56,  1,  9, 57, 20, 79, 29, 16, 32, 54,\n",
       "       93, 10, 46, 59, 84, 76, 15, 10, 15,  0, 69,  4, 51, 51, 94, 36, 39,\n",
       "       62,  2, 24, 26, 35, 25, 87,  0, 55, 34, 38,  1, 45,  7, 93, 56, 38,\n",
       "       21, 51, 75, 81, 74, 33, 20, 37,  9, 40, 60, 31, 83, 50, 71, 67, 30,\n",
       "       66,  1, 43, 61, 23, 65, 84, 87, 46, 57, 16,  2, 28, 12, 96, 44, 76,\n",
       "       29, 75, 41, 87, 67, 61, 30,  5, 12, 62,  3, 83, 81,  6, 85,  4, 37,\n",
       "       57, 84, 39, 71, 61,  6, 76, 14, 31, 98, 40, 17, 51, 16, 42, 63, 86,\n",
       "       37, 69, 86, 71, 80, 78, 14, 35, 25,  5, 39,  8,  9, 26, 44, 60, 13,\n",
       "       14, 77, 13, 80, 87, 18, 60, 78, 92, 51, 45, 78, 41, 51, 30, 14, 35,\n",
       "       46, 21,  8,  6, 92, 38, 40, 15, 32, 17, 93, 71, 92, 27, 78, 15, 19,\n",
       "       60, 21, 38, 36, 49, 74, 67, 95, 31, 82, 45, 16, 83, 63, 80, 42, 22,\n",
       "       74, 53, 15, 44, 47, 57, 94, 76, 17, 32, 24, 15, 93, 24, 80, 59, 46,\n",
       "       12, 51, 77, 79, 70, 69, 16,  2, 63, 83, 55, 12, 53,  1, 67,  0,  2,\n",
       "       36, 42, 10,  9, 52, 59,  6, 22, 86, 31, 51, 37, 43, 75, 90, 24, 86,\n",
       "       96, 45, 32, 98, 36, 66, 48, 73, 73, 79, 56, 41, 21, 25, 27, 97, 18,\n",
       "       44, 45, 40, 80, 63, 20, 35,  0,  8, 27, 25, 35, 59, 61, 21, 37, 29,\n",
       "        6, 19, 78, 50, 54, 37, 93, 33, 46, 79, 59, 29, 43,  0, 23, 17, 38,\n",
       "       66, 38, 89, 17, 25, 31, 65, 10, 26, 86, 58, 42, 46, 24, 95, 93,  8,\n",
       "       53, 32, 14, 10, 94,  8,  8, 64, 44, 74, 30, 97, 22, 11, 68, 56, 90,\n",
       "       96, 16, 43, 57, 91, 24, 28, 82, 90, 64, 61, 92, 28, 84, 70, 45, 85,\n",
       "       34,  7, 88, 89, 61, 26, 88, 41, 46,  8, 91, 41, 14, 98, 28, 26, 36,\n",
       "       70, 74,  7, 52, 70, 42, 66, 22, 13, 44, 91, 53, 22, 16, 40, 40, 28,\n",
       "       70,  6, 60, 95, 23, 16, 50, 29, 49,  9, 18, 55, 63, 60, 19, 28, 30,\n",
       "       31, 85, 66, 88, 63, 83, 64, 96, 13, 34, 27, 95, 36, 72, 29, 91, 22,\n",
       "       65, 71, 66, 11, 32,  2, 75, 39,  5, 37, 67, 81, 55, 61, 57, 81, 82,\n",
       "       63, 55, 54, 35, 86, 25, 24, 96, 10, 58, 59, 28, 89, 54, 52, 85, 68,\n",
       "       69,  8, 39, 95, 39, 82, 48, 74, 52, 74, 55,  9, 47, 84, 91, 12, 96,\n",
       "       82, 64,  7, 40, 73, 77, 11, 36, 68, 23, 28, 46, 75, 43,  2, 11, 47,\n",
       "       53, 56, 62, 62, 80, 56, 30,  3, 88, 37, 33, 73, 76, 21,  5, 76, 87,\n",
       "       68, 83, 62, 57, 47, 19, 88, 96, 42, 23, 44, 87, 82, 49, 63, 24, 94,\n",
       "       69, 54,  5, 79, 43, 12, 50,  5, 52, 92,  4, 84,  1, 33, 49, 26, 18,\n",
       "       44, 13, 24, 73, 89, 78, 67, 41, 11, 46, 47, 69,  0, 18, 98, 44, 85,\n",
       "       29, 53,  1, 45,  3,  9, 13,  2, 66, 59, 79,  6, 17, 43, 83, 26,  1,\n",
       "       12, 49, 71, 89, 58, 93, 39, 42, 15, 38, 55, 15, 93,  4, 90, 88, 55,\n",
       "       40, 55, 17, 34, 94, 57, 92, 81, 26, 60, 89, 49, 89, 30, 65, 58,  4,\n",
       "       19,  4, 76, 74, 71, 21, 54, 13, 16, 72, 68, 62, 61, 25, 72,  7, 12,\n",
       "       18, 77, 90, 62, 14,  3, 78, 65, 37, 27, 50, 95, 98, 60, 72, 58, 38,\n",
       "       87, 93, 19,  7, 83, 50,  3, 91, 77,  7, 64, 61, 69, 23, 76, 65, 48,\n",
       "       41, 92, 20, 91, 18, 70,  9,  9, 29, 85, 67,  0, 35, 98, 91, 90, 31,\n",
       "       53, 39, 24, 85, 96, 17,  7, 11, 96, 39, 56, 90, 79, 45, 64, 97, 41,\n",
       "       19, 74, 11, 10, 62, 95, 28, 96, 10,  7, 68,  7, 93, 34, 42, 68, 41,\n",
       "       14, 22, 58, 12, 71, 27, 98, 72, 91,  3, 43, 19, 61, 75, 20, 81, 63,\n",
       "       67, 56, 26, 47, 11, 31, 57, 62, 66, 19, 75, 97, 94, 13, 75, 95, 32,\n",
       "       50, 97, 52, 87, 32,  3, 47, 77, 48, 33, 73, 64, 49, 68, 43, 94, 77,\n",
       "       68, 47, 82,  2, 30, 23, 33, 34, 66, 33, 35, 88, 68, 27, 87, 54, 79,\n",
       "       34, 67, 65, 18,  4, 26, 30, 52, 86,  0, 29, 80, 67, 95, 39, 25, 70,\n",
       "       58, 35, 27, 17, 38, 91, 13, 23, 77, 79, 77, 22, 49, 98, 48, 46, 48,\n",
       "        5, 63, 97, 80, 53, 20, 25, 78, 10, 65, 33, 41, 85, 90, 98, 97, 71,\n",
       "       95, 52,  3, 29, 69, 51, 70, 27, 22, 34,  6, 48, 72, 21, 89, 17, 97,\n",
       "       72, 80, 10, 57, 64, 92, 38, 15, 73, 87, 73, 48, 42, 82, 33, 56,  3,\n",
       "       42,  1, 53, 55, 90, 19,  6, 30, 86, 64, 49,  2,  8, 45, 76, 92,  0,\n",
       "       23, 69, 59, 80, 90, 32,  5, 59, 85, 89, 94, 45, 48, 86, 81, 14,  4,\n",
       "       77, 56, 82,  2, 85, 70, 88,  0, 75, 14, 86, 81, 97, 70, 72, 34, 40,\n",
       "        5, 11, 78, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [49], real class 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y_train = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X_train = train.values\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[1,:]),y_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for each class [[ 0.01091786  0.01042494  0.00935513  0.01029758  0.00984497  0.00891128\n",
      "   0.00920131  0.01139842  0.01068972  0.01053185  0.01181694  0.01174732\n",
      "   0.01106921  0.01096111  0.01144747  0.00814812  0.01130018  0.00982344\n",
      "   0.01200358  0.01023216  0.01071545  0.01030047  0.00895939  0.00883382\n",
      "   0.01003238  0.00859203  0.01111046  0.00948944  0.00891398  0.00843461\n",
      "   0.00903262  0.00977109  0.00849246  0.00799346  0.00871169  0.00919821\n",
      "   0.00934749  0.00775337  0.00844684  0.00812858  0.00786713  0.01119368\n",
      "   0.00990995  0.0111045   0.00883244  0.01010551  0.01074626  0.01142711\n",
      "   0.01308516  0.01414812  0.01026451  0.00964398  0.01058022  0.01116527\n",
      "   0.00976365  0.00995305  0.01039983  0.00757312  0.01082281  0.00967711\n",
      "   0.01052349  0.01006014  0.00897279  0.01000533  0.009439    0.00973575\n",
      "   0.00922029  0.00908074  0.01161337  0.01025971  0.00903077  0.01013524\n",
      "   0.00847725  0.01180713  0.010618    0.01161295  0.01006097  0.0092202\n",
      "   0.00935672  0.01013467  0.00986705  0.01024985  0.00962547  0.01043564\n",
      "   0.00957214  0.0107744   0.01153057  0.00947285  0.00930663  0.01185045\n",
      "   0.00997555  0.01075785  0.01226149  0.01109386  0.01142606  0.01133568\n",
      "   0.00995917  0.00982359  0.01067333]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[1,:]),y_train[1]))\n",
    "print ('Probabilities for each class %s'% logistic.predict_proba(X_train[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [3], real class 3\n",
      "Probabilities for each class [[ 0.01040101  0.00980071  0.00930261  0.0119656   0.00824518  0.00900574\n",
      "   0.00958055  0.0099081   0.00977108  0.0097681   0.01088873  0.00963703\n",
      "   0.01082692  0.00968351  0.00974604  0.00979536  0.00969883  0.00910111\n",
      "   0.01067259  0.0105475   0.01018988  0.01025222  0.00989487  0.00956956\n",
      "   0.01046216  0.00925991  0.01044268  0.00943666  0.01033588  0.00968172\n",
      "   0.01035773  0.00979144  0.01059151  0.00867763  0.00974851  0.00834971\n",
      "   0.01025594  0.00890292  0.00945386  0.00917637  0.0086171   0.01098779\n",
      "   0.01058855  0.01058652  0.00870087  0.01056001  0.01125951  0.01035575\n",
      "   0.01084336  0.01009571  0.01171532  0.01099434  0.01188176  0.01155794\n",
      "   0.01020917  0.01113596  0.01103031  0.00800876  0.0099192   0.00927695\n",
      "   0.0108405   0.00964301  0.00938307  0.00938283  0.011426    0.01035049\n",
      "   0.00955401  0.01013896  0.01084438  0.01083521  0.00935672  0.00916561\n",
      "   0.00888153  0.01033461  0.01009434  0.01174393  0.01030758  0.00927835\n",
      "   0.00912696  0.01072552  0.00935823  0.0111868   0.00928928  0.01190482\n",
      "   0.00996878  0.01111771  0.01117445  0.01049839  0.00882207  0.00984198\n",
      "   0.01063725  0.01023572  0.01060147  0.01037721  0.01122768  0.00924243\n",
      "   0.01043544  0.00990702  0.0112573 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[0,:]),y_train[0]))\n",
    "print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679797979798\n"
     ]
    }
   ],
   "source": [
    "print(logistic.score (X_train ,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-64170db19d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted class %s, real class %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'C'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y_train = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X_train = train.values\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train,y_train,C=1.5,max_iter=400)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[0,:]),y_train[0]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [3], real class 3\n",
      "0.712121212121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y_train = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X_train = train.values\n",
    "logistic = LogisticRegression(C=1.5,max_iter=400)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[0,:]),y_train[0]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [3], real class 3\n",
      "0.712121212121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y_train = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X_train = train.values\n",
    "logistic = LogisticRegression(C=1.5,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[0,:]),y_train[0]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [3], real class 3\n",
      "0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y_train = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X = train.values\n",
    "logistic = LogisticRegression(C=8,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_train[0,:]),y_train[0]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perm_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fc89330765ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1236\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perm_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id','species'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1236)\n",
    "np.random.shuffle(perm_data)\n",
    "X_train = x[:600,:]\n",
    "X_test = x[600:,:]\n",
    "y_train = y[:600]\n",
    "y_test = y[600:]\n",
    "\n",
    "logistic = LogisticRegression(C=8,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[601,:]),y_test[601]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.00000000e+00,   7.81200000e-03,   2.34380000e-02, ...,\n",
       "          4.88300000e-03,   0.00000000e+00,   2.53910000e-02],\n",
       "       [  4.90000000e+01,   5.85900000e-03,   0.00000000e+00, ...,\n",
       "          9.77000000e-04,   3.90620000e-02,   2.24610000e-02],\n",
       "       [  6.50000000e+01,   5.85900000e-03,   9.76600000e-03, ...,\n",
       "          0.00000000e+00,   2.05080000e-02,   2.93000000e-03],\n",
       "       ..., \n",
       "       [  1.10000000e+01,   1.95300000e-03,   3.90600000e-03, ...,\n",
       "          2.73440000e-02,   0.00000000e+00,   1.95300000e-03],\n",
       "       [  7.80000000e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.95300000e-03,   2.93000000e-03],\n",
       "       [  5.00000000e+01,   2.34380000e-02,   1.95310000e-02, ...,\n",
       "          2.34380000e-02,   2.53910000e-02,   2.24610000e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.192380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>98</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>43</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>75</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.333010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>83</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>84</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>66</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>73</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>22</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>73</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>85</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.055664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>45</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.121090</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>48</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>86</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>81</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>77</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147460</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>56</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>2</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>85</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>70</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>88</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>75</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>86</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>81</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>97</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>70</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>72</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>34</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>40</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>5</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>50</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     species   margin1   margin2   margin3   margin4   margin5   margin6  \\\n",
       "0          3  0.007812  0.023438  0.023438  0.003906  0.011719  0.009766   \n",
       "1         49  0.005859  0.000000  0.031250  0.015625  0.025391  0.001953   \n",
       "2         65  0.005859  0.009766  0.019531  0.007812  0.003906  0.005859   \n",
       "3         94  0.000000  0.003906  0.023438  0.005859  0.021484  0.019531   \n",
       "4         84  0.005859  0.003906  0.048828  0.009766  0.013672  0.015625   \n",
       "5         40  0.070312  0.093750  0.033203  0.001953  0.000000  0.152340   \n",
       "6         54  0.021484  0.031250  0.017578  0.009766  0.001953  0.042969   \n",
       "7         78  0.000000  0.000000  0.037109  0.050781  0.003906  0.000000   \n",
       "8         53  0.005859  0.001953  0.033203  0.015625  0.001953  0.000000   \n",
       "9         89  0.000000  0.000000  0.009766  0.037109  0.072266  0.000000   \n",
       "10        98  0.019531  0.031250  0.001953  0.005859  0.003906  0.013672   \n",
       "11        16  0.001953  0.001953  0.023438  0.025391  0.076172  0.000000   \n",
       "12        74  0.015625  0.011719  0.041016  0.003906  0.023438  0.015625   \n",
       "13        50  0.011719  0.011719  0.054688  0.017578  0.007812  0.009766   \n",
       "14        58  0.011719  0.007812  0.111330  0.027344  0.023438  0.039062   \n",
       "15        31  0.027344  0.025391  0.066406  0.007812  0.003906  0.052734   \n",
       "16        43  0.009766  0.062500  0.033203  0.029297  0.011719  0.044922   \n",
       "17         4  0.000000  0.000000  0.001953  0.029297  0.111330  0.000000   \n",
       "18        75  0.001953  0.000000  0.015625  0.031250  0.011719  0.000000   \n",
       "19        44  0.005859  0.027344  0.017578  0.041016  0.007812  0.017578   \n",
       "20        83  0.019531  0.019531  0.064453  0.025391  0.007812  0.005859   \n",
       "21        84  0.019531  0.029297  0.080078  0.033203  0.025391  0.023438   \n",
       "22        13  0.000000  0.005859  0.023438  0.001953  0.013672  0.001953   \n",
       "23        66  0.029297  0.033203  0.033203  0.021484  0.001953  0.068359   \n",
       "24        15  0.001953  0.023438  0.005859  0.029297  0.000000  0.000000   \n",
       "25         6  0.015625  0.031250  0.101560  0.015625  0.005859  0.017578   \n",
       "26        73  0.027344  0.009766  0.021484  0.042969  0.017578  0.009766   \n",
       "27        22  0.085938  0.095703  0.007812  0.003906  0.000000  0.148440   \n",
       "28        73  0.001953  0.003906  0.017578  0.044922  0.041016  0.011719   \n",
       "29        31  0.019531  0.041016  0.093750  0.005859  0.011719  0.037109   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "960       85  0.003906  0.000000  0.013672  0.029297  0.007812  0.005859   \n",
       "961       89  0.000000  0.000000  0.007812  0.015625  0.072266  0.000000   \n",
       "962       94  0.000000  0.017578  0.015625  0.011719  0.037109  0.003906   \n",
       "963       45  0.009766  0.011719  0.121090  0.003906  0.009766  0.009766   \n",
       "964       48  0.001953  0.001953  0.007812  0.019531  0.037109  0.000000   \n",
       "965       86  0.003906  0.003906  0.033203  0.037109  0.003906  0.003906   \n",
       "966       81  0.011719  0.005859  0.048828  0.009766  0.003906  0.046875   \n",
       "967       14  0.000000  0.000000  0.005859  0.013672  0.031250  0.000000   \n",
       "968        4  0.000000  0.000000  0.003906  0.039062  0.082031  0.000000   \n",
       "969       77  0.025391  0.042969  0.025391  0.033203  0.001953  0.033203   \n",
       "970       56  0.011719  0.013672  0.021484  0.031250  0.017578  0.003906   \n",
       "971       82  0.000000  0.005859  0.072266  0.076172  0.007812  0.000000   \n",
       "972        2  0.029297  0.005859  0.056641  0.031250  0.000000  0.021484   \n",
       "973       85  0.011719  0.005859  0.001953  0.003906  0.011719  0.011719   \n",
       "974       70  0.037109  0.056641  0.019531  0.021484  0.001953  0.068359   \n",
       "975       88  0.074219  0.093750  0.007812  0.013672  0.000000  0.119140   \n",
       "976        0  0.000000  0.000000  0.013672  0.015625  0.048828  0.000000   \n",
       "977       75  0.001953  0.000000  0.041016  0.015625  0.015625  0.000000   \n",
       "978       14  0.000000  0.000000  0.007812  0.007812  0.039062  0.000000   \n",
       "979       86  0.009766  0.003906  0.025391  0.017578  0.005859  0.003906   \n",
       "980       81  0.031250  0.009766  0.078125  0.017578  0.003906  0.058594   \n",
       "981       97  0.027344  0.044922  0.023438  0.017578  0.001953  0.025391   \n",
       "982       70  0.017578  0.074219  0.015625  0.013672  0.000000  0.074219   \n",
       "983       72  0.023438  0.076172  0.015625  0.019531  0.000000  0.039062   \n",
       "984       34  0.080078  0.048828  0.023438  0.007812  0.000000  0.091797   \n",
       "985       40  0.060547  0.119140  0.007812  0.003906  0.000000  0.148440   \n",
       "986        5  0.001953  0.003906  0.021484  0.107420  0.001953  0.000000   \n",
       "987       11  0.001953  0.003906  0.000000  0.021484  0.078125  0.003906   \n",
       "988       78  0.000000  0.000000  0.046875  0.056641  0.009766  0.000000   \n",
       "989       50  0.023438  0.019531  0.031250  0.015625  0.005859  0.019531   \n",
       "\n",
       "      margin7   margin8   margin9    ...      texture55  texture56  texture57  \\\n",
       "0    0.027344  0.000000  0.001953    ...       0.007812   0.000000   0.002930   \n",
       "1    0.019531  0.000000  0.000000    ...       0.000977   0.000000   0.000000   \n",
       "2    0.068359  0.000000  0.000000    ...       0.154300   0.000000   0.005859   \n",
       "3    0.023438  0.000000  0.013672    ...       0.000000   0.000977   0.000000   \n",
       "4    0.005859  0.000000  0.000000    ...       0.096680   0.000000   0.021484   \n",
       "5    0.007812  0.000000  0.003906    ...       0.145510   0.000000   0.041992   \n",
       "6    0.039062  0.000000  0.003906    ...       0.085938   0.000000   0.040039   \n",
       "7    0.003906  0.000000  0.048828    ...       0.038086   0.025391   0.009766   \n",
       "8    0.023438  0.000000  0.000000    ...       0.000000   0.000000   0.008789   \n",
       "9    0.000000  0.000000  0.007812    ...       0.000000   0.000000   0.000000   \n",
       "10   0.033203  0.000000  0.011719    ...       0.009766   0.000000   0.000000   \n",
       "11   0.029297  0.000000  0.005859    ...       0.013672   0.003906   0.014648   \n",
       "12   0.019531  0.000000  0.009766    ...       0.003906   0.000000   0.086914   \n",
       "13   0.011719  0.007812  0.025391    ...       0.002930   0.000000   0.015625   \n",
       "14   0.013672  0.000000  0.001953    ...       0.000977   0.001953   0.000000   \n",
       "15   0.031250  0.000000  0.003906    ...       0.008789   0.000000   0.004883   \n",
       "16   0.005859  0.000000  0.009766    ...       0.000000   0.000000   0.004883   \n",
       "17   0.000000  0.000000  0.029297    ...       0.012695   0.000000   0.079102   \n",
       "18   0.021484  0.001953  0.005859    ...       0.000000   0.000000   0.001953   \n",
       "19   0.046875  0.001953  0.000000    ...       0.000000   0.000977   0.000000   \n",
       "20   0.023438  0.000000  0.005859    ...       0.000000   0.000977   0.001953   \n",
       "21   0.009766  0.005859  0.000000    ...       0.021484   0.000000   0.057617   \n",
       "22   0.015625  0.000000  0.003906    ...       0.000000   0.019531   0.000000   \n",
       "23   0.037109  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "24   0.000000  0.000000  0.011719    ...       0.103520   0.009766   0.000000   \n",
       "25   0.000000  0.000000  0.041016    ...       0.029297   0.033203   0.007812   \n",
       "26   0.037109  0.031250  0.005859    ...       0.000000   0.000000   0.003906   \n",
       "27   0.007812  0.000000  0.007812    ...       0.018555   0.052734   0.005859   \n",
       "28   0.039062  0.013672  0.011719    ...       0.000000   0.000000   0.000000   \n",
       "29   0.035156  0.000000  0.000000    ...       0.076172   0.000000   0.049805   \n",
       "..        ...       ...       ...    ...            ...        ...        ...   \n",
       "960  0.019531  0.000000  0.000000    ...       0.000000   0.000000   0.013672   \n",
       "961  0.011719  0.003906  0.003906    ...       0.000000   0.000000   0.000000   \n",
       "962  0.019531  0.000000  0.007812    ...       0.000000   0.002930   0.000000   \n",
       "963  0.007812  0.005859  0.000000    ...       0.213870   0.000000   0.041016   \n",
       "964  0.013672  0.000000  0.009766    ...       0.000000   0.000000   0.003906   \n",
       "965  0.031250  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "966  0.050781  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "967  0.011719  0.000000  0.001953    ...       0.000000   0.020508   0.002930   \n",
       "968  0.000000  0.000000  0.029297    ...       0.007812   0.000000   0.144530   \n",
       "969  0.011719  0.000000  0.001953    ...       0.000000   0.147460   0.000977   \n",
       "970  0.025391  0.003906  0.000000    ...       0.000000   0.000000   0.002930   \n",
       "971  0.000000  0.000000  0.005859    ...       0.040039   0.000977   0.006836   \n",
       "972  0.031250  0.003906  0.000000    ...       0.109380   0.000000   0.013672   \n",
       "973  0.031250  0.000000  0.000000    ...       0.066406   0.000000   0.096680   \n",
       "974  0.015625  0.000000  0.000000    ...       0.046875   0.000000   0.009766   \n",
       "975  0.003906  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "976  0.033203  0.000000  0.003906    ...       0.094727   0.000000   0.016602   \n",
       "977  0.009766  0.000000  0.003906    ...       0.000000   0.000000   0.017578   \n",
       "978  0.001953  0.005859  0.000000    ...       0.000000   0.056641   0.000000   \n",
       "979  0.035156  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "980  0.048828  0.000000  0.005859    ...       0.000977   0.000000   0.024414   \n",
       "981  0.083984  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "982  0.005859  0.000000  0.007812    ...       0.025391   0.000000   0.003906   \n",
       "983  0.011719  0.000000  0.005859    ...       0.053711   0.000000   0.002930   \n",
       "984  0.009766  0.000000  0.007812    ...       0.003906   0.003906   0.041016   \n",
       "985  0.017578  0.000000  0.001953    ...       0.242190   0.000000   0.034180   \n",
       "986  0.000000  0.000000  0.029297    ...       0.170900   0.000000   0.018555   \n",
       "987  0.007812  0.000000  0.003906    ...       0.004883   0.000977   0.004883   \n",
       "988  0.000000  0.000000  0.037109    ...       0.083008   0.030273   0.000977   \n",
       "989  0.035156  0.000000  0.003906    ...       0.000000   0.000000   0.002930   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.002930   0.035156   0.000000   0.000000   0.004883   0.000000   \n",
       "1     0.000977   0.023438   0.000000   0.000000   0.000977   0.039062   \n",
       "2     0.000977   0.007812   0.000000   0.000000   0.000000   0.020508   \n",
       "3     0.000000   0.020508   0.000000   0.000000   0.017578   0.000000   \n",
       "4     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5     0.000000   0.005859   0.000000   0.000000   0.000000   0.001953   \n",
       "6     0.000000   0.009766   0.000000   0.000000   0.000000   0.039062   \n",
       "7     0.002930   0.021484   0.000000   0.037109   0.006836   0.002930   \n",
       "8     0.000000   0.017578   0.000000   0.000000   0.000977   0.033203   \n",
       "9     0.070312   0.013672   0.192380   0.000000   0.074219   0.000000   \n",
       "10    0.002930   0.024414   0.000000   0.000000   0.006836   0.000000   \n",
       "11    0.003906   0.036133   0.000000   0.000000   0.012695   0.005859   \n",
       "12    0.000000   0.013672   0.000000   0.000000   0.000000   0.011719   \n",
       "13    0.000000   0.015625   0.000000   0.000000   0.000000   0.018555   \n",
       "14    0.034180   0.007812   0.038086   0.000000   0.089844   0.023438   \n",
       "15    0.004883   0.009766   0.001953   0.000000   0.003906   0.000000   \n",
       "16    0.000000   0.068359   0.000000   0.000000   0.000000   0.036133   \n",
       "17    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "18    0.005859   0.049805   0.000000   0.000000   0.006836   0.000000   \n",
       "19    0.020508   0.001953   0.333010   0.000000   0.169920   0.000000   \n",
       "20    0.016602   0.023438   0.004883   0.000000   0.003906   0.013672   \n",
       "21    0.000000   0.002930   0.000000   0.000000   0.000000   0.000000   \n",
       "22    0.031250   0.005859   0.054688   0.000000   0.103520   0.000977   \n",
       "23    0.044922   0.003906   0.004883   0.000000   0.069336   0.000000   \n",
       "24    0.002930   0.002930   0.077148   0.000000   0.052734   0.000000   \n",
       "25    0.006836   0.014648   0.000000   0.000000   0.020508   0.000000   \n",
       "26    0.000977   0.009766   0.001953   0.000000   0.001953   0.023438   \n",
       "27    0.000977   0.019531   0.000000   0.011719   0.041992   0.004883   \n",
       "28    0.000977   0.007812   0.000000   0.000000   0.007812   0.024414   \n",
       "29    0.000000   0.000977   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "960   0.000000   0.019531   0.000000   0.000000   0.003906   0.029297   \n",
       "961   0.041016   0.057617   0.011719   0.000000   0.010742   0.000000   \n",
       "962   0.015625   0.029297   0.000000   0.000000   0.030273   0.000000   \n",
       "963   0.000000   0.003906   0.000000   0.000000   0.000000   0.000000   \n",
       "964   0.000977   0.039062   0.000000   0.000000   0.000000   0.022461   \n",
       "965   0.011719   0.028320   0.000000   0.000000   0.011719   0.008789   \n",
       "966   0.039062   0.000977   0.008789   0.000000   0.032227   0.000000   \n",
       "967   0.009766   0.026367   0.000000   0.000000   0.009766   0.011719   \n",
       "968   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "969   0.018555   0.008789   0.009766   0.000000   0.078125   0.022461   \n",
       "970   0.000000   0.019531   0.000000   0.000000   0.001953   0.010742   \n",
       "971   0.002930   0.022461   0.000000   0.082031   0.001953   0.005859   \n",
       "972   0.004883   0.018555   0.000000   0.000000   0.001953   0.000977   \n",
       "973   0.000000   0.006836   0.000000   0.000000   0.000000   0.030273   \n",
       "974   0.000000   0.010742   0.000000   0.000977   0.005859   0.011719   \n",
       "975   0.005859   0.007812   0.000000   0.000000   0.030273   0.012695   \n",
       "976   0.000000   0.018555   0.000000   0.000000   0.000977   0.028320   \n",
       "977   0.011719   0.070312   0.000000   0.000000   0.000977   0.000000   \n",
       "978   0.035156   0.011719   0.024414   0.000000   0.066406   0.013672   \n",
       "979   0.008789   0.016602   0.000000   0.000000   0.024414   0.037109   \n",
       "980   0.006836   0.017578   0.000977   0.000000   0.016602   0.000000   \n",
       "981   0.036133   0.082031   0.000000   0.000000   0.026367   0.008789   \n",
       "982   0.000000   0.009766   0.000000   0.012695   0.000000   0.009766   \n",
       "983   0.000977   0.005859   0.000000   0.081055   0.000000   0.003906   \n",
       "984   0.006836   0.017578   0.000000   0.000000   0.003906   0.000977   \n",
       "985   0.000000   0.010742   0.000000   0.000000   0.000000   0.000000   \n",
       "986   0.000000   0.011719   0.000000   0.000000   0.000977   0.000000   \n",
       "987   0.027344   0.016602   0.007812   0.000000   0.027344   0.000000   \n",
       "988   0.002930   0.014648   0.000000   0.041992   0.000000   0.001953   \n",
       "989   0.000000   0.012695   0.000000   0.000000   0.023438   0.025391   \n",
       "\n",
       "     texture64  \n",
       "0     0.025391  \n",
       "1     0.022461  \n",
       "2     0.002930  \n",
       "3     0.047852  \n",
       "4     0.031250  \n",
       "5     0.013672  \n",
       "6     0.003906  \n",
       "7     0.036133  \n",
       "8     0.074219  \n",
       "9     0.000000  \n",
       "10    0.004883  \n",
       "11    0.000000  \n",
       "12    0.002930  \n",
       "13    0.022461  \n",
       "14    0.001953  \n",
       "15    0.036133  \n",
       "16    0.000977  \n",
       "17    0.019531  \n",
       "18    0.001953  \n",
       "19    0.000000  \n",
       "20    0.004883  \n",
       "21    0.035156  \n",
       "22    0.000000  \n",
       "23    0.000000  \n",
       "24    0.000000  \n",
       "25    0.035156  \n",
       "26    0.007812  \n",
       "27    0.020508  \n",
       "28    0.015625  \n",
       "29    0.001953  \n",
       "..         ...  \n",
       "960   0.055664  \n",
       "961   0.002930  \n",
       "962   0.012695  \n",
       "963   0.014648  \n",
       "964   0.033203  \n",
       "965   0.000000  \n",
       "966   0.000977  \n",
       "967   0.041016  \n",
       "968   0.010742  \n",
       "969   0.000000  \n",
       "970   0.067383  \n",
       "971   0.029297  \n",
       "972   0.004883  \n",
       "973   0.011719  \n",
       "974   0.002930  \n",
       "975   0.000977  \n",
       "976   0.000000  \n",
       "977   0.012695  \n",
       "978   0.000000  \n",
       "979   0.003906  \n",
       "980   0.041016  \n",
       "981   0.000000  \n",
       "982   0.000000  \n",
       "983   0.019531  \n",
       "984   0.032227  \n",
       "985   0.018555  \n",
       "986   0.021484  \n",
       "987   0.001953  \n",
       "988   0.002930  \n",
       "989   0.022461  \n",
       "\n",
       "[990 rows x 193 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 601 is out of bounds for axis 0 with size 390",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a4ce56b5a6b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted class %s, real class %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m601\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 601 is out of bounds for axis 0 with size 390"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1236)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=8,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[601,:]),y_test[601]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 81.], real class 81.0\n",
      "0.746666666667\n",
      "0.538461538462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1236)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=4,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 42.], real class 81.0\n",
      "0.608333333333\n",
      "0.369230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1236)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=1,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 42.], real class 81.0\n",
      "0.398333333333\n",
      "0.212820512821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1236)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=1,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 81.], real class 81.0\n",
      "0.791666666667\n",
      "0.576923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1236)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=10,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 52.], real class 75.0\n",
      "0.74\n",
      "0.469230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=10,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 52.], real class 75.0\n",
      "0.691666666667\n",
      "0.402564102564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=14,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 52.], real class 75.0\n",
      "0.813333333333\n",
      "0.553846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(C=14,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0007a233afeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Neural Network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted class %s, real class %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'C'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), C=20,max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 68.], real class 75.0\n",
      "0.0166666666667\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 75.], real class 75.0\n",
      "1.0\n",
      "0.753846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-5, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 68.], real class 75.0\n",
      "0.0166666666667\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(5, 2), max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 68.], real class 75.0\n",
      "0.0166666666667\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(5, 2), max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 75.], real class 75.0\n",
      "1.0\n",
      "0.928205128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-3, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 75.], real class 75.0\n",
      "1.0\n",
      "0.920512820513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-2, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 12.], real class 75.0\n",
      "0.123333333333\n",
      "0.0487179487179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-1, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 75.], real class 75.0\n",
      "0.991666666667\n",
      "0.825641025641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=1e-1, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 75.], real class 75.0\n",
      "0.89\n",
      "0.635897435897\n",
      "Neural Network\n",
      "Predicted class [ 75.], real class 75.0\n",
      "1.0\n",
      "0.923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1237)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.005, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 83.], real class 50.0\n",
      "0.865\n",
      "0.602564102564\n",
      "Neural Network\n",
      "Predicted class [ 50.], real class 50.0\n",
      "1.0\n",
      "0.902564102564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 83.], real class 50.0\n",
      "0.865\n",
      "0.602564102564\n",
      "Neural Network\n",
      "Predicted class [ 50.], real class 50.0\n",
      "1.0\n",
      "0.905128205128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:600,1:]\n",
    "X_test = X[600:,1:]\n",
    "y_train = X[:600,0]\n",
    "y_test = X[600: ,0]\n",
    "\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (logistic.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_test ,y_test))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.005, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s, real class %s' % (clf.predict(X_test[300,:]),y_test[300]))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_test ,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Alnus_Rubra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-16410d0d15e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted class %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    310\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Alnus_Rubra'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[600:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[600: ,0]\n",
    "\n",
    "test = pd.read_csv('train.csv')\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Alnus_Rubra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d78b2632ed73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted class %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    310\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Alnus_Rubra'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[600:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[600: ,0]\n",
    "\n",
    "test = pd.read_csv('train.csv')\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Alnus_Rubra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b0b11dd1ee25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted class '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    310\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Alnus_Rubra'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[600:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[600: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class ',logistic.predict(X_test[300,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alnus_Rubra', 0.0, 0.0, 0.023438, 0.007812, 0.027344, 0.0,\n",
       "       0.013672, 0.003906, 0.001953, 0.011719, 0.0, 0.023438, 0.001953,\n",
       "       0.0, 0.046875, 0.0, 0.009766, 0.001953, 0.001953, 0.007812,\n",
       "       0.048827999999999996, 0.019531, 0.0, 0.025391, 0.001953, 0.019531,\n",
       "       0.0, 0.019531, 0.001953, 0.025391, 0.033202999999999996, 0.011719,\n",
       "       0.0058590000000000005, 0.003906, 0.003906, 0.009766, 0.007812,\n",
       "       0.029297000000000004, 0.035156, 0.017578, 0.009766, 0.003906,\n",
       "       0.03125, 0.021484, 0.009766, 0.025391, 0.035156,\n",
       "       0.14844000000000002, 0.0058590000000000005, 0.052734, 0.003906,\n",
       "       0.001953, 0.035156, 0.009766, 0.06640599999999999, 0.021484,\n",
       "       0.001953, 0.001953, 0.003906, 0.001953, 0.0, 0.001953, 0.001953,\n",
       "       0.0, 0.00082511, 0.00088157, 0.0008839000000000001, 0.00084647,\n",
       "       0.00077918, 0.00071205, 0.00064367, 0.00057588, 0.00051378,\n",
       "       0.00045426, 0.00040683, 0.00038909, 0.00041239999999999994,\n",
       "       0.00047662, 0.00055151, 0.00062636, 0.00068368, 0.00060787,\n",
       "       0.00052907, 0.00045134, 0.00040038, 0.00039769999999999996,\n",
       "       0.00042099, 0.00046967, 0.00053512, 0.00059953, 0.00066849,\n",
       "       0.00074087, 0.00080812, 0.00086561, 0.00086289, 0.00084135,\n",
       "       0.00082706, 0.00079258, 0.00075333, 0.00073794, 0.00072356,\n",
       "       0.00067106, 0.00063123, 0.00057337, 0.00057405, 0.00056717,\n",
       "       0.00054535, 0.00053224, 0.00052289, 0.00048168, 0.00040009,\n",
       "       0.00032171, 0.00031156, 0.00038471, 0.00046053, 0.00051354,\n",
       "       0.00052353, 0.0005056, 0.00052108, 0.00054144, 0.00058298,\n",
       "       0.00060466, 0.0006355, 0.00066259, 0.00071439, 0.00074867,\n",
       "       0.0007714999999999999, 0.00079383, 0.0, 0.003906, 0.036133,\n",
       "       0.011719, 0.0, 0.0, 0.004883, 0.007812, 0.001953, 0.001953,\n",
       "       0.053711, 0.0, 0.004883, 0.024413999999999998, 0.0, 0.007812, 0.0,\n",
       "       0.004883, 0.007812, 0.010742, 0.0, 0.1084, 0.0058590000000000005,\n",
       "       0.000977, 0.07421900000000001, 0.001953, 0.001953, 0.006836,\n",
       "       0.06347699999999999, 0.0058590000000000005, 0.021484, 0.015625, 0.0,\n",
       "       0.0, 0.004883, 0.0, 0.0, 0.001953, 0.008789, 0.007812, 0.0,\n",
       "       0.011719, 0.023438, 0.0, 0.009766, 0.0, 0.08203099999999999, 0.0,\n",
       "       0.019531, 0.059570000000000005, 0.083984, 0.043945, 0.014648,\n",
       "       0.011719, 0.0, 0.0058590000000000005, 0.004883,\n",
       "       0.020508000000000002, 0.048827999999999996, 0.0, 0.0, 0.010742,\n",
       "       0.012695, 0.023438], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[300,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.959595959596\n",
      "0.951282051282\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n",
      "Probabilities for each class [[  3.99425231e-08   4.87269844e-10   4.26103948e-10   4.74734511e-04\n",
      "    5.71604081e-10   6.54484870e-11   6.50857880e-07   3.23463469e-11\n",
      "    1.07325586e-09   5.49899411e-09   7.84636049e-08   5.24678858e-11\n",
      "    7.84268213e-08   5.73196319e-12   1.40083392e-09   2.79238241e-08\n",
      "    2.55300715e-07   9.34258164e-07   8.31464906e-07   8.97444660e-08\n",
      "    3.03143298e-05   2.60817044e-11   1.00115925e-08   5.97857087e-09\n",
      "    1.28962878e-08   5.68289994e-09   2.26629834e-08   1.07074263e-07\n",
      "    3.09134888e-07   3.82284883e-08   3.34694023e-08   1.18185230e-06\n",
      "    1.56536843e-05   6.15365197e-08   1.92529661e-09   3.13472168e-08\n",
      "    1.41812795e-05   2.36427101e-08   2.36647829e-10   6.36218484e-08\n",
      "    4.34217946e-09   2.34278452e-06   3.70461510e-10   9.19695376e-07\n",
      "    6.40621382e-08   1.47079796e-05   1.60433123e-05   4.81841819e-09\n",
      "    9.50797377e-10   1.44737875e-08   1.80824277e-05   9.98532768e-01\n",
      "    3.55561479e-08   2.45357926e-08   4.25023271e-08   1.05362350e-04\n",
      "    1.66248402e-07   1.16285920e-07   1.32575254e-04   7.59973083e-06\n",
      "    9.01660811e-06   1.06420954e-09   6.57978172e-08   9.03079276e-09\n",
      "    1.26096188e-05   6.75841032e-06   9.17771427e-11   2.17593384e-09\n",
      "    9.48302678e-07   6.58761222e-06   4.01723805e-07   3.49450503e-07\n",
      "    2.58971377e-10   1.11689853e-10   4.73152044e-05   5.07556111e-08\n",
      "    8.80942551e-06   1.78030795e-09   9.18845935e-09   1.52663375e-04\n",
      "    6.40906019e-06   2.19319174e-07   9.60030267e-05   2.32071202e-07\n",
      "    2.22169182e-04   4.14283375e-06   5.09677044e-08   5.84228805e-07\n",
      "    4.79779627e-10   1.47415419e-07   1.70944138e-05   1.55452677e-08\n",
      "    6.72603079e-08   3.64596974e-05   9.97275574e-08   1.10319419e-11\n",
      "    4.16270864e-09   1.60003340e-09   3.46161452e-08]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "print ('Probabilities for each class %s'% clf.predict_proba(X_test[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].values\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: FutureWarning: sort is deprecated, use sort_values(inplace=True) for INPLACE sorting\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This Series is a view of some other array, to sort in-place you must create a copy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-e5682220c5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mleaf_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopen_file_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopen_file_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, axis, ascending, kind, na_position, inplace)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m         return self.sort_values(ascending=ascending, kind=kind,\n\u001b[0;32m-> 1833\u001b[0;31m                                 na_position=na_position, inplace=inplace)\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m     def order(self, na_last=None, ascending=True, kind='quicksort',\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;31m# GH 5856/5853\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_cached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             raise ValueError(\"This Series is a view of some other array, to \"\n\u001b[0m\u001b[1;32m   1733\u001b[0m                              \"sort in-place you must create a copy\")\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This Series is a view of some other array, to sort in-place you must create a copy"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train.sort_values(['species',ascending=[True], inplace=True])\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-0d0bf32bd283>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-0d0bf32bd283>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    leaf_class = train.sort_values(['species'],ascending=[True], inplace=True]).values\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train.sort_values(['species'],ascending=[True], inplace=True]).values\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-373a76753ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mleaf_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopen_file_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopen_file_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train.sort_values(['species'],ascending=[True], inplace=True).values\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-6ad2d71bffe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mleaf_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictions_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopen_file_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train = train.sort_values(['species'],ascending=[True], inplace=True)\n",
    "leaf_class = train['species'].values\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train = train.sort_values(['species'],ascending=[True], inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acer_Opalus', 'Pterocarya_Stenoptera', 'Quercus_Hartwissiana',\n",
       "       'Tilia_Tomentosa', 'Quercus_Variabilis', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Canariensis', 'Quercus_Rubra', 'Quercus_Brantii',\n",
       "       'Salix_Fragilis', 'Zelkova_Serrata', 'Betula_Austrosinensis',\n",
       "       'Quercus_Pontica', 'Quercus_Afares', 'Quercus_Coccifera',\n",
       "       'Fagus_Sylvatica', 'Phildelphus', 'Acer_Palmatum',\n",
       "       'Quercus_Pubescens', 'Populus_Adenopoda', 'Quercus_Trojana',\n",
       "       'Quercus_Variabilis', 'Alnus_Sieboldiana', 'Quercus_Ilex',\n",
       "       'Arundinaria_Simonii', 'Acer_Platanoids', 'Quercus_Phillyraeoides',\n",
       "       'Cornus_Chinensis', 'Quercus_Phillyraeoides', 'Fagus_Sylvatica',\n",
       "       'Liriodendron_Tulipifera', 'Cytisus_Battandieri', 'Tilia_Tomentosa',\n",
       "       'Rhododendron_x_Russellianum', 'Alnus_Rubra',\n",
       "       'Eucalyptus_Glaucescens', 'Cercis_Siliquastrum',\n",
       "       'Cotinus_Coggygria', 'Celtis_Koraiensis', 'Quercus_Crassifolia',\n",
       "       'Quercus_Variabilis', 'Quercus_Hartwissiana', 'Quercus_Kewensis',\n",
       "       'Quercus_Coccifera', 'Cornus_Controversa', 'Quercus_Pyrenaica',\n",
       "       'Callicarpa_Bodinieri', 'Quercus_Alnifolia', 'Quercus_Canariensis',\n",
       "       'Acer_Saccharinum', 'Prunus_X_Shmittii', 'Prunus_Avium',\n",
       "       'Quercus_Greggii', 'Quercus_Suber', 'Quercus_Trojana',\n",
       "       'Liriodendron_Tulipifera', 'Quercus_Coccifera',\n",
       "       'Cercis_Siliquastrum', 'Quercus_Suber', 'Celtis_Koraiensis',\n",
       "       'Quercus_Dolicholepis', 'Rhododendron_x_Russellianum',\n",
       "       'Ilex_Cornuta', 'Tilia_Oliveri', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Texana', 'Celtis_Koraiensis', 'Ginkgo_Biloba',\n",
       "       'Acer_Palmatum', 'Quercus_Variabilis', 'Liriodendron_Tulipifera',\n",
       "       'Liquidambar_Styraciflua', 'Quercus_Phellos', 'Quercus_Crassifolia',\n",
       "       'Quercus_Palustris', 'Quercus_Phellos', 'Quercus_Alnifolia',\n",
       "       'Quercus_Afares', 'Quercus_Canariensis', 'Alnus_Maximowiczii',\n",
       "       'Quercus_Agrifolia', 'Callicarpa_Bodinieri', 'Prunus_Avium',\n",
       "       'Acer_Pictum', 'Acer_Rufinerve', 'Lithocarpus_Cleistocarpus',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Celtis_Koraiensis',\n",
       "       'Ilex_Aquifolium', 'Acer_Circinatum', 'Quercus_Coccinea',\n",
       "       'Acer_Circinatum', 'Quercus_Cerris', 'Acer_Circinatum',\n",
       "       'Acer_Saccharinum', 'Quercus_Chrysolepis', 'Celtis_Koraiensis',\n",
       "       'Quercus_Semecarpifolia', 'Eucalyptus_Neglecta',\n",
       "       'Betula_Austrosinensis', 'Ginkgo_Biloba', 'Quercus_Canariensis',\n",
       "       'Tilia_Platyphyllos', 'Alnus_Cordata', 'Populus_Nigra',\n",
       "       'Quercus_Coccinea', 'Quercus_Variabilis', 'Quercus_Pyrenaica',\n",
       "       'Arundinaria_Simonii', 'Alnus_Cordata', 'Arundinaria_Simonii',\n",
       "       'Acer_Capillipes', 'Quercus_Kewensis', 'Acer_Palmatum',\n",
       "       'Quercus_Agrifolia', 'Quercus_Agrifolia', 'Tilia_Tomentosa',\n",
       "       'Liriodendron_Tulipifera', 'Magnolia_Heptapeta',\n",
       "       'Quercus_Dolicholepis', 'Acer_Mono', 'Cornus_Macrophylla',\n",
       "       'Crataegus_Monogyna', 'Liquidambar_Styraciflua',\n",
       "       'Cotinus_Coggygria', 'Quercus_x_Turneri', 'Acer_Capillipes',\n",
       "       'Quercus_Castaneifolia', 'Ilex_Cornuta', 'Lithocarpus_Edulis',\n",
       "       'Acer_Circinatum', 'Populus_Grandidentata', 'Acer_Rubrum',\n",
       "       'Tilia_Platyphyllos', 'Quercus_Cerris', 'Lithocarpus_Edulis',\n",
       "       'Cercis_Siliquastrum', 'Quercus_Agrifolia', 'Quercus_Pubescens',\n",
       "       'Quercus_Suber', 'Quercus_Pontica', 'Ilex_Aquifolium',\n",
       "       'Celtis_Koraiensis', 'Lithocarpus_Cleistocarpus',\n",
       "       'Acer_Saccharinum', 'Magnolia_Salicifolia', 'Quercus_Crassifolia',\n",
       "       'Fagus_Sylvatica', 'Quercus_Trojana', 'Quercus_Afares',\n",
       "       'Quercus_Palustris', 'Quercus_Imbricaria', 'Eucalyptus_Urnigera',\n",
       "       'Quercus_Ilex', 'Acer_Circinatum', 'Phildelphus',\n",
       "       'Quercus_Crassipes', 'Cornus_Controversa', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Variabilis', 'Quercus_x_Turneri', 'Populus_Nigra',\n",
       "       'Quercus_Chrysolepis', 'Betula_Austrosinensis', 'Acer_Mono',\n",
       "       'Eucalyptus_Glaucescens', 'Alnus_Rubra', 'Viburnum_Tinus',\n",
       "       'Populus_Adenopoda', 'Quercus_Pyrenaica', 'Eucalyptus_Neglecta',\n",
       "       'Quercus_Pubescens', 'Morus_Nigra', 'Quercus_x_Turneri',\n",
       "       'Quercus_Imbricaria', 'Quercus_Crassipes', 'Eucalyptus_Urnigera',\n",
       "       'Acer_Pictum', 'Alnus_Rubra', 'Quercus_Dolicholepis', 'Acer_Opalus',\n",
       "       'Quercus_Trojana', 'Quercus_Suber', 'Acer_Platanoids',\n",
       "       'Quercus_Vulcanica', 'Acer_Palmatum', 'Lithocarpus_Cleistocarpus',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Variabilis', 'Magnolia_Heptapeta',\n",
       "       'Quercus_Palustris', 'Quercus_Crassipes', 'Acer_Platanoids',\n",
       "       'Quercus_Pyrenaica', 'Alnus_Viridis', 'Fagus_Sylvatica',\n",
       "       'Zelkova_Serrata', 'Magnolia_Salicifolia', 'Betula_Pendula',\n",
       "       'Quercus_Agrifolia', 'Betula_Austrosinensis', 'Olea_Europaea',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_x_Hispanica',\n",
       "       'Lithocarpus_Cleistocarpus', 'Quercus_Kewensis',\n",
       "       'Quercus_x_Hispanica', 'Quercus_Palustris', 'Quercus_Shumardii',\n",
       "       'Quercus_Rubra', 'Alnus_Viridis', 'Liquidambar_Styraciflua',\n",
       "       'Cotinus_Coggygria', 'Acer_Pictum', 'Magnolia_Heptapeta',\n",
       "       'Acer_Rufinerve', 'Acer_Saccharinum', 'Crataegus_Monogyna',\n",
       "       'Populus_Adenopoda', 'Quercus_Crassifolia', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Viridis', 'Quercus_Rhysophylla', 'Alnus_Sieboldiana',\n",
       "       'Quercus_Shumardii', 'Quercus_x_Turneri', 'Callicarpa_Bodinieri',\n",
       "       'Quercus_Crassifolia', 'Quercus_Rubra', 'Tilia_Oliveri',\n",
       "       'Quercus_Agrifolia', 'Populus_Grandidentata', 'Quercus_Rubra',\n",
       "       'Morus_Nigra', 'Quercus_Agrifolia', 'Eucalyptus_Urnigera',\n",
       "       'Alnus_Viridis', 'Liquidambar_Styraciflua', 'Populus_Nigra',\n",
       "       'Cercis_Siliquastrum', 'Acer_Rufinerve', 'Acer_Platanoids',\n",
       "       'Tilia_Oliveri', 'Lithocarpus_Edulis', 'Magnolia_Salicifolia',\n",
       "       'Arundinaria_Simonii', 'Ginkgo_Biloba', 'Betula_Pendula',\n",
       "       'Tilia_Platyphyllos', 'Quercus_Palustris', 'Tilia_Oliveri',\n",
       "       'Cytisus_Battandieri', 'Quercus_Rubra', 'Arundinaria_Simonii',\n",
       "       'Castanea_Sativa', 'Quercus_Crassifolia', 'Cercis_Siliquastrum',\n",
       "       'Lithocarpus_Edulis', 'Liriodendron_Tulipifera',\n",
       "       'Pterocarya_Stenoptera', 'Quercus_Pontica', 'Quercus_Imbricaria',\n",
       "       'Ulmus_Bergmanniana', 'Fagus_Sylvatica', 'Quercus_Texana',\n",
       "       'Populus_Grandidentata', 'Betula_Austrosinensis', 'Quercus_Trojana',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Shumardii', 'Olea_Europaea',\n",
       "       'Cornus_Chinensis', 'Quercus_Pontica', 'Quercus_Brantii',\n",
       "       'Arundinaria_Simonii', 'Populus_Adenopoda', 'Prunus_Avium',\n",
       "       'Quercus_Chrysolepis', 'Tilia_Tomentosa', 'Quercus_Pyrenaica',\n",
       "       'Betula_Pendula', 'Ginkgo_Biloba', 'Cornus_Macrophylla',\n",
       "       'Arundinaria_Simonii', 'Tilia_Platyphyllos', 'Cornus_Macrophylla',\n",
       "       'Quercus_Shumardii', 'Quercus_Coccinea', 'Populus_Nigra',\n",
       "       'Alnus_Rubra', 'Quercus_Agrifolia', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Semecarpifolia', 'Quercus_Nigra', 'Quercus_Kewensis',\n",
       "       'Betula_Austrosinensis', 'Acer_Mono', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Trojana', 'Quercus_Castaneifolia', 'Alnus_Rubra',\n",
       "       'Quercus_Brantii', 'Acer_Circinatum', 'Quercus_Imbricaria',\n",
       "       'Acer_Capillipes', 'Acer_Mono', 'Liriodendron_Tulipifera',\n",
       "       'Olea_Europaea', 'Alnus_Cordata', 'Acer_Saccharinum',\n",
       "       'Quercus_Alnifolia', 'Quercus_Coccinea', 'Acer_Platanoids',\n",
       "       'Cornus_Chinensis', 'Quercus_x_Hispanica', 'Fagus_Sylvatica',\n",
       "       'Quercus_Agrifolia', 'Lithocarpus_Cleistocarpus', 'Phildelphus',\n",
       "       'Quercus_Pubescens', 'Salix_Intergra', 'Cornus_Macrophylla',\n",
       "       'Quercus_x_Hispanica', 'Viburnum_Tinus', 'Populus_Grandidentata',\n",
       "       'Ginkgo_Biloba', 'Zelkova_Serrata', 'Liriodendron_Tulipifera',\n",
       "       'Quercus_Ilex', 'Prunus_X_Shmittii', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Cerris', 'Morus_Nigra', 'Cercis_Siliquastrum',\n",
       "       'Cotinus_Coggygria', 'Cytisus_Battandieri',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Callicarpa_Bodinieri',\n",
       "       'Populus_Adenopoda', 'Populus_Grandidentata',\n",
       "       'Magnolia_Salicifolia', 'Quercus_Shumardii',\n",
       "       'Quercus_Ellipsoidalis', 'Celtis_Koraiensis',\n",
       "       'Liquidambar_Styraciflua', 'Acer_Capillipes', 'Acer_Rufinerve',\n",
       "       'Cytisus_Battandieri', 'Cotinus_Coggygria',\n",
       "       'Liquidambar_Styraciflua', 'Quercus_Coccinea', 'Quercus_Crassipes',\n",
       "       'Cercis_Siliquastrum', 'Lithocarpus_Cleistocarpus',\n",
       "       'Eucalyptus_Neglecta', 'Acer_Platanoids', 'Castanea_Sativa',\n",
       "       'Quercus_Rubra', 'Quercus_Afares', 'Quercus_Canariensis',\n",
       "       'Lithocarpus_Cleistocarpus', 'Tilia_Platyphyllos',\n",
       "       'Ilex_Aquifolium', 'Populus_Nigra', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Coccinea', 'Eucalyptus_Neglecta', 'Phildelphus',\n",
       "       'Acer_Capillipes', 'Cornus_Controversa', 'Betula_Pendula',\n",
       "       'Lithocarpus_Edulis', 'Quercus_Ilex', 'Lithocarpus_Edulis',\n",
       "       'Salix_Fragilis', 'Betula_Pendula', 'Cotinus_Coggygria',\n",
       "       'Fagus_Sylvatica', 'Quercus_Hartwissiana', 'Alnus_Cordata',\n",
       "       'Crataegus_Monogyna', 'Quercus_x_Hispanica', 'Quercus_Coccifera',\n",
       "       'Olea_Europaea', 'Populus_Nigra', 'Cornus_Macrophylla',\n",
       "       'Ulmus_Bergmanniana', 'Tilia_Platyphyllos', 'Acer_Rufinerve',\n",
       "       'Quercus_Brantii', 'Ginkgo_Biloba', 'Alnus_Viridis',\n",
       "       'Alnus_Cordata', 'Tilia_Tomentosa', 'Acer_Rufinerve',\n",
       "       'Acer_Rufinerve', 'Quercus_Greggii', 'Populus_Adenopoda',\n",
       "       'Quercus_Pontica', 'Eucalyptus_Urnigera',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Cornus_Chinensis',\n",
       "       'Alnus_Maximowiczii', 'Quercus_Infectoria_sub', 'Quercus_Cerris',\n",
       "       'Salix_Intergra', 'Viburnum_Tinus', 'Betula_Austrosinensis',\n",
       "       'Phildelphus', 'Quercus_Chrysolepis', 'Sorbus_Aria',\n",
       "       'Cornus_Macrophylla', 'Eucalyptus_Glaucescens', 'Quercus_Texana',\n",
       "       'Salix_Intergra', 'Quercus_Greggii', 'Quercus_Crassipes',\n",
       "       'Tilia_Oliveri', 'Eucalyptus_Glaucescens', 'Quercus_Variabilis',\n",
       "       'Quercus_Nigra', 'Populus_Grandidentata', 'Quercus_Vulcanica',\n",
       "       'Ilex_Cornuta', 'Acer_Rubrum', 'Rhododendron_x_Russellianum',\n",
       "       'Salix_Fragilis', 'Quercus_Crassipes', 'Crataegus_Monogyna',\n",
       "       'Rhododendron_x_Russellianum', 'Morus_Nigra', 'Populus_Nigra',\n",
       "       'Acer_Rufinerve', 'Sorbus_Aria', 'Morus_Nigra', 'Alnus_Viridis',\n",
       "       'Zelkova_Serrata', 'Eucalyptus_Glaucescens', 'Crataegus_Monogyna',\n",
       "       'Liriodendron_Tulipifera', 'Quercus_Nigra', 'Quercus_Pontica',\n",
       "       'Acer_Rubrum', 'Quercus_Alnifolia', 'Quercus_Nigra',\n",
       "       'Olea_Europaea', 'Quercus_Ilex', 'Cornus_Chinensis',\n",
       "       'Alnus_Sieboldiana', 'Populus_Adenopoda', 'Sorbus_Aria',\n",
       "       'Quercus_Brantii', 'Cornus_Chinensis', 'Betula_Austrosinensis',\n",
       "       'Magnolia_Salicifolia', 'Magnolia_Salicifolia',\n",
       "       'Eucalyptus_Glaucescens', 'Quercus_Nigra', 'Acer_Platanoids',\n",
       "       'Quercus_Crassifolia', 'Ulmus_Bergmanniana', 'Cornus_Controversa',\n",
       "       'Betula_Austrosinensis', 'Quercus_Afares', 'Eucalyptus_Neglecta',\n",
       "       'Pterocarya_Stenoptera', 'Acer_Saccharinum', 'Callicarpa_Bodinieri',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Crassifolia', 'Castanea_Sativa', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Urnigera', 'Fagus_Sylvatica', 'Quercus_Vulcanica',\n",
       "       'Quercus_Ilex', 'Rhododendron_x_Russellianum',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Trojana', 'Quercus_Greggii',\n",
       "       'Viburnum_Tinus', 'Alnus_Sieboldiana', 'Ilex_Cornuta',\n",
       "       'Cytisus_Battandieri', 'Ulmus_Bergmanniana',\n",
       "       'Liriodendron_Tulipifera', 'Quercus_Phellos', 'Eucalyptus_Neglecta',\n",
       "       'Sorbus_Aria', 'Cornus_Chinensis', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Palustris', 'Quercus_Ilex', 'Alnus_Maximowiczii',\n",
       "       'Ginkgo_Biloba', 'Acer_Mono', 'Quercus_Pubescens',\n",
       "       'Magnolia_Heptapeta', 'Acer_Pictum', 'Lithocarpus_Cleistocarpus',\n",
       "       'Quercus_Imbricaria', 'Quercus_Suber', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Crassipes', 'Quercus_Chrysolepis', 'Quercus_Suber',\n",
       "       'Quercus_Texana', 'Quercus_Ellipsoidalis', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Canariensis', 'Liquidambar_Styraciflua',\n",
       "       'Quercus_x_Hispanica', 'Cotinus_Coggygria', 'Cornus_Macrophylla',\n",
       "       'Viburnum_Tinus', 'Alnus_Cordata', 'Quercus_Coccifera',\n",
       "       'Quercus_Coccinea', 'Eucalyptus_Glaucescens', 'Salix_Fragilis',\n",
       "       'Quercus_Canariensis', 'Quercus_Alnifolia', 'Quercus_Vulcanica',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Kewensis', 'Acer_Rufinerve',\n",
       "       'Magnolia_Heptapeta', 'Ulmus_Bergmanniana', 'Magnolia_Heptapeta',\n",
       "       'Quercus_Texana', 'Prunus_X_Shmittii', 'Quercus_Pontica',\n",
       "       'Quercus_Alnifolia', 'Quercus_Pontica', 'Quercus_Castaneifolia',\n",
       "       'Acer_Saccharinum', 'Prunus_Avium', 'Quercus_Variabilis',\n",
       "       'Sorbus_Aria', 'Alnus_Rubra', 'Viburnum_Tinus', 'Quercus_Texana',\n",
       "       'Quercus_Greggii', 'Acer_Rubrum', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Rhysophylla',\n",
       "       'Alnus_Maximowiczii', 'Liriodendron_Tulipifera',\n",
       "       'Quercus_Infectoria_sub', 'Cornus_Controversa',\n",
       "       'Eucalyptus_Glaucescens', 'Populus_Nigra', 'Quercus_Pubescens',\n",
       "       'Phildelphus', 'Acer_Mono', 'Alnus_Maximowiczii', 'Prunus_Avium',\n",
       "       'Quercus_Brantii', 'Quercus_Cerris', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Shumardii', 'Quercus_Cerris',\n",
       "       'Eucalyptus_Urnigera', 'Acer_Opalus', 'Rhododendron_x_Russellianum',\n",
       "       'Lithocarpus_Cleistocarpus', 'Ilex_Aquifolium',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Pyrenaica',\n",
       "       'Cercis_Siliquastrum', 'Acer_Pictum', 'Quercus_Pyrenaica',\n",
       "       'Quercus_x_Turneri', 'Quercus_Infectoria_sub', 'Quercus_Trojana',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Chrysolepis', 'Prunus_Avium',\n",
       "       'Castanea_Sativa', 'Rhododendron_x_Russellianum', 'Viburnum_Tinus',\n",
       "       'Olea_Europaea', 'Cornus_Controversa', 'Populus_Adenopoda',\n",
       "       'Quercus_x_Turneri', 'Quercus_Texana', 'Pterocarya_Stenoptera',\n",
       "       'Quercus_Ellipsoidalis', 'Cornus_Macrophylla', 'Tilia_Tomentosa',\n",
       "       'Quercus_Kewensis', 'Quercus_Canariensis', 'Acer_Pictum',\n",
       "       'Quercus_Semecarpifolia', 'Phildelphus', 'Alnus_Rubra',\n",
       "       'Quercus_Afares', 'Acer_Pictum', 'Quercus_Alnifolia',\n",
       "       'Tilia_Oliveri', 'Acer_Palmatum', 'Quercus_Variabilis',\n",
       "       'Acer_Circinatum', 'Ilex_Aquifolium', 'Pterocarya_Stenoptera',\n",
       "       'Crataegus_Monogyna', 'Callicarpa_Bodinieri', 'Populus_Adenopoda',\n",
       "       'Alnus_Sieboldiana', 'Cornus_Macrophylla', 'Quercus_Phillyraeoides',\n",
       "       'Salix_Fragilis', 'Quercus_Rubra', 'Quercus_Imbricaria',\n",
       "       'Morus_Nigra', 'Alnus_Maximowiczii', 'Populus_Nigra',\n",
       "       'Prunus_Avium', 'Quercus_Kewensis', 'Acer_Capillipes',\n",
       "       'Callicarpa_Bodinieri', 'Zelkova_Serrata', 'Populus_Adenopoda',\n",
       "       'Quercus_Vulcanica', 'Eucalyptus_Neglecta', 'Quercus_Brantii',\n",
       "       'Acer_Circinatum', 'Populus_Grandidentata', 'Acer_Opalus',\n",
       "       'Acer_Saccharinum', 'Alnus_Sieboldiana', 'Acer_Mono',\n",
       "       'Quercus_Ilex', 'Quercus_Coccinea', 'Quercus_Semecarpifolia',\n",
       "       'Acer_Platanoids', 'Betula_Pendula', 'Phildelphus',\n",
       "       'Quercus_Trojana', 'Crataegus_Monogyna', 'Acer_Circinatum',\n",
       "       'Alnus_Rubra', 'Pterocarya_Stenoptera', 'Quercus_Palustris',\n",
       "       'Salix_Fragilis', 'Quercus_Coccifera', 'Tilia_Platyphyllos',\n",
       "       'Magnolia_Heptapeta', 'Olea_Europaea', 'Arundinaria_Simonii',\n",
       "       'Lithocarpus_Edulis', 'Quercus_Castaneifolia',\n",
       "       'Arundinaria_Simonii', 'Tilia_Platyphyllos', 'Acer_Palmatum',\n",
       "       'Salix_Intergra', 'Rhododendron_x_Russellianum',\n",
       "       'Quercus_Castaneifolia', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Castaneifolia', 'Betula_Pendula', 'Ilex_Cornuta',\n",
       "       'Tilia_Tomentosa', 'Quercus_Chrysolepis', 'Tilia_Oliveri',\n",
       "       'Quercus_Suber', 'Crataegus_Monogyna', 'Quercus_Crassifolia',\n",
       "       'Salix_Fragilis', 'Pterocarya_Stenoptera', 'Salix_Fragilis',\n",
       "       'Eucalyptus_Urnigera', 'Quercus_Hartwissiana', 'Quercus_Coccifera',\n",
       "       'Acer_Palmatum', 'Castanea_Sativa', 'Acer_Palmatum',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Pontica', 'Quercus_Palustris',\n",
       "       'Cercis_Siliquastrum', 'Quercus_Canariensis', 'Alnus_Sieboldiana',\n",
       "       'Betula_Austrosinensis', 'Quercus_Phellos',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Crassipes', 'Cotinus_Coggygria', 'Quercus_Phellos',\n",
       "       'Acer_Rubrum', 'Alnus_Rubra', 'Callicarpa_Bodinieri',\n",
       "       'Quercus_Rhysophylla', 'Salix_Intergra', 'Quercus_Dolicholepis',\n",
       "       'Alnus_Viridis', 'Acer_Opalus', 'Quercus_Rubra',\n",
       "       'Quercus_Hartwissiana', 'Lithocarpus_Cleistocarpus',\n",
       "       'Cytisus_Battandieri', 'Quercus_Afares', 'Ulmus_Bergmanniana',\n",
       "       'Zelkova_Serrata', 'Quercus_Crassifolia', 'Quercus_Phellos',\n",
       "       'Quercus_Coccifera', 'Lithocarpus_Edulis', 'Quercus_x_Turneri',\n",
       "       'Tilia_Platyphyllos', 'Castanea_Sativa', 'Acer_Rubrum',\n",
       "       'Quercus_Trojana', 'Quercus_Afares', 'Acer_Opalus', 'Sorbus_Aria',\n",
       "       'Quercus_Rhysophylla', 'Acer_Rubrum', 'Quercus_Greggii',\n",
       "       'Quercus_Crassipes', 'Quercus_Kewensis', 'Cornus_Controversa',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Hartwissiana', 'Prunus_X_Shmittii',\n",
       "       'Morus_Nigra', 'Tilia_Oliveri', 'Celtis_Koraiensis', 'Sorbus_Aria',\n",
       "       'Callicarpa_Bodinieri', 'Quercus_Nigra', 'Acer_Saccharinum',\n",
       "       'Acer_Saccharinum', 'Eucalyptus_Neglecta', 'Quercus_Vulcanica',\n",
       "       'Quercus_Imbricaria', 'Acer_Capillipes', 'Liquidambar_Styraciflua',\n",
       "       'Zelkova_Serrata', 'Sorbus_Aria', 'Salix_Intergra',\n",
       "       'Fagus_Sylvatica', 'Quercus_Brantii', 'Magnolia_Heptapeta',\n",
       "       'Cornus_Macrophylla', 'Quercus_Vulcanica', 'Viburnum_Tinus',\n",
       "       'Betula_Pendula', 'Acer_Rubrum', 'Alnus_Maximowiczii',\n",
       "       'Viburnum_Tinus', 'Magnolia_Heptapeta', 'Quercus_Cerris',\n",
       "       'Salix_Intergra', 'Quercus_Semecarpifolia', 'Populus_Grandidentata',\n",
       "       'Quercus_Greggii', 'Viburnum_x_Rhytidophylloides', 'Morus_Nigra',\n",
       "       'Castanea_Sativa', 'Quercus_Pontica', 'Alnus_Maximowiczii',\n",
       "       'Alnus_Cordata', 'Quercus_Dolicholepis', 'Ulmus_Bergmanniana',\n",
       "       'Eucalyptus_Glaucescens', 'Viburnum_Tinus', 'Alnus_Cordata',\n",
       "       'Acer_Rubrum', 'Quercus_Infectoria_sub', 'Acer_Rubrum',\n",
       "       'Tilia_Platyphyllos', 'Ilex_Cornuta', 'Olea_Europaea',\n",
       "       'Quercus_Infectoria_sub', 'Morus_Nigra', 'Alnus_Viridis',\n",
       "       'Cornus_Chinensis', 'Quercus_Coccifera', 'Alnus_Rubra',\n",
       "       'Quercus_Palustris', 'Cytisus_Battandieri', 'Zelkova_Serrata',\n",
       "       'Quercus_Phellos', 'Sorbus_Aria', 'Acer_Opalus', 'Phildelphus',\n",
       "       'Castanea_Sativa', 'Quercus_Crassipes', 'Quercus_Pubescens',\n",
       "       'Celtis_Koraiensis', 'Quercus_Suber', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Imbricaria', 'Quercus_Cerris', 'Crataegus_Monogyna',\n",
       "       'Prunus_Avium', 'Alnus_Maximowiczii', 'Fagus_Sylvatica',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Dolicholepis', 'Quercus_Ilex',\n",
       "       'Castanea_Sativa', 'Quercus_Pubescens',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Tilia_Tomentosa',\n",
       "       'Alnus_Sieboldiana', 'Quercus_Pubescens', 'Ulmus_Bergmanniana',\n",
       "       'Ginkgo_Biloba', 'Quercus_Afares', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Quercus_Alnifolia', 'Quercus_x_Turneri', 'Ginkgo_Biloba',\n",
       "       'Acer_Opalus', 'Prunus_Avium', 'Quercus_Rhysophylla',\n",
       "       'Prunus_X_Shmittii', 'Ilex_Aquifolium', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Greggii', 'Pterocarya_Stenoptera',\n",
       "       'Quercus_Infectoria_sub', 'Phildelphus', 'Tilia_Tomentosa',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Infectoria_sub', 'Prunus_Avium',\n",
       "       'Quercus_Texana', 'Acer_Mono', 'Eucalyptus_Urnigera',\n",
       "       'Cornus_Controversa', 'Ilex_Aquifolium', 'Ilex_Cornuta',\n",
       "       'Quercus_Ilex', 'Ilex_Aquifolium', 'Liquidambar_Styraciflua',\n",
       "       'Rhododendron_x_Russellianum', 'Quercus_Infectoria_sub',\n",
       "       'Cytisus_Battandieri', 'Quercus_x_Turneri', 'Quercus_Canariensis',\n",
       "       'Quercus_Semecarpifolia', 'Ilex_Cornuta', 'Quercus_Imbricaria',\n",
       "       'Quercus_Hartwissiana', 'Callicarpa_Bodinieri', 'Acer_Palmatum',\n",
       "       'Crataegus_Monogyna', 'Eucalyptus_Urnigera', 'Quercus_Alnifolia',\n",
       "       'Quercus_x_Hispanica', 'Acer_Capillipes', 'Eucalyptus_Neglecta',\n",
       "       'Quercus_Shumardii', 'Quercus_Imbricaria', 'Ulmus_Bergmanniana',\n",
       "       'Magnolia_Heptapeta', 'Cotinus_Coggygria', 'Quercus_Nigra',\n",
       "       'Quercus_Coccifera', 'Liquidambar_Styraciflua',\n",
       "       'Cytisus_Battandieri', 'Betula_Pendula', 'Lithocarpus_Edulis',\n",
       "       'Sorbus_Aria', 'Alnus_Sieboldiana', 'Cornus_Controversa',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Rhysophylla', 'Cornus_Chinensis', 'Pterocarya_Stenoptera',\n",
       "       'Zelkova_Serrata', 'Prunus_X_Shmittii', 'Populus_Nigra',\n",
       "       'Prunus_X_Shmittii', 'Acer_Pictum', 'Quercus_Ellipsoidalis',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Quercus_Shumardii',\n",
       "       'Quercus_Brantii', 'Celtis_Koraiensis', 'Cotinus_Coggygria',\n",
       "       'Quercus_Rubra', 'Alnus_Cordata', 'Quercus_Hartwissiana',\n",
       "       'Ilex_Aquifolium', 'Morus_Nigra', 'Quercus_Vulcanica',\n",
       "       'Salix_Intergra', 'Zelkova_Serrata', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Quercus_Palustris', 'Ulmus_Bergmanniana', 'Quercus_Alnifolia',\n",
       "       'Acer_Opalus', 'Eucalyptus_Neglecta', 'Quercus_Kewensis',\n",
       "       'Quercus_Agrifolia', 'Quercus_Nigra', 'Cytisus_Battandieri',\n",
       "       'Cornus_Chinensis', 'Ilex_Cornuta', 'Acer_Platanoids',\n",
       "       'Prunus_X_Shmittii', 'Quercus_Phellos', 'Cercis_Siliquastrum',\n",
       "       'Salix_Fragilis', 'Betula_Pendula', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Quercus_Phellos', 'Quercus_Shumardii', 'Alnus_Cordata',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Greggii', 'Tilia_Oliveri',\n",
       "       'Lithocarpus_Edulis', 'Arundinaria_Simonii',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_x_Turneri',\n",
       "       'Quercus_Phillyraeoides', 'Prunus_X_Shmittii', 'Olea_Europaea',\n",
       "       'Quercus_Texana', 'Ilex_Aquifolium', 'Quercus_Cerris',\n",
       "       'Acer_Opalus', 'Olea_Europaea', 'Acer_Circinatum',\n",
       "       'Quercus_Brantii', 'Quercus_Castaneifolia', 'Salix_Intergra',\n",
       "       'Castanea_Sativa', 'Acer_Platanoids', 'Eucalyptus_Urnigera',\n",
       "       'Quercus_x_Hispanica', 'Quercus_Greggii', 'Pterocarya_Stenoptera',\n",
       "       'Acer_Mono', 'Acer_Rufinerve', 'Populus_Grandidentata',\n",
       "       'Quercus_Pyrenaica', 'Tilia_Oliveri', 'Acer_Capillipes',\n",
       "       'Cornus_Controversa', 'Quercus_Kewensis', 'Quercus_Coccinea',\n",
       "       'Quercus_Shumardii', 'Salix_Intergra', 'Ginkgo_Biloba',\n",
       "       'Acer_Pictum', 'Quercus_Coccinea', 'Quercus_Vulcanica',\n",
       "       'Salix_Fragilis', 'Tilia_Tomentosa', 'Populus_Grandidentata',\n",
       "       'Prunus_X_Shmittii', 'Quercus_x_Hispanica', 'Quercus_Suber',\n",
       "       'Alnus_Viridis', 'Acer_Palmatum', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Cerris', 'Quercus_Texana', 'Acer_Mono',\n",
       "       'Quercus_Vulcanica', 'Quercus_Nigra', 'Rhododendron_x_Russellianum',\n",
       "       'Acer_Capillipes', 'Quercus_Pubescens', 'Alnus_Viridis',\n",
       "       'Quercus_x_Hispanica', 'Quercus_Suber',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Quercus_Nigra', 'Quercus_Phellos',\n",
       "       'Ilex_Cornuta', 'Magnolia_Salicifolia', 'Acer_Pictum',\n",
       "       'Alnus_Maximowiczii', 'Quercus_Rubra', 'Quercus_Afares'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].values\n",
    "leaf_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].values\n",
    "leaf_class.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].values\n",
    "leaf_class = leaf_class.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaf_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acer_Opalus', 'Pterocarya_Stenoptera', 'Quercus_Hartwissiana',\n",
       "       'Tilia_Tomentosa', 'Quercus_Variabilis', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Canariensis', 'Quercus_Rubra', 'Quercus_Brantii',\n",
       "       'Salix_Fragilis', 'Zelkova_Serrata', 'Betula_Austrosinensis',\n",
       "       'Quercus_Pontica', 'Quercus_Afares', 'Quercus_Coccifera',\n",
       "       'Fagus_Sylvatica', 'Phildelphus', 'Acer_Palmatum',\n",
       "       'Quercus_Pubescens', 'Populus_Adenopoda', 'Quercus_Trojana',\n",
       "       'Quercus_Variabilis', 'Alnus_Sieboldiana', 'Quercus_Ilex',\n",
       "       'Arundinaria_Simonii', 'Acer_Platanoids', 'Quercus_Phillyraeoides',\n",
       "       'Cornus_Chinensis', 'Quercus_Phillyraeoides', 'Fagus_Sylvatica',\n",
       "       'Liriodendron_Tulipifera', 'Cytisus_Battandieri', 'Tilia_Tomentosa',\n",
       "       'Rhododendron_x_Russellianum', 'Alnus_Rubra',\n",
       "       'Eucalyptus_Glaucescens', 'Cercis_Siliquastrum',\n",
       "       'Cotinus_Coggygria', 'Celtis_Koraiensis', 'Quercus_Crassifolia',\n",
       "       'Quercus_Variabilis', 'Quercus_Hartwissiana', 'Quercus_Kewensis',\n",
       "       'Quercus_Coccifera', 'Cornus_Controversa', 'Quercus_Pyrenaica',\n",
       "       'Callicarpa_Bodinieri', 'Quercus_Alnifolia', 'Quercus_Canariensis',\n",
       "       'Acer_Saccharinum', 'Prunus_X_Shmittii', 'Prunus_Avium',\n",
       "       'Quercus_Greggii', 'Quercus_Suber', 'Quercus_Trojana',\n",
       "       'Liriodendron_Tulipifera', 'Quercus_Coccifera',\n",
       "       'Cercis_Siliquastrum', 'Quercus_Suber', 'Celtis_Koraiensis',\n",
       "       'Quercus_Dolicholepis', 'Rhododendron_x_Russellianum',\n",
       "       'Ilex_Cornuta', 'Tilia_Oliveri', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Texana', 'Celtis_Koraiensis', 'Ginkgo_Biloba',\n",
       "       'Acer_Palmatum', 'Quercus_Variabilis', 'Liriodendron_Tulipifera',\n",
       "       'Liquidambar_Styraciflua', 'Quercus_Phellos', 'Quercus_Crassifolia',\n",
       "       'Quercus_Palustris', 'Quercus_Phellos', 'Quercus_Alnifolia',\n",
       "       'Quercus_Afares', 'Quercus_Canariensis', 'Alnus_Maximowiczii',\n",
       "       'Quercus_Agrifolia', 'Callicarpa_Bodinieri', 'Prunus_Avium',\n",
       "       'Acer_Pictum', 'Acer_Rufinerve', 'Lithocarpus_Cleistocarpus',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Celtis_Koraiensis',\n",
       "       'Ilex_Aquifolium', 'Acer_Circinatum', 'Quercus_Coccinea',\n",
       "       'Acer_Circinatum', 'Quercus_Cerris', 'Acer_Circinatum',\n",
       "       'Acer_Saccharinum', 'Quercus_Chrysolepis', 'Celtis_Koraiensis',\n",
       "       'Quercus_Semecarpifolia', 'Eucalyptus_Neglecta',\n",
       "       'Betula_Austrosinensis', 'Ginkgo_Biloba', 'Quercus_Canariensis',\n",
       "       'Tilia_Platyphyllos', 'Alnus_Cordata', 'Populus_Nigra',\n",
       "       'Quercus_Coccinea', 'Quercus_Variabilis', 'Quercus_Pyrenaica',\n",
       "       'Arundinaria_Simonii', 'Alnus_Cordata', 'Arundinaria_Simonii',\n",
       "       'Acer_Capillipes', 'Quercus_Kewensis', 'Acer_Palmatum',\n",
       "       'Quercus_Agrifolia', 'Quercus_Agrifolia', 'Tilia_Tomentosa',\n",
       "       'Liriodendron_Tulipifera', 'Magnolia_Heptapeta',\n",
       "       'Quercus_Dolicholepis', 'Acer_Mono', 'Cornus_Macrophylla',\n",
       "       'Crataegus_Monogyna', 'Liquidambar_Styraciflua',\n",
       "       'Cotinus_Coggygria', 'Quercus_x_Turneri', 'Acer_Capillipes',\n",
       "       'Quercus_Castaneifolia', 'Ilex_Cornuta', 'Lithocarpus_Edulis',\n",
       "       'Acer_Circinatum', 'Populus_Grandidentata', 'Acer_Rubrum',\n",
       "       'Tilia_Platyphyllos', 'Quercus_Cerris', 'Lithocarpus_Edulis',\n",
       "       'Cercis_Siliquastrum', 'Quercus_Agrifolia', 'Quercus_Pubescens',\n",
       "       'Quercus_Suber', 'Quercus_Pontica', 'Ilex_Aquifolium',\n",
       "       'Celtis_Koraiensis', 'Lithocarpus_Cleistocarpus',\n",
       "       'Acer_Saccharinum', 'Magnolia_Salicifolia', 'Quercus_Crassifolia',\n",
       "       'Fagus_Sylvatica', 'Quercus_Trojana', 'Quercus_Afares',\n",
       "       'Quercus_Palustris', 'Quercus_Imbricaria', 'Eucalyptus_Urnigera',\n",
       "       'Quercus_Ilex', 'Acer_Circinatum', 'Phildelphus',\n",
       "       'Quercus_Crassipes', 'Cornus_Controversa', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Variabilis', 'Quercus_x_Turneri', 'Populus_Nigra',\n",
       "       'Quercus_Chrysolepis', 'Betula_Austrosinensis', 'Acer_Mono',\n",
       "       'Eucalyptus_Glaucescens', 'Alnus_Rubra', 'Viburnum_Tinus',\n",
       "       'Populus_Adenopoda', 'Quercus_Pyrenaica', 'Eucalyptus_Neglecta',\n",
       "       'Quercus_Pubescens', 'Morus_Nigra', 'Quercus_x_Turneri',\n",
       "       'Quercus_Imbricaria', 'Quercus_Crassipes', 'Eucalyptus_Urnigera',\n",
       "       'Acer_Pictum', 'Alnus_Rubra', 'Quercus_Dolicholepis', 'Acer_Opalus',\n",
       "       'Quercus_Trojana', 'Quercus_Suber', 'Acer_Platanoids',\n",
       "       'Quercus_Vulcanica', 'Acer_Palmatum', 'Lithocarpus_Cleistocarpus',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Variabilis', 'Magnolia_Heptapeta',\n",
       "       'Quercus_Palustris', 'Quercus_Crassipes', 'Acer_Platanoids',\n",
       "       'Quercus_Pyrenaica', 'Alnus_Viridis', 'Fagus_Sylvatica',\n",
       "       'Zelkova_Serrata', 'Magnolia_Salicifolia', 'Betula_Pendula',\n",
       "       'Quercus_Agrifolia', 'Betula_Austrosinensis', 'Olea_Europaea',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_x_Hispanica',\n",
       "       'Lithocarpus_Cleistocarpus', 'Quercus_Kewensis',\n",
       "       'Quercus_x_Hispanica', 'Quercus_Palustris', 'Quercus_Shumardii',\n",
       "       'Quercus_Rubra', 'Alnus_Viridis', 'Liquidambar_Styraciflua',\n",
       "       'Cotinus_Coggygria', 'Acer_Pictum', 'Magnolia_Heptapeta',\n",
       "       'Acer_Rufinerve', 'Acer_Saccharinum', 'Crataegus_Monogyna',\n",
       "       'Populus_Adenopoda', 'Quercus_Crassifolia', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Viridis', 'Quercus_Rhysophylla', 'Alnus_Sieboldiana',\n",
       "       'Quercus_Shumardii', 'Quercus_x_Turneri', 'Callicarpa_Bodinieri',\n",
       "       'Quercus_Crassifolia', 'Quercus_Rubra', 'Tilia_Oliveri',\n",
       "       'Quercus_Agrifolia', 'Populus_Grandidentata', 'Quercus_Rubra',\n",
       "       'Morus_Nigra', 'Quercus_Agrifolia', 'Eucalyptus_Urnigera',\n",
       "       'Alnus_Viridis', 'Liquidambar_Styraciflua', 'Populus_Nigra',\n",
       "       'Cercis_Siliquastrum', 'Acer_Rufinerve', 'Acer_Platanoids',\n",
       "       'Tilia_Oliveri', 'Lithocarpus_Edulis', 'Magnolia_Salicifolia',\n",
       "       'Arundinaria_Simonii', 'Ginkgo_Biloba', 'Betula_Pendula',\n",
       "       'Tilia_Platyphyllos', 'Quercus_Palustris', 'Tilia_Oliveri',\n",
       "       'Cytisus_Battandieri', 'Quercus_Rubra', 'Arundinaria_Simonii',\n",
       "       'Castanea_Sativa', 'Quercus_Crassifolia', 'Cercis_Siliquastrum',\n",
       "       'Lithocarpus_Edulis', 'Liriodendron_Tulipifera',\n",
       "       'Pterocarya_Stenoptera', 'Quercus_Pontica', 'Quercus_Imbricaria',\n",
       "       'Ulmus_Bergmanniana', 'Fagus_Sylvatica', 'Quercus_Texana',\n",
       "       'Populus_Grandidentata', 'Betula_Austrosinensis', 'Quercus_Trojana',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Shumardii', 'Olea_Europaea',\n",
       "       'Cornus_Chinensis', 'Quercus_Pontica', 'Quercus_Brantii',\n",
       "       'Arundinaria_Simonii', 'Populus_Adenopoda', 'Prunus_Avium',\n",
       "       'Quercus_Chrysolepis', 'Tilia_Tomentosa', 'Quercus_Pyrenaica',\n",
       "       'Betula_Pendula', 'Ginkgo_Biloba', 'Cornus_Macrophylla',\n",
       "       'Arundinaria_Simonii', 'Tilia_Platyphyllos', 'Cornus_Macrophylla',\n",
       "       'Quercus_Shumardii', 'Quercus_Coccinea', 'Populus_Nigra',\n",
       "       'Alnus_Rubra', 'Quercus_Agrifolia', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Semecarpifolia', 'Quercus_Nigra', 'Quercus_Kewensis',\n",
       "       'Betula_Austrosinensis', 'Acer_Mono', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Trojana', 'Quercus_Castaneifolia', 'Alnus_Rubra',\n",
       "       'Quercus_Brantii', 'Acer_Circinatum', 'Quercus_Imbricaria',\n",
       "       'Acer_Capillipes', 'Acer_Mono', 'Liriodendron_Tulipifera',\n",
       "       'Olea_Europaea', 'Alnus_Cordata', 'Acer_Saccharinum',\n",
       "       'Quercus_Alnifolia', 'Quercus_Coccinea', 'Acer_Platanoids',\n",
       "       'Cornus_Chinensis', 'Quercus_x_Hispanica', 'Fagus_Sylvatica',\n",
       "       'Quercus_Agrifolia', 'Lithocarpus_Cleistocarpus', 'Phildelphus',\n",
       "       'Quercus_Pubescens', 'Salix_Intergra', 'Cornus_Macrophylla',\n",
       "       'Quercus_x_Hispanica', 'Viburnum_Tinus', 'Populus_Grandidentata',\n",
       "       'Ginkgo_Biloba', 'Zelkova_Serrata', 'Liriodendron_Tulipifera',\n",
       "       'Quercus_Ilex', 'Prunus_X_Shmittii', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Cerris', 'Morus_Nigra', 'Cercis_Siliquastrum',\n",
       "       'Cotinus_Coggygria', 'Cytisus_Battandieri',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Callicarpa_Bodinieri',\n",
       "       'Populus_Adenopoda', 'Populus_Grandidentata',\n",
       "       'Magnolia_Salicifolia', 'Quercus_Shumardii',\n",
       "       'Quercus_Ellipsoidalis', 'Celtis_Koraiensis',\n",
       "       'Liquidambar_Styraciflua', 'Acer_Capillipes', 'Acer_Rufinerve',\n",
       "       'Cytisus_Battandieri', 'Cotinus_Coggygria',\n",
       "       'Liquidambar_Styraciflua', 'Quercus_Coccinea', 'Quercus_Crassipes',\n",
       "       'Cercis_Siliquastrum', 'Lithocarpus_Cleistocarpus',\n",
       "       'Eucalyptus_Neglecta', 'Acer_Platanoids', 'Castanea_Sativa',\n",
       "       'Quercus_Rubra', 'Quercus_Afares', 'Quercus_Canariensis',\n",
       "       'Lithocarpus_Cleistocarpus', 'Tilia_Platyphyllos',\n",
       "       'Ilex_Aquifolium', 'Populus_Nigra', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Coccinea', 'Eucalyptus_Neglecta', 'Phildelphus',\n",
       "       'Acer_Capillipes', 'Cornus_Controversa', 'Betula_Pendula',\n",
       "       'Lithocarpus_Edulis', 'Quercus_Ilex', 'Lithocarpus_Edulis',\n",
       "       'Salix_Fragilis', 'Betula_Pendula', 'Cotinus_Coggygria',\n",
       "       'Fagus_Sylvatica', 'Quercus_Hartwissiana', 'Alnus_Cordata',\n",
       "       'Crataegus_Monogyna', 'Quercus_x_Hispanica', 'Quercus_Coccifera',\n",
       "       'Olea_Europaea', 'Populus_Nigra', 'Cornus_Macrophylla',\n",
       "       'Ulmus_Bergmanniana', 'Tilia_Platyphyllos', 'Acer_Rufinerve',\n",
       "       'Quercus_Brantii', 'Ginkgo_Biloba', 'Alnus_Viridis',\n",
       "       'Alnus_Cordata', 'Tilia_Tomentosa', 'Acer_Rufinerve',\n",
       "       'Acer_Rufinerve', 'Quercus_Greggii', 'Populus_Adenopoda',\n",
       "       'Quercus_Pontica', 'Eucalyptus_Urnigera',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Cornus_Chinensis',\n",
       "       'Alnus_Maximowiczii', 'Quercus_Infectoria_sub', 'Quercus_Cerris',\n",
       "       'Salix_Intergra', 'Viburnum_Tinus', 'Betula_Austrosinensis',\n",
       "       'Phildelphus', 'Quercus_Chrysolepis', 'Sorbus_Aria',\n",
       "       'Cornus_Macrophylla', 'Eucalyptus_Glaucescens', 'Quercus_Texana',\n",
       "       'Salix_Intergra', 'Quercus_Greggii', 'Quercus_Crassipes',\n",
       "       'Tilia_Oliveri', 'Eucalyptus_Glaucescens', 'Quercus_Variabilis',\n",
       "       'Quercus_Nigra', 'Populus_Grandidentata', 'Quercus_Vulcanica',\n",
       "       'Ilex_Cornuta', 'Acer_Rubrum', 'Rhododendron_x_Russellianum',\n",
       "       'Salix_Fragilis', 'Quercus_Crassipes', 'Crataegus_Monogyna',\n",
       "       'Rhododendron_x_Russellianum', 'Morus_Nigra', 'Populus_Nigra',\n",
       "       'Acer_Rufinerve', 'Sorbus_Aria', 'Morus_Nigra', 'Alnus_Viridis',\n",
       "       'Zelkova_Serrata', 'Eucalyptus_Glaucescens', 'Crataegus_Monogyna',\n",
       "       'Liriodendron_Tulipifera', 'Quercus_Nigra', 'Quercus_Pontica',\n",
       "       'Acer_Rubrum', 'Quercus_Alnifolia', 'Quercus_Nigra',\n",
       "       'Olea_Europaea', 'Quercus_Ilex', 'Cornus_Chinensis',\n",
       "       'Alnus_Sieboldiana', 'Populus_Adenopoda', 'Sorbus_Aria',\n",
       "       'Quercus_Brantii', 'Cornus_Chinensis', 'Betula_Austrosinensis',\n",
       "       'Magnolia_Salicifolia', 'Magnolia_Salicifolia',\n",
       "       'Eucalyptus_Glaucescens', 'Quercus_Nigra', 'Acer_Platanoids',\n",
       "       'Quercus_Crassifolia', 'Ulmus_Bergmanniana', 'Cornus_Controversa',\n",
       "       'Betula_Austrosinensis', 'Quercus_Afares', 'Eucalyptus_Neglecta',\n",
       "       'Pterocarya_Stenoptera', 'Acer_Saccharinum', 'Callicarpa_Bodinieri',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Crassifolia', 'Castanea_Sativa', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Urnigera', 'Fagus_Sylvatica', 'Quercus_Vulcanica',\n",
       "       'Quercus_Ilex', 'Rhododendron_x_Russellianum',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Trojana', 'Quercus_Greggii',\n",
       "       'Viburnum_Tinus', 'Alnus_Sieboldiana', 'Ilex_Cornuta',\n",
       "       'Cytisus_Battandieri', 'Ulmus_Bergmanniana',\n",
       "       'Liriodendron_Tulipifera', 'Quercus_Phellos', 'Eucalyptus_Neglecta',\n",
       "       'Sorbus_Aria', 'Cornus_Chinensis', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Palustris', 'Quercus_Ilex', 'Alnus_Maximowiczii',\n",
       "       'Ginkgo_Biloba', 'Acer_Mono', 'Quercus_Pubescens',\n",
       "       'Magnolia_Heptapeta', 'Acer_Pictum', 'Lithocarpus_Cleistocarpus',\n",
       "       'Quercus_Imbricaria', 'Quercus_Suber', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Crassipes', 'Quercus_Chrysolepis', 'Quercus_Suber',\n",
       "       'Quercus_Texana', 'Quercus_Ellipsoidalis', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Canariensis', 'Liquidambar_Styraciflua',\n",
       "       'Quercus_x_Hispanica', 'Cotinus_Coggygria', 'Cornus_Macrophylla',\n",
       "       'Viburnum_Tinus', 'Alnus_Cordata', 'Quercus_Coccifera',\n",
       "       'Quercus_Coccinea', 'Eucalyptus_Glaucescens', 'Salix_Fragilis',\n",
       "       'Quercus_Canariensis', 'Quercus_Alnifolia', 'Quercus_Vulcanica',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Kewensis', 'Acer_Rufinerve',\n",
       "       'Magnolia_Heptapeta', 'Ulmus_Bergmanniana', 'Magnolia_Heptapeta',\n",
       "       'Quercus_Texana', 'Prunus_X_Shmittii', 'Quercus_Pontica',\n",
       "       'Quercus_Alnifolia', 'Quercus_Pontica', 'Quercus_Castaneifolia',\n",
       "       'Acer_Saccharinum', 'Prunus_Avium', 'Quercus_Variabilis',\n",
       "       'Sorbus_Aria', 'Alnus_Rubra', 'Viburnum_Tinus', 'Quercus_Texana',\n",
       "       'Quercus_Greggii', 'Acer_Rubrum', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Rhysophylla',\n",
       "       'Alnus_Maximowiczii', 'Liriodendron_Tulipifera',\n",
       "       'Quercus_Infectoria_sub', 'Cornus_Controversa',\n",
       "       'Eucalyptus_Glaucescens', 'Populus_Nigra', 'Quercus_Pubescens',\n",
       "       'Phildelphus', 'Acer_Mono', 'Alnus_Maximowiczii', 'Prunus_Avium',\n",
       "       'Quercus_Brantii', 'Quercus_Cerris', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Shumardii', 'Quercus_Cerris',\n",
       "       'Eucalyptus_Urnigera', 'Acer_Opalus', 'Rhododendron_x_Russellianum',\n",
       "       'Lithocarpus_Cleistocarpus', 'Ilex_Aquifolium',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Pyrenaica',\n",
       "       'Cercis_Siliquastrum', 'Acer_Pictum', 'Quercus_Pyrenaica',\n",
       "       'Quercus_x_Turneri', 'Quercus_Infectoria_sub', 'Quercus_Trojana',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Chrysolepis', 'Prunus_Avium',\n",
       "       'Castanea_Sativa', 'Rhododendron_x_Russellianum', 'Viburnum_Tinus',\n",
       "       'Olea_Europaea', 'Cornus_Controversa', 'Populus_Adenopoda',\n",
       "       'Quercus_x_Turneri', 'Quercus_Texana', 'Pterocarya_Stenoptera',\n",
       "       'Quercus_Ellipsoidalis', 'Cornus_Macrophylla', 'Tilia_Tomentosa',\n",
       "       'Quercus_Kewensis', 'Quercus_Canariensis', 'Acer_Pictum',\n",
       "       'Quercus_Semecarpifolia', 'Phildelphus', 'Alnus_Rubra',\n",
       "       'Quercus_Afares', 'Acer_Pictum', 'Quercus_Alnifolia',\n",
       "       'Tilia_Oliveri', 'Acer_Palmatum', 'Quercus_Variabilis',\n",
       "       'Acer_Circinatum', 'Ilex_Aquifolium', 'Pterocarya_Stenoptera',\n",
       "       'Crataegus_Monogyna', 'Callicarpa_Bodinieri', 'Populus_Adenopoda',\n",
       "       'Alnus_Sieboldiana', 'Cornus_Macrophylla', 'Quercus_Phillyraeoides',\n",
       "       'Salix_Fragilis', 'Quercus_Rubra', 'Quercus_Imbricaria',\n",
       "       'Morus_Nigra', 'Alnus_Maximowiczii', 'Populus_Nigra',\n",
       "       'Prunus_Avium', 'Quercus_Kewensis', 'Acer_Capillipes',\n",
       "       'Callicarpa_Bodinieri', 'Zelkova_Serrata', 'Populus_Adenopoda',\n",
       "       'Quercus_Vulcanica', 'Eucalyptus_Neglecta', 'Quercus_Brantii',\n",
       "       'Acer_Circinatum', 'Populus_Grandidentata', 'Acer_Opalus',\n",
       "       'Acer_Saccharinum', 'Alnus_Sieboldiana', 'Acer_Mono',\n",
       "       'Quercus_Ilex', 'Quercus_Coccinea', 'Quercus_Semecarpifolia',\n",
       "       'Acer_Platanoids', 'Betula_Pendula', 'Phildelphus',\n",
       "       'Quercus_Trojana', 'Crataegus_Monogyna', 'Acer_Circinatum',\n",
       "       'Alnus_Rubra', 'Pterocarya_Stenoptera', 'Quercus_Palustris',\n",
       "       'Salix_Fragilis', 'Quercus_Coccifera', 'Tilia_Platyphyllos',\n",
       "       'Magnolia_Heptapeta', 'Olea_Europaea', 'Arundinaria_Simonii',\n",
       "       'Lithocarpus_Edulis', 'Quercus_Castaneifolia',\n",
       "       'Arundinaria_Simonii', 'Tilia_Platyphyllos', 'Acer_Palmatum',\n",
       "       'Salix_Intergra', 'Rhododendron_x_Russellianum',\n",
       "       'Quercus_Castaneifolia', 'Magnolia_Salicifolia',\n",
       "       'Quercus_Castaneifolia', 'Betula_Pendula', 'Ilex_Cornuta',\n",
       "       'Tilia_Tomentosa', 'Quercus_Chrysolepis', 'Tilia_Oliveri',\n",
       "       'Quercus_Suber', 'Crataegus_Monogyna', 'Quercus_Crassifolia',\n",
       "       'Salix_Fragilis', 'Pterocarya_Stenoptera', 'Salix_Fragilis',\n",
       "       'Eucalyptus_Urnigera', 'Quercus_Hartwissiana', 'Quercus_Coccifera',\n",
       "       'Acer_Palmatum', 'Castanea_Sativa', 'Acer_Palmatum',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Pontica', 'Quercus_Palustris',\n",
       "       'Cercis_Siliquastrum', 'Quercus_Canariensis', 'Alnus_Sieboldiana',\n",
       "       'Betula_Austrosinensis', 'Quercus_Phellos',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Crassipes', 'Cotinus_Coggygria', 'Quercus_Phellos',\n",
       "       'Acer_Rubrum', 'Alnus_Rubra', 'Callicarpa_Bodinieri',\n",
       "       'Quercus_Rhysophylla', 'Salix_Intergra', 'Quercus_Dolicholepis',\n",
       "       'Alnus_Viridis', 'Acer_Opalus', 'Quercus_Rubra',\n",
       "       'Quercus_Hartwissiana', 'Lithocarpus_Cleistocarpus',\n",
       "       'Cytisus_Battandieri', 'Quercus_Afares', 'Ulmus_Bergmanniana',\n",
       "       'Zelkova_Serrata', 'Quercus_Crassifolia', 'Quercus_Phellos',\n",
       "       'Quercus_Coccifera', 'Lithocarpus_Edulis', 'Quercus_x_Turneri',\n",
       "       'Tilia_Platyphyllos', 'Castanea_Sativa', 'Acer_Rubrum',\n",
       "       'Quercus_Trojana', 'Quercus_Afares', 'Acer_Opalus', 'Sorbus_Aria',\n",
       "       'Quercus_Rhysophylla', 'Acer_Rubrum', 'Quercus_Greggii',\n",
       "       'Quercus_Crassipes', 'Quercus_Kewensis', 'Cornus_Controversa',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Hartwissiana', 'Prunus_X_Shmittii',\n",
       "       'Morus_Nigra', 'Tilia_Oliveri', 'Celtis_Koraiensis', 'Sorbus_Aria',\n",
       "       'Callicarpa_Bodinieri', 'Quercus_Nigra', 'Acer_Saccharinum',\n",
       "       'Acer_Saccharinum', 'Eucalyptus_Neglecta', 'Quercus_Vulcanica',\n",
       "       'Quercus_Imbricaria', 'Acer_Capillipes', 'Liquidambar_Styraciflua',\n",
       "       'Zelkova_Serrata', 'Sorbus_Aria', 'Salix_Intergra',\n",
       "       'Fagus_Sylvatica', 'Quercus_Brantii', 'Magnolia_Heptapeta',\n",
       "       'Cornus_Macrophylla', 'Quercus_Vulcanica', 'Viburnum_Tinus',\n",
       "       'Betula_Pendula', 'Acer_Rubrum', 'Alnus_Maximowiczii',\n",
       "       'Viburnum_Tinus', 'Magnolia_Heptapeta', 'Quercus_Cerris',\n",
       "       'Salix_Intergra', 'Quercus_Semecarpifolia', 'Populus_Grandidentata',\n",
       "       'Quercus_Greggii', 'Viburnum_x_Rhytidophylloides', 'Morus_Nigra',\n",
       "       'Castanea_Sativa', 'Quercus_Pontica', 'Alnus_Maximowiczii',\n",
       "       'Alnus_Cordata', 'Quercus_Dolicholepis', 'Ulmus_Bergmanniana',\n",
       "       'Eucalyptus_Glaucescens', 'Viburnum_Tinus', 'Alnus_Cordata',\n",
       "       'Acer_Rubrum', 'Quercus_Infectoria_sub', 'Acer_Rubrum',\n",
       "       'Tilia_Platyphyllos', 'Ilex_Cornuta', 'Olea_Europaea',\n",
       "       'Quercus_Infectoria_sub', 'Morus_Nigra', 'Alnus_Viridis',\n",
       "       'Cornus_Chinensis', 'Quercus_Coccifera', 'Alnus_Rubra',\n",
       "       'Quercus_Palustris', 'Cytisus_Battandieri', 'Zelkova_Serrata',\n",
       "       'Quercus_Phellos', 'Sorbus_Aria', 'Acer_Opalus', 'Phildelphus',\n",
       "       'Castanea_Sativa', 'Quercus_Crassipes', 'Quercus_Pubescens',\n",
       "       'Celtis_Koraiensis', 'Quercus_Suber', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Imbricaria', 'Quercus_Cerris', 'Crataegus_Monogyna',\n",
       "       'Prunus_Avium', 'Alnus_Maximowiczii', 'Fagus_Sylvatica',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Dolicholepis', 'Quercus_Ilex',\n",
       "       'Castanea_Sativa', 'Quercus_Pubescens',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Tilia_Tomentosa',\n",
       "       'Alnus_Sieboldiana', 'Quercus_Pubescens', 'Ulmus_Bergmanniana',\n",
       "       'Ginkgo_Biloba', 'Quercus_Afares', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Quercus_Alnifolia', 'Quercus_x_Turneri', 'Ginkgo_Biloba',\n",
       "       'Acer_Opalus', 'Prunus_Avium', 'Quercus_Rhysophylla',\n",
       "       'Prunus_X_Shmittii', 'Ilex_Aquifolium', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Greggii', 'Pterocarya_Stenoptera',\n",
       "       'Quercus_Infectoria_sub', 'Phildelphus', 'Tilia_Tomentosa',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Infectoria_sub', 'Prunus_Avium',\n",
       "       'Quercus_Texana', 'Acer_Mono', 'Eucalyptus_Urnigera',\n",
       "       'Cornus_Controversa', 'Ilex_Aquifolium', 'Ilex_Cornuta',\n",
       "       'Quercus_Ilex', 'Ilex_Aquifolium', 'Liquidambar_Styraciflua',\n",
       "       'Rhododendron_x_Russellianum', 'Quercus_Infectoria_sub',\n",
       "       'Cytisus_Battandieri', 'Quercus_x_Turneri', 'Quercus_Canariensis',\n",
       "       'Quercus_Semecarpifolia', 'Ilex_Cornuta', 'Quercus_Imbricaria',\n",
       "       'Quercus_Hartwissiana', 'Callicarpa_Bodinieri', 'Acer_Palmatum',\n",
       "       'Crataegus_Monogyna', 'Eucalyptus_Urnigera', 'Quercus_Alnifolia',\n",
       "       'Quercus_x_Hispanica', 'Acer_Capillipes', 'Eucalyptus_Neglecta',\n",
       "       'Quercus_Shumardii', 'Quercus_Imbricaria', 'Ulmus_Bergmanniana',\n",
       "       'Magnolia_Heptapeta', 'Cotinus_Coggygria', 'Quercus_Nigra',\n",
       "       'Quercus_Coccifera', 'Liquidambar_Styraciflua',\n",
       "       'Cytisus_Battandieri', 'Betula_Pendula', 'Lithocarpus_Edulis',\n",
       "       'Sorbus_Aria', 'Alnus_Sieboldiana', 'Cornus_Controversa',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Rhysophylla', 'Cornus_Chinensis', 'Pterocarya_Stenoptera',\n",
       "       'Zelkova_Serrata', 'Prunus_X_Shmittii', 'Populus_Nigra',\n",
       "       'Prunus_X_Shmittii', 'Acer_Pictum', 'Quercus_Ellipsoidalis',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Quercus_Shumardii',\n",
       "       'Quercus_Brantii', 'Celtis_Koraiensis', 'Cotinus_Coggygria',\n",
       "       'Quercus_Rubra', 'Alnus_Cordata', 'Quercus_Hartwissiana',\n",
       "       'Ilex_Aquifolium', 'Morus_Nigra', 'Quercus_Vulcanica',\n",
       "       'Salix_Intergra', 'Zelkova_Serrata', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Quercus_Palustris', 'Ulmus_Bergmanniana', 'Quercus_Alnifolia',\n",
       "       'Acer_Opalus', 'Eucalyptus_Neglecta', 'Quercus_Kewensis',\n",
       "       'Quercus_Agrifolia', 'Quercus_Nigra', 'Cytisus_Battandieri',\n",
       "       'Cornus_Chinensis', 'Ilex_Cornuta', 'Acer_Platanoids',\n",
       "       'Prunus_X_Shmittii', 'Quercus_Phellos', 'Cercis_Siliquastrum',\n",
       "       'Salix_Fragilis', 'Betula_Pendula', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Quercus_Phellos', 'Quercus_Shumardii', 'Alnus_Cordata',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Greggii', 'Tilia_Oliveri',\n",
       "       'Lithocarpus_Edulis', 'Arundinaria_Simonii',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_x_Turneri',\n",
       "       'Quercus_Phillyraeoides', 'Prunus_X_Shmittii', 'Olea_Europaea',\n",
       "       'Quercus_Texana', 'Ilex_Aquifolium', 'Quercus_Cerris',\n",
       "       'Acer_Opalus', 'Olea_Europaea', 'Acer_Circinatum',\n",
       "       'Quercus_Brantii', 'Quercus_Castaneifolia', 'Salix_Intergra',\n",
       "       'Castanea_Sativa', 'Acer_Platanoids', 'Eucalyptus_Urnigera',\n",
       "       'Quercus_x_Hispanica', 'Quercus_Greggii', 'Pterocarya_Stenoptera',\n",
       "       'Acer_Mono', 'Acer_Rufinerve', 'Populus_Grandidentata',\n",
       "       'Quercus_Pyrenaica', 'Tilia_Oliveri', 'Acer_Capillipes',\n",
       "       'Cornus_Controversa', 'Quercus_Kewensis', 'Quercus_Coccinea',\n",
       "       'Quercus_Shumardii', 'Salix_Intergra', 'Ginkgo_Biloba',\n",
       "       'Acer_Pictum', 'Quercus_Coccinea', 'Quercus_Vulcanica',\n",
       "       'Salix_Fragilis', 'Tilia_Tomentosa', 'Populus_Grandidentata',\n",
       "       'Prunus_X_Shmittii', 'Quercus_x_Hispanica', 'Quercus_Suber',\n",
       "       'Alnus_Viridis', 'Acer_Palmatum', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Cerris', 'Quercus_Texana', 'Acer_Mono',\n",
       "       'Quercus_Vulcanica', 'Quercus_Nigra', 'Rhododendron_x_Russellianum',\n",
       "       'Acer_Capillipes', 'Quercus_Pubescens', 'Alnus_Viridis',\n",
       "       'Quercus_x_Hispanica', 'Quercus_Suber',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Quercus_Nigra', 'Quercus_Phellos',\n",
       "       'Ilex_Cornuta', 'Magnolia_Salicifolia', 'Acer_Pictum',\n",
       "       'Alnus_Maximowiczii', 'Quercus_Rubra', 'Quercus_Afares'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].values\n",
    "leaf_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].values\n",
    "leaf_class = np.sort(leaf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acer_Capillipes', 'Acer_Capillipes', 'Acer_Capillipes',\n",
       "       'Acer_Capillipes', 'Acer_Capillipes', 'Acer_Capillipes',\n",
       "       'Acer_Capillipes', 'Acer_Capillipes', 'Acer_Capillipes',\n",
       "       'Acer_Capillipes', 'Acer_Circinatum', 'Acer_Circinatum',\n",
       "       'Acer_Circinatum', 'Acer_Circinatum', 'Acer_Circinatum',\n",
       "       'Acer_Circinatum', 'Acer_Circinatum', 'Acer_Circinatum',\n",
       "       'Acer_Circinatum', 'Acer_Circinatum', 'Acer_Mono', 'Acer_Mono',\n",
       "       'Acer_Mono', 'Acer_Mono', 'Acer_Mono', 'Acer_Mono', 'Acer_Mono',\n",
       "       'Acer_Mono', 'Acer_Mono', 'Acer_Mono', 'Acer_Opalus', 'Acer_Opalus',\n",
       "       'Acer_Opalus', 'Acer_Opalus', 'Acer_Opalus', 'Acer_Opalus',\n",
       "       'Acer_Opalus', 'Acer_Opalus', 'Acer_Opalus', 'Acer_Opalus',\n",
       "       'Acer_Palmatum', 'Acer_Palmatum', 'Acer_Palmatum', 'Acer_Palmatum',\n",
       "       'Acer_Palmatum', 'Acer_Palmatum', 'Acer_Palmatum', 'Acer_Palmatum',\n",
       "       'Acer_Palmatum', 'Acer_Palmatum', 'Acer_Pictum', 'Acer_Pictum',\n",
       "       'Acer_Pictum', 'Acer_Pictum', 'Acer_Pictum', 'Acer_Pictum',\n",
       "       'Acer_Pictum', 'Acer_Pictum', 'Acer_Pictum', 'Acer_Pictum',\n",
       "       'Acer_Platanoids', 'Acer_Platanoids', 'Acer_Platanoids',\n",
       "       'Acer_Platanoids', 'Acer_Platanoids', 'Acer_Platanoids',\n",
       "       'Acer_Platanoids', 'Acer_Platanoids', 'Acer_Platanoids',\n",
       "       'Acer_Platanoids', 'Acer_Rubrum', 'Acer_Rubrum', 'Acer_Rubrum',\n",
       "       'Acer_Rubrum', 'Acer_Rubrum', 'Acer_Rubrum', 'Acer_Rubrum',\n",
       "       'Acer_Rubrum', 'Acer_Rubrum', 'Acer_Rubrum', 'Acer_Rufinerve',\n",
       "       'Acer_Rufinerve', 'Acer_Rufinerve', 'Acer_Rufinerve',\n",
       "       'Acer_Rufinerve', 'Acer_Rufinerve', 'Acer_Rufinerve',\n",
       "       'Acer_Rufinerve', 'Acer_Rufinerve', 'Acer_Rufinerve',\n",
       "       'Acer_Saccharinum', 'Acer_Saccharinum', 'Acer_Saccharinum',\n",
       "       'Acer_Saccharinum', 'Acer_Saccharinum', 'Acer_Saccharinum',\n",
       "       'Acer_Saccharinum', 'Acer_Saccharinum', 'Acer_Saccharinum',\n",
       "       'Acer_Saccharinum', 'Alnus_Cordata', 'Alnus_Cordata',\n",
       "       'Alnus_Cordata', 'Alnus_Cordata', 'Alnus_Cordata', 'Alnus_Cordata',\n",
       "       'Alnus_Cordata', 'Alnus_Cordata', 'Alnus_Cordata', 'Alnus_Cordata',\n",
       "       'Alnus_Maximowiczii', 'Alnus_Maximowiczii', 'Alnus_Maximowiczii',\n",
       "       'Alnus_Maximowiczii', 'Alnus_Maximowiczii', 'Alnus_Maximowiczii',\n",
       "       'Alnus_Maximowiczii', 'Alnus_Maximowiczii', 'Alnus_Maximowiczii',\n",
       "       'Alnus_Maximowiczii', 'Alnus_Rubra', 'Alnus_Rubra', 'Alnus_Rubra',\n",
       "       'Alnus_Rubra', 'Alnus_Rubra', 'Alnus_Rubra', 'Alnus_Rubra',\n",
       "       'Alnus_Rubra', 'Alnus_Rubra', 'Alnus_Rubra', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Sieboldiana', 'Alnus_Sieboldiana', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Sieboldiana', 'Alnus_Sieboldiana', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Sieboldiana', 'Alnus_Sieboldiana', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Viridis', 'Alnus_Viridis', 'Alnus_Viridis', 'Alnus_Viridis',\n",
       "       'Alnus_Viridis', 'Alnus_Viridis', 'Alnus_Viridis', 'Alnus_Viridis',\n",
       "       'Alnus_Viridis', 'Alnus_Viridis', 'Arundinaria_Simonii',\n",
       "       'Arundinaria_Simonii', 'Arundinaria_Simonii', 'Arundinaria_Simonii',\n",
       "       'Arundinaria_Simonii', 'Arundinaria_Simonii', 'Arundinaria_Simonii',\n",
       "       'Arundinaria_Simonii', 'Arundinaria_Simonii', 'Arundinaria_Simonii',\n",
       "       'Betula_Austrosinensis', 'Betula_Austrosinensis',\n",
       "       'Betula_Austrosinensis', 'Betula_Austrosinensis',\n",
       "       'Betula_Austrosinensis', 'Betula_Austrosinensis',\n",
       "       'Betula_Austrosinensis', 'Betula_Austrosinensis',\n",
       "       'Betula_Austrosinensis', 'Betula_Austrosinensis', 'Betula_Pendula',\n",
       "       'Betula_Pendula', 'Betula_Pendula', 'Betula_Pendula',\n",
       "       'Betula_Pendula', 'Betula_Pendula', 'Betula_Pendula',\n",
       "       'Betula_Pendula', 'Betula_Pendula', 'Betula_Pendula',\n",
       "       'Callicarpa_Bodinieri', 'Callicarpa_Bodinieri',\n",
       "       'Callicarpa_Bodinieri', 'Callicarpa_Bodinieri',\n",
       "       'Callicarpa_Bodinieri', 'Callicarpa_Bodinieri',\n",
       "       'Callicarpa_Bodinieri', 'Callicarpa_Bodinieri',\n",
       "       'Callicarpa_Bodinieri', 'Callicarpa_Bodinieri', 'Castanea_Sativa',\n",
       "       'Castanea_Sativa', 'Castanea_Sativa', 'Castanea_Sativa',\n",
       "       'Castanea_Sativa', 'Castanea_Sativa', 'Castanea_Sativa',\n",
       "       'Castanea_Sativa', 'Castanea_Sativa', 'Castanea_Sativa',\n",
       "       'Celtis_Koraiensis', 'Celtis_Koraiensis', 'Celtis_Koraiensis',\n",
       "       'Celtis_Koraiensis', 'Celtis_Koraiensis', 'Celtis_Koraiensis',\n",
       "       'Celtis_Koraiensis', 'Celtis_Koraiensis', 'Celtis_Koraiensis',\n",
       "       'Celtis_Koraiensis', 'Cercis_Siliquastrum', 'Cercis_Siliquastrum',\n",
       "       'Cercis_Siliquastrum', 'Cercis_Siliquastrum', 'Cercis_Siliquastrum',\n",
       "       'Cercis_Siliquastrum', 'Cercis_Siliquastrum', 'Cercis_Siliquastrum',\n",
       "       'Cercis_Siliquastrum', 'Cercis_Siliquastrum', 'Cornus_Chinensis',\n",
       "       'Cornus_Chinensis', 'Cornus_Chinensis', 'Cornus_Chinensis',\n",
       "       'Cornus_Chinensis', 'Cornus_Chinensis', 'Cornus_Chinensis',\n",
       "       'Cornus_Chinensis', 'Cornus_Chinensis', 'Cornus_Chinensis',\n",
       "       'Cornus_Controversa', 'Cornus_Controversa', 'Cornus_Controversa',\n",
       "       'Cornus_Controversa', 'Cornus_Controversa', 'Cornus_Controversa',\n",
       "       'Cornus_Controversa', 'Cornus_Controversa', 'Cornus_Controversa',\n",
       "       'Cornus_Controversa', 'Cornus_Macrophylla', 'Cornus_Macrophylla',\n",
       "       'Cornus_Macrophylla', 'Cornus_Macrophylla', 'Cornus_Macrophylla',\n",
       "       'Cornus_Macrophylla', 'Cornus_Macrophylla', 'Cornus_Macrophylla',\n",
       "       'Cornus_Macrophylla', 'Cornus_Macrophylla', 'Cotinus_Coggygria',\n",
       "       'Cotinus_Coggygria', 'Cotinus_Coggygria', 'Cotinus_Coggygria',\n",
       "       'Cotinus_Coggygria', 'Cotinus_Coggygria', 'Cotinus_Coggygria',\n",
       "       'Cotinus_Coggygria', 'Cotinus_Coggygria', 'Cotinus_Coggygria',\n",
       "       'Crataegus_Monogyna', 'Crataegus_Monogyna', 'Crataegus_Monogyna',\n",
       "       'Crataegus_Monogyna', 'Crataegus_Monogyna', 'Crataegus_Monogyna',\n",
       "       'Crataegus_Monogyna', 'Crataegus_Monogyna', 'Crataegus_Monogyna',\n",
       "       'Crataegus_Monogyna', 'Cytisus_Battandieri', 'Cytisus_Battandieri',\n",
       "       'Cytisus_Battandieri', 'Cytisus_Battandieri', 'Cytisus_Battandieri',\n",
       "       'Cytisus_Battandieri', 'Cytisus_Battandieri', 'Cytisus_Battandieri',\n",
       "       'Cytisus_Battandieri', 'Cytisus_Battandieri',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Glaucescens',\n",
       "       'Eucalyptus_Neglecta', 'Eucalyptus_Neglecta', 'Eucalyptus_Neglecta',\n",
       "       'Eucalyptus_Neglecta', 'Eucalyptus_Neglecta', 'Eucalyptus_Neglecta',\n",
       "       'Eucalyptus_Neglecta', 'Eucalyptus_Neglecta', 'Eucalyptus_Neglecta',\n",
       "       'Eucalyptus_Neglecta', 'Eucalyptus_Urnigera', 'Eucalyptus_Urnigera',\n",
       "       'Eucalyptus_Urnigera', 'Eucalyptus_Urnigera', 'Eucalyptus_Urnigera',\n",
       "       'Eucalyptus_Urnigera', 'Eucalyptus_Urnigera', 'Eucalyptus_Urnigera',\n",
       "       'Eucalyptus_Urnigera', 'Eucalyptus_Urnigera', 'Fagus_Sylvatica',\n",
       "       'Fagus_Sylvatica', 'Fagus_Sylvatica', 'Fagus_Sylvatica',\n",
       "       'Fagus_Sylvatica', 'Fagus_Sylvatica', 'Fagus_Sylvatica',\n",
       "       'Fagus_Sylvatica', 'Fagus_Sylvatica', 'Fagus_Sylvatica',\n",
       "       'Ginkgo_Biloba', 'Ginkgo_Biloba', 'Ginkgo_Biloba', 'Ginkgo_Biloba',\n",
       "       'Ginkgo_Biloba', 'Ginkgo_Biloba', 'Ginkgo_Biloba', 'Ginkgo_Biloba',\n",
       "       'Ginkgo_Biloba', 'Ginkgo_Biloba', 'Ilex_Aquifolium',\n",
       "       'Ilex_Aquifolium', 'Ilex_Aquifolium', 'Ilex_Aquifolium',\n",
       "       'Ilex_Aquifolium', 'Ilex_Aquifolium', 'Ilex_Aquifolium',\n",
       "       'Ilex_Aquifolium', 'Ilex_Aquifolium', 'Ilex_Aquifolium',\n",
       "       'Ilex_Cornuta', 'Ilex_Cornuta', 'Ilex_Cornuta', 'Ilex_Cornuta',\n",
       "       'Ilex_Cornuta', 'Ilex_Cornuta', 'Ilex_Cornuta', 'Ilex_Cornuta',\n",
       "       'Ilex_Cornuta', 'Ilex_Cornuta', 'Liquidambar_Styraciflua',\n",
       "       'Liquidambar_Styraciflua', 'Liquidambar_Styraciflua',\n",
       "       'Liquidambar_Styraciflua', 'Liquidambar_Styraciflua',\n",
       "       'Liquidambar_Styraciflua', 'Liquidambar_Styraciflua',\n",
       "       'Liquidambar_Styraciflua', 'Liquidambar_Styraciflua',\n",
       "       'Liquidambar_Styraciflua', 'Liriodendron_Tulipifera',\n",
       "       'Liriodendron_Tulipifera', 'Liriodendron_Tulipifera',\n",
       "       'Liriodendron_Tulipifera', 'Liriodendron_Tulipifera',\n",
       "       'Liriodendron_Tulipifera', 'Liriodendron_Tulipifera',\n",
       "       'Liriodendron_Tulipifera', 'Liriodendron_Tulipifera',\n",
       "       'Liriodendron_Tulipifera', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Cleistocarpus', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Cleistocarpus', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Cleistocarpus', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Cleistocarpus', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Cleistocarpus', 'Lithocarpus_Edulis',\n",
       "       'Lithocarpus_Edulis', 'Lithocarpus_Edulis', 'Lithocarpus_Edulis',\n",
       "       'Lithocarpus_Edulis', 'Lithocarpus_Edulis', 'Lithocarpus_Edulis',\n",
       "       'Lithocarpus_Edulis', 'Lithocarpus_Edulis', 'Lithocarpus_Edulis',\n",
       "       'Magnolia_Heptapeta', 'Magnolia_Heptapeta', 'Magnolia_Heptapeta',\n",
       "       'Magnolia_Heptapeta', 'Magnolia_Heptapeta', 'Magnolia_Heptapeta',\n",
       "       'Magnolia_Heptapeta', 'Magnolia_Heptapeta', 'Magnolia_Heptapeta',\n",
       "       'Magnolia_Heptapeta', 'Magnolia_Salicifolia',\n",
       "       'Magnolia_Salicifolia', 'Magnolia_Salicifolia',\n",
       "       'Magnolia_Salicifolia', 'Magnolia_Salicifolia',\n",
       "       'Magnolia_Salicifolia', 'Magnolia_Salicifolia',\n",
       "       'Magnolia_Salicifolia', 'Magnolia_Salicifolia',\n",
       "       'Magnolia_Salicifolia', 'Morus_Nigra', 'Morus_Nigra', 'Morus_Nigra',\n",
       "       'Morus_Nigra', 'Morus_Nigra', 'Morus_Nigra', 'Morus_Nigra',\n",
       "       'Morus_Nigra', 'Morus_Nigra', 'Morus_Nigra', 'Olea_Europaea',\n",
       "       'Olea_Europaea', 'Olea_Europaea', 'Olea_Europaea', 'Olea_Europaea',\n",
       "       'Olea_Europaea', 'Olea_Europaea', 'Olea_Europaea', 'Olea_Europaea',\n",
       "       'Olea_Europaea', 'Phildelphus', 'Phildelphus', 'Phildelphus',\n",
       "       'Phildelphus', 'Phildelphus', 'Phildelphus', 'Phildelphus',\n",
       "       'Phildelphus', 'Phildelphus', 'Phildelphus', 'Populus_Adenopoda',\n",
       "       'Populus_Adenopoda', 'Populus_Adenopoda', 'Populus_Adenopoda',\n",
       "       'Populus_Adenopoda', 'Populus_Adenopoda', 'Populus_Adenopoda',\n",
       "       'Populus_Adenopoda', 'Populus_Adenopoda', 'Populus_Adenopoda',\n",
       "       'Populus_Grandidentata', 'Populus_Grandidentata',\n",
       "       'Populus_Grandidentata', 'Populus_Grandidentata',\n",
       "       'Populus_Grandidentata', 'Populus_Grandidentata',\n",
       "       'Populus_Grandidentata', 'Populus_Grandidentata',\n",
       "       'Populus_Grandidentata', 'Populus_Grandidentata', 'Populus_Nigra',\n",
       "       'Populus_Nigra', 'Populus_Nigra', 'Populus_Nigra', 'Populus_Nigra',\n",
       "       'Populus_Nigra', 'Populus_Nigra', 'Populus_Nigra', 'Populus_Nigra',\n",
       "       'Populus_Nigra', 'Prunus_Avium', 'Prunus_Avium', 'Prunus_Avium',\n",
       "       'Prunus_Avium', 'Prunus_Avium', 'Prunus_Avium', 'Prunus_Avium',\n",
       "       'Prunus_Avium', 'Prunus_Avium', 'Prunus_Avium', 'Prunus_X_Shmittii',\n",
       "       'Prunus_X_Shmittii', 'Prunus_X_Shmittii', 'Prunus_X_Shmittii',\n",
       "       'Prunus_X_Shmittii', 'Prunus_X_Shmittii', 'Prunus_X_Shmittii',\n",
       "       'Prunus_X_Shmittii', 'Prunus_X_Shmittii', 'Prunus_X_Shmittii',\n",
       "       'Pterocarya_Stenoptera', 'Pterocarya_Stenoptera',\n",
       "       'Pterocarya_Stenoptera', 'Pterocarya_Stenoptera',\n",
       "       'Pterocarya_Stenoptera', 'Pterocarya_Stenoptera',\n",
       "       'Pterocarya_Stenoptera', 'Pterocarya_Stenoptera',\n",
       "       'Pterocarya_Stenoptera', 'Pterocarya_Stenoptera', 'Quercus_Afares',\n",
       "       'Quercus_Afares', 'Quercus_Afares', 'Quercus_Afares',\n",
       "       'Quercus_Afares', 'Quercus_Afares', 'Quercus_Afares',\n",
       "       'Quercus_Afares', 'Quercus_Afares', 'Quercus_Afares',\n",
       "       'Quercus_Agrifolia', 'Quercus_Agrifolia', 'Quercus_Agrifolia',\n",
       "       'Quercus_Agrifolia', 'Quercus_Agrifolia', 'Quercus_Agrifolia',\n",
       "       'Quercus_Agrifolia', 'Quercus_Agrifolia', 'Quercus_Agrifolia',\n",
       "       'Quercus_Agrifolia', 'Quercus_Alnifolia', 'Quercus_Alnifolia',\n",
       "       'Quercus_Alnifolia', 'Quercus_Alnifolia', 'Quercus_Alnifolia',\n",
       "       'Quercus_Alnifolia', 'Quercus_Alnifolia', 'Quercus_Alnifolia',\n",
       "       'Quercus_Alnifolia', 'Quercus_Alnifolia', 'Quercus_Brantii',\n",
       "       'Quercus_Brantii', 'Quercus_Brantii', 'Quercus_Brantii',\n",
       "       'Quercus_Brantii', 'Quercus_Brantii', 'Quercus_Brantii',\n",
       "       'Quercus_Brantii', 'Quercus_Brantii', 'Quercus_Brantii',\n",
       "       'Quercus_Canariensis', 'Quercus_Canariensis', 'Quercus_Canariensis',\n",
       "       'Quercus_Canariensis', 'Quercus_Canariensis', 'Quercus_Canariensis',\n",
       "       'Quercus_Canariensis', 'Quercus_Canariensis', 'Quercus_Canariensis',\n",
       "       'Quercus_Canariensis', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Castaneifolia',\n",
       "       'Quercus_Castaneifolia', 'Quercus_Cerris', 'Quercus_Cerris',\n",
       "       'Quercus_Cerris', 'Quercus_Cerris', 'Quercus_Cerris',\n",
       "       'Quercus_Cerris', 'Quercus_Cerris', 'Quercus_Cerris',\n",
       "       'Quercus_Cerris', 'Quercus_Cerris', 'Quercus_Chrysolepis',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Chrysolepis', 'Quercus_Chrysolepis',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Chrysolepis', 'Quercus_Chrysolepis',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Chrysolepis', 'Quercus_Chrysolepis',\n",
       "       'Quercus_Coccifera', 'Quercus_Coccifera', 'Quercus_Coccifera',\n",
       "       'Quercus_Coccifera', 'Quercus_Coccifera', 'Quercus_Coccifera',\n",
       "       'Quercus_Coccifera', 'Quercus_Coccifera', 'Quercus_Coccifera',\n",
       "       'Quercus_Coccifera', 'Quercus_Coccinea', 'Quercus_Coccinea',\n",
       "       'Quercus_Coccinea', 'Quercus_Coccinea', 'Quercus_Coccinea',\n",
       "       'Quercus_Coccinea', 'Quercus_Coccinea', 'Quercus_Coccinea',\n",
       "       'Quercus_Coccinea', 'Quercus_Coccinea', 'Quercus_Crassifolia',\n",
       "       'Quercus_Crassifolia', 'Quercus_Crassifolia', 'Quercus_Crassifolia',\n",
       "       'Quercus_Crassifolia', 'Quercus_Crassifolia', 'Quercus_Crassifolia',\n",
       "       'Quercus_Crassifolia', 'Quercus_Crassifolia', 'Quercus_Crassifolia',\n",
       "       'Quercus_Crassipes', 'Quercus_Crassipes', 'Quercus_Crassipes',\n",
       "       'Quercus_Crassipes', 'Quercus_Crassipes', 'Quercus_Crassipes',\n",
       "       'Quercus_Crassipes', 'Quercus_Crassipes', 'Quercus_Crassipes',\n",
       "       'Quercus_Crassipes', 'Quercus_Dolicholepis', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Dolicholepis', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Ellipsoidalis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Ellipsoidalis', 'Quercus_Greggii',\n",
       "       'Quercus_Greggii', 'Quercus_Greggii', 'Quercus_Greggii',\n",
       "       'Quercus_Greggii', 'Quercus_Greggii', 'Quercus_Greggii',\n",
       "       'Quercus_Greggii', 'Quercus_Greggii', 'Quercus_Greggii',\n",
       "       'Quercus_Hartwissiana', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Hartwissiana', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Hartwissiana', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Hartwissiana', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Hartwissiana', 'Quercus_Hartwissiana', 'Quercus_Ilex',\n",
       "       'Quercus_Ilex', 'Quercus_Ilex', 'Quercus_Ilex', 'Quercus_Ilex',\n",
       "       'Quercus_Ilex', 'Quercus_Ilex', 'Quercus_Ilex', 'Quercus_Ilex',\n",
       "       'Quercus_Ilex', 'Quercus_Imbricaria', 'Quercus_Imbricaria',\n",
       "       'Quercus_Imbricaria', 'Quercus_Imbricaria', 'Quercus_Imbricaria',\n",
       "       'Quercus_Imbricaria', 'Quercus_Imbricaria', 'Quercus_Imbricaria',\n",
       "       'Quercus_Imbricaria', 'Quercus_Imbricaria',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Infectoria_sub', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Kewensis', 'Quercus_Kewensis', 'Quercus_Kewensis',\n",
       "       'Quercus_Kewensis', 'Quercus_Kewensis', 'Quercus_Kewensis',\n",
       "       'Quercus_Kewensis', 'Quercus_Kewensis', 'Quercus_Kewensis',\n",
       "       'Quercus_Kewensis', 'Quercus_Nigra', 'Quercus_Nigra',\n",
       "       'Quercus_Nigra', 'Quercus_Nigra', 'Quercus_Nigra', 'Quercus_Nigra',\n",
       "       'Quercus_Nigra', 'Quercus_Nigra', 'Quercus_Nigra', 'Quercus_Nigra',\n",
       "       'Quercus_Palustris', 'Quercus_Palustris', 'Quercus_Palustris',\n",
       "       'Quercus_Palustris', 'Quercus_Palustris', 'Quercus_Palustris',\n",
       "       'Quercus_Palustris', 'Quercus_Palustris', 'Quercus_Palustris',\n",
       "       'Quercus_Palustris', 'Quercus_Phellos', 'Quercus_Phellos',\n",
       "       'Quercus_Phellos', 'Quercus_Phellos', 'Quercus_Phellos',\n",
       "       'Quercus_Phellos', 'Quercus_Phellos', 'Quercus_Phellos',\n",
       "       'Quercus_Phellos', 'Quercus_Phellos', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Phillyraeoides',\n",
       "       'Quercus_Phillyraeoides', 'Quercus_Pontica', 'Quercus_Pontica',\n",
       "       'Quercus_Pontica', 'Quercus_Pontica', 'Quercus_Pontica',\n",
       "       'Quercus_Pontica', 'Quercus_Pontica', 'Quercus_Pontica',\n",
       "       'Quercus_Pontica', 'Quercus_Pontica', 'Quercus_Pubescens',\n",
       "       'Quercus_Pubescens', 'Quercus_Pubescens', 'Quercus_Pubescens',\n",
       "       'Quercus_Pubescens', 'Quercus_Pubescens', 'Quercus_Pubescens',\n",
       "       'Quercus_Pubescens', 'Quercus_Pubescens', 'Quercus_Pubescens',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Pyrenaica', 'Quercus_Pyrenaica',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Pyrenaica', 'Quercus_Pyrenaica',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Pyrenaica', 'Quercus_Pyrenaica',\n",
       "       'Quercus_Pyrenaica', 'Quercus_Rhysophylla', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Rhysophylla', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Rhysophylla', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Rhysophylla', 'Quercus_Rhysophylla', 'Quercus_Rubra',\n",
       "       'Quercus_Rubra', 'Quercus_Rubra', 'Quercus_Rubra', 'Quercus_Rubra',\n",
       "       'Quercus_Rubra', 'Quercus_Rubra', 'Quercus_Rubra', 'Quercus_Rubra',\n",
       "       'Quercus_Rubra', 'Quercus_Semecarpifolia', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Semecarpifolia', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Semecarpifolia', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Semecarpifolia', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Semecarpifolia', 'Quercus_Semecarpifolia',\n",
       "       'Quercus_Shumardii', 'Quercus_Shumardii', 'Quercus_Shumardii',\n",
       "       'Quercus_Shumardii', 'Quercus_Shumardii', 'Quercus_Shumardii',\n",
       "       'Quercus_Shumardii', 'Quercus_Shumardii', 'Quercus_Shumardii',\n",
       "       'Quercus_Shumardii', 'Quercus_Suber', 'Quercus_Suber',\n",
       "       'Quercus_Suber', 'Quercus_Suber', 'Quercus_Suber', 'Quercus_Suber',\n",
       "       'Quercus_Suber', 'Quercus_Suber', 'Quercus_Suber', 'Quercus_Suber',\n",
       "       'Quercus_Texana', 'Quercus_Texana', 'Quercus_Texana',\n",
       "       'Quercus_Texana', 'Quercus_Texana', 'Quercus_Texana',\n",
       "       'Quercus_Texana', 'Quercus_Texana', 'Quercus_Texana',\n",
       "       'Quercus_Texana', 'Quercus_Trojana', 'Quercus_Trojana',\n",
       "       'Quercus_Trojana', 'Quercus_Trojana', 'Quercus_Trojana',\n",
       "       'Quercus_Trojana', 'Quercus_Trojana', 'Quercus_Trojana',\n",
       "       'Quercus_Trojana', 'Quercus_Trojana', 'Quercus_Variabilis',\n",
       "       'Quercus_Variabilis', 'Quercus_Variabilis', 'Quercus_Variabilis',\n",
       "       'Quercus_Variabilis', 'Quercus_Variabilis', 'Quercus_Variabilis',\n",
       "       'Quercus_Variabilis', 'Quercus_Variabilis', 'Quercus_Variabilis',\n",
       "       'Quercus_Vulcanica', 'Quercus_Vulcanica', 'Quercus_Vulcanica',\n",
       "       'Quercus_Vulcanica', 'Quercus_Vulcanica', 'Quercus_Vulcanica',\n",
       "       'Quercus_Vulcanica', 'Quercus_Vulcanica', 'Quercus_Vulcanica',\n",
       "       'Quercus_Vulcanica', 'Quercus_x_Hispanica', 'Quercus_x_Hispanica',\n",
       "       'Quercus_x_Hispanica', 'Quercus_x_Hispanica', 'Quercus_x_Hispanica',\n",
       "       'Quercus_x_Hispanica', 'Quercus_x_Hispanica', 'Quercus_x_Hispanica',\n",
       "       'Quercus_x_Hispanica', 'Quercus_x_Hispanica', 'Quercus_x_Turneri',\n",
       "       'Quercus_x_Turneri', 'Quercus_x_Turneri', 'Quercus_x_Turneri',\n",
       "       'Quercus_x_Turneri', 'Quercus_x_Turneri', 'Quercus_x_Turneri',\n",
       "       'Quercus_x_Turneri', 'Quercus_x_Turneri', 'Quercus_x_Turneri',\n",
       "       'Rhododendron_x_Russellianum', 'Rhododendron_x_Russellianum',\n",
       "       'Rhododendron_x_Russellianum', 'Rhododendron_x_Russellianum',\n",
       "       'Rhododendron_x_Russellianum', 'Rhododendron_x_Russellianum',\n",
       "       'Rhododendron_x_Russellianum', 'Rhododendron_x_Russellianum',\n",
       "       'Rhododendron_x_Russellianum', 'Rhododendron_x_Russellianum',\n",
       "       'Salix_Fragilis', 'Salix_Fragilis', 'Salix_Fragilis',\n",
       "       'Salix_Fragilis', 'Salix_Fragilis', 'Salix_Fragilis',\n",
       "       'Salix_Fragilis', 'Salix_Fragilis', 'Salix_Fragilis',\n",
       "       'Salix_Fragilis', 'Salix_Intergra', 'Salix_Intergra',\n",
       "       'Salix_Intergra', 'Salix_Intergra', 'Salix_Intergra',\n",
       "       'Salix_Intergra', 'Salix_Intergra', 'Salix_Intergra',\n",
       "       'Salix_Intergra', 'Salix_Intergra', 'Sorbus_Aria', 'Sorbus_Aria',\n",
       "       'Sorbus_Aria', 'Sorbus_Aria', 'Sorbus_Aria', 'Sorbus_Aria',\n",
       "       'Sorbus_Aria', 'Sorbus_Aria', 'Sorbus_Aria', 'Sorbus_Aria',\n",
       "       'Tilia_Oliveri', 'Tilia_Oliveri', 'Tilia_Oliveri', 'Tilia_Oliveri',\n",
       "       'Tilia_Oliveri', 'Tilia_Oliveri', 'Tilia_Oliveri', 'Tilia_Oliveri',\n",
       "       'Tilia_Oliveri', 'Tilia_Oliveri', 'Tilia_Platyphyllos',\n",
       "       'Tilia_Platyphyllos', 'Tilia_Platyphyllos', 'Tilia_Platyphyllos',\n",
       "       'Tilia_Platyphyllos', 'Tilia_Platyphyllos', 'Tilia_Platyphyllos',\n",
       "       'Tilia_Platyphyllos', 'Tilia_Platyphyllos', 'Tilia_Platyphyllos',\n",
       "       'Tilia_Tomentosa', 'Tilia_Tomentosa', 'Tilia_Tomentosa',\n",
       "       'Tilia_Tomentosa', 'Tilia_Tomentosa', 'Tilia_Tomentosa',\n",
       "       'Tilia_Tomentosa', 'Tilia_Tomentosa', 'Tilia_Tomentosa',\n",
       "       'Tilia_Tomentosa', 'Ulmus_Bergmanniana', 'Ulmus_Bergmanniana',\n",
       "       'Ulmus_Bergmanniana', 'Ulmus_Bergmanniana', 'Ulmus_Bergmanniana',\n",
       "       'Ulmus_Bergmanniana', 'Ulmus_Bergmanniana', 'Ulmus_Bergmanniana',\n",
       "       'Ulmus_Bergmanniana', 'Ulmus_Bergmanniana', 'Viburnum_Tinus',\n",
       "       'Viburnum_Tinus', 'Viburnum_Tinus', 'Viburnum_Tinus',\n",
       "       'Viburnum_Tinus', 'Viburnum_Tinus', 'Viburnum_Tinus',\n",
       "       'Viburnum_Tinus', 'Viburnum_Tinus', 'Viburnum_Tinus',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Viburnum_x_Rhytidophylloides', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Zelkova_Serrata', 'Zelkova_Serrata', 'Zelkova_Serrata',\n",
       "       'Zelkova_Serrata', 'Zelkova_Serrata', 'Zelkova_Serrata',\n",
       "       'Zelkova_Serrata', 'Zelkova_Serrata', 'Zelkova_Serrata',\n",
       "       'Zelkova_Serrata'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acer_Capillipes', 'Acer_Circinatum', 'Acer_Mono', 'Acer_Opalus',\n",
       "       'Acer_Palmatum', 'Acer_Pictum', 'Acer_Platanoids', 'Acer_Rubrum',\n",
       "       'Acer_Rufinerve', 'Acer_Saccharinum', 'Alnus_Cordata',\n",
       "       'Alnus_Maximowiczii', 'Alnus_Rubra', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Viridis', 'Arundinaria_Simonii', 'Betula_Austrosinensis',\n",
       "       'Betula_Pendula', 'Callicarpa_Bodinieri', 'Castanea_Sativa',\n",
       "       'Celtis_Koraiensis', 'Cercis_Siliquastrum', 'Cornus_Chinensis',\n",
       "       'Cornus_Controversa', 'Cornus_Macrophylla', 'Cotinus_Coggygria',\n",
       "       'Crataegus_Monogyna', 'Cytisus_Battandieri',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Neglecta',\n",
       "       'Eucalyptus_Urnigera', 'Fagus_Sylvatica', 'Ginkgo_Biloba',\n",
       "       'Ilex_Aquifolium', 'Ilex_Cornuta', 'Liquidambar_Styraciflua',\n",
       "       'Liriodendron_Tulipifera', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Edulis', 'Magnolia_Heptapeta', 'Magnolia_Salicifolia',\n",
       "       'Morus_Nigra', 'Olea_Europaea', 'Phildelphus', 'Populus_Adenopoda',\n",
       "       'Populus_Grandidentata', 'Populus_Nigra', 'Prunus_Avium',\n",
       "       'Prunus_X_Shmittii', 'Pterocarya_Stenoptera', 'Quercus_Afares',\n",
       "       'Quercus_Agrifolia', 'Quercus_Alnifolia', 'Quercus_Brantii',\n",
       "       'Quercus_Canariensis', 'Quercus_Castaneifolia', 'Quercus_Cerris',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Coccifera', 'Quercus_Coccinea',\n",
       "       'Quercus_Crassifolia', 'Quercus_Crassipes', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Greggii', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Ilex', 'Quercus_Imbricaria', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Kewensis', 'Quercus_Nigra', 'Quercus_Palustris',\n",
       "       'Quercus_Phellos', 'Quercus_Phillyraeoides', 'Quercus_Pontica',\n",
       "       'Quercus_Pubescens', 'Quercus_Pyrenaica', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Rubra', 'Quercus_Semecarpifolia', 'Quercus_Shumardii',\n",
       "       'Quercus_Suber', 'Quercus_Texana', 'Quercus_Trojana',\n",
       "       'Quercus_Variabilis', 'Quercus_Vulcanica', 'Quercus_x_Hispanica',\n",
       "       'Quercus_x_Turneri', 'Rhododendron_x_Russellianum',\n",
       "       'Salix_Fragilis', 'Salix_Intergra', 'Sorbus_Aria', 'Tilia_Oliveri',\n",
       "       'Tilia_Platyphyllos', 'Tilia_Tomentosa', 'Ulmus_Bergmanniana',\n",
       "       'Viburnum_Tinus', 'Viburnum_x_Rhytidophylloides', 'Zelkova_Serrata'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 1 must have a \"write\" method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-976f8c38e69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mleaf_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mopen_file_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mopen_file_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#open_file_object.writerows(zip(ids, output))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 1 must have a \"write\" method"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(\"ids\",predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 1 must have a \"write\" method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-2442ee569e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mleaf_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mopen_file_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mopen_file_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#open_file_object.writerows(zip(ids, output))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 1 must have a \"write\" method"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.insert(a,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "#open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n",
      "Probabilities for each class [[  3.99425231e-08   4.87269844e-10   4.26103948e-10 ...,   4.16270864e-09\n",
      "    1.60003340e-09   3.46161452e-08]\n",
      " [  8.44236834e-10   4.84170895e-08   2.35613486e-08 ...,   9.85724641e-06\n",
      "    8.39615967e-11   4.00867347e-05]\n",
      " [  5.20993540e-07   9.94501103e-01   4.12582747e-06 ...,   9.71842347e-10\n",
      "    1.33413071e-10   5.65613284e-04]\n",
      " ..., \n",
      " [  8.51204969e-06   1.19964202e-07   5.52967556e-07 ...,   1.05231605e-07\n",
      "    6.23987861e-10   9.53194169e-07]\n",
      " [  1.08060593e-08   2.61988182e-07   6.20341930e-05 ...,   6.16262555e-08\n",
      "    6.18578304e-09   5.49004177e-06]\n",
      " [  1.63627968e-15   3.20552624e-06   1.53667043e-05 ...,   5.37001528e-08\n",
      "    2.68391961e-08   1.33041518e-08]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "print ('Probabilities for each class %s'% clf.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "output = clf.predict_proba(X_test).astype(float)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [2, 2],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 1, 1],\n",
       "       [7, 2, 2],\n",
       "       [8, 3, 3]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 1], [2, 2], [3, 3]])\n",
    "np.insert(a,0,[6,7,8],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "output = clf.predict_proba(X_test).astype(str)\n",
    "output = np.insert(output,0,ids,axis=1)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "for word in output:\n",
    "    open_file_object.writerow(word)\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "output = clf.predict_proba(X_test).astype(str)\n",
    "output = np.insert(output,0,ids,axis=1)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "for word in output:\n",
    "    open_file_object.writerow(word)\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.048828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195310</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>39</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>41</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.132810</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.030273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>52</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>59</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>62</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>65</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>70</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>74</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138670</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>79</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.113280</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>86</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1493</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1495</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1497</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1498</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1503</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.142580</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1510</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1513</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1522</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.088867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1526</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1528</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244140</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1533</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1534</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1535</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130860</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1537</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.028320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1542</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1546</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1553</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1558</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1560</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.115230</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1564</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1565</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1567</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1573</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126950</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1580</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136720</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   margin1   margin2   margin3   margin4   margin5   margin6  \\\n",
       "0       4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625   \n",
       "1       7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672   \n",
       "2       9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000   \n",
       "3      12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000   \n",
       "4      13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000   \n",
       "5      16  0.021484  0.033203  0.021484  0.009766  0.015625  0.035156   \n",
       "6      19  0.015625  0.025391  0.046875  0.009766  0.005859  0.027344   \n",
       "7      23  0.007812  0.031250  0.011719  0.050781  0.000000  0.117190   \n",
       "8      24  0.003906  0.007812  0.074219  0.017578  0.015625  0.003906   \n",
       "9      28  0.000000  0.000000  0.005859  0.021484  0.054688  0.000000   \n",
       "10     33  0.025391  0.041016  0.033203  0.017578  0.009766  0.056641   \n",
       "11     36  0.001953  0.009766  0.025391  0.003906  0.015625  0.003906   \n",
       "12     39  0.013672  0.000000  0.046875  0.019531  0.005859  0.000000   \n",
       "13     41  0.007812  0.005859  0.060547  0.013672  0.011719  0.000000   \n",
       "14     44  0.017578  0.039062  0.031250  0.005859  0.005859  0.039062   \n",
       "15     46  0.011719  0.017578  0.111330  0.009766  0.007812  0.011719   \n",
       "16     47  0.025391  0.015625  0.132810  0.003906  0.001953  0.009766   \n",
       "17     51  0.007812  0.007812  0.019531  0.039062  0.031250  0.021484   \n",
       "18     52  0.001953  0.007812  0.031250  0.005859  0.013672  0.003906   \n",
       "19     53  0.000000  0.000000  0.082031  0.111330  0.017578  0.000000   \n",
       "20     57  0.013672  0.023438  0.007812  0.054688  0.001953  0.083984   \n",
       "21     59  0.011719  0.011719  0.048828  0.005859  0.007812  0.015625   \n",
       "22     62  0.037109  0.158200  0.013672  0.009766  0.000000  0.203130   \n",
       "23     65  0.027344  0.056641  0.021484  0.011719  0.005859  0.019531   \n",
       "24     68  0.000000  0.000000  0.003906  0.021484  0.023438  0.000000   \n",
       "25     70  0.015625  0.023438  0.025391  0.011719  0.005859  0.005859   \n",
       "26     74  0.019531  0.064453  0.009766  0.039062  0.000000  0.138670   \n",
       "27     77  0.000000  0.095703  0.000000  0.017578  0.000000  0.000000   \n",
       "28     79  0.001953  0.000000  0.015625  0.113280  0.023438  0.000000   \n",
       "29     86  0.017578  0.078125  0.023438  0.003906  0.001953  0.048828   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "564  1493  0.005859  0.000000  0.021484  0.039062  0.011719  0.003906   \n",
       "565  1495  0.007812  0.015625  0.058594  0.023438  0.027344  0.013672   \n",
       "566  1497  0.005859  0.005859  0.009766  0.005859  0.074219  0.001953   \n",
       "567  1498  0.021484  0.015625  0.078125  0.019531  0.003906  0.044922   \n",
       "568  1503  0.041016  0.142580  0.021484  0.001953  0.000000  0.154300   \n",
       "569  1510  0.001953  0.000000  0.009766  0.015625  0.027344  0.000000   \n",
       "570  1513  0.011719  0.007812  0.021484  0.001953  0.041016  0.001953   \n",
       "571  1517  0.000000  0.000000  0.097656  0.052734  0.005859  0.000000   \n",
       "572  1522  0.001953  0.005859  0.025391  0.011719  0.007812  0.000000   \n",
       "573  1526  0.015625  0.017578  0.074219  0.011719  0.001953  0.015625   \n",
       "574  1528  0.074219  0.064453  0.013672  0.003906  0.000000  0.244140   \n",
       "575  1533  0.017578  0.017578  0.031250  0.009766  0.003906  0.015625   \n",
       "576  1534  0.001953  0.000000  0.017578  0.011719  0.027344  0.003906   \n",
       "577  1535  0.042969  0.144530  0.035156  0.005859  0.000000  0.130860   \n",
       "578  1537  0.003906  0.003906  0.021484  0.011719  0.046875  0.003906   \n",
       "579  1540  0.000000  0.001953  0.015625  0.013672  0.029297  0.001953   \n",
       "580  1542  0.005859  0.011719  0.033203  0.001953  0.035156  0.015625   \n",
       "581  1546  0.011719  0.013672  0.017578  0.007812  0.007812  0.005859   \n",
       "582  1553  0.009766  0.001953  0.044922  0.015625  0.021484  0.003906   \n",
       "583  1558  0.001953  0.000000  0.009766  0.001953  0.021484  0.005859   \n",
       "584  1560  0.042969  0.107420  0.025391  0.011719  0.003906  0.115230   \n",
       "585  1564  0.009766  0.025391  0.064453  0.005859  0.001953  0.095703   \n",
       "586  1565  0.007812  0.019531  0.013672  0.031250  0.011719  0.003906   \n",
       "587  1567  0.003906  0.017578  0.042969  0.033203  0.041016  0.007812   \n",
       "588  1573  0.064453  0.039062  0.009766  0.015625  0.003906  0.060547   \n",
       "589  1576  0.000000  0.000000  0.003906  0.015625  0.041016  0.000000   \n",
       "590  1577  0.000000  0.003906  0.003906  0.005859  0.017578  0.000000   \n",
       "591  1579  0.017578  0.029297  0.015625  0.013672  0.003906  0.015625   \n",
       "592  1580  0.013672  0.009766  0.060547  0.025391  0.035156  0.025391   \n",
       "593  1583  0.000000  0.117190  0.000000  0.019531  0.000000  0.136720   \n",
       "\n",
       "      margin7   margin8   margin9    ...      texture55  texture56  texture57  \\\n",
       "0    0.005859  0.000000  0.005859    ...       0.006836   0.000000   0.015625   \n",
       "1    0.007812  0.000000  0.033203    ...       0.000000   0.000000   0.006836   \n",
       "2    0.023438  0.000000  0.011719    ...       0.128910   0.000000   0.000977   \n",
       "3    0.003906  0.000000  0.003906    ...       0.012695   0.015625   0.002930   \n",
       "4    0.009766  0.000000  0.005859    ...       0.000000   0.042969   0.016602   \n",
       "5    0.039062  0.000000  0.003906    ...       0.000000   0.000000   0.000000   \n",
       "6    0.042969  0.000000  0.000000    ...       0.001953   0.000000   0.000000   \n",
       "7    0.003906  0.000000  0.011719    ...       0.047852   0.000000   0.030273   \n",
       "8    0.011719  0.000000  0.009766    ...       0.102540   0.000000   0.023438   \n",
       "9    0.015625  0.000000  0.011719    ...       0.195310   0.039062   0.003906   \n",
       "10   0.023438  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "11   0.041016  0.000000  0.005859    ...       0.049805   0.000000   0.081055   \n",
       "12   0.041016  0.000000  0.013672    ...       0.000000   0.000000   0.002930   \n",
       "13   0.037109  0.000000  0.000000    ...       0.002930   0.000000   0.001953   \n",
       "14   0.013672  0.000000  0.007812    ...       0.000000   0.000000   0.007812   \n",
       "15   0.027344  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "16   0.001953  0.000000  0.000000    ...       0.288090   0.000000   0.046875   \n",
       "17   0.039062  0.000000  0.001953    ...       0.009766   0.000000   0.000977   \n",
       "18   0.015625  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "19   0.000000  0.000000  0.013672    ...       0.005859   0.000000   0.000000   \n",
       "20   0.019531  0.000000  0.001953    ...       0.009766   0.000000   0.064453   \n",
       "21   0.046875  0.005859  0.000000    ...       0.035156   0.000000   0.051758   \n",
       "22   0.000000  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "23   0.029297  0.000000  0.000000    ...       0.000000   0.124020   0.000000   \n",
       "24   0.003906  0.000000  0.005859    ...       0.119140   0.000000   0.002930   \n",
       "25   0.011719  0.000000  0.000000    ...       0.040039   0.000000   0.029297   \n",
       "26   0.001953  0.000000  0.009766    ...       0.001953   0.000000   0.010742   \n",
       "27   0.000000  0.005859  0.000000    ...       0.031250   0.007812   0.000000   \n",
       "28   0.000000  0.000000  0.017578    ...       0.018555   0.000000   0.000000   \n",
       "29   0.029297  0.000000  0.000000    ...       0.015625   0.000977   0.023438   \n",
       "..        ...       ...       ...    ...            ...        ...        ...   \n",
       "564  0.003906  0.000000  0.005859    ...       0.000000   0.000977   0.016602   \n",
       "565  0.056641  0.003906  0.009766    ...       0.000000   0.000000   0.001953   \n",
       "566  0.011719  0.000000  0.005859    ...       0.038086   0.000000   0.006836   \n",
       "567  0.048828  0.000000  0.005859    ...       0.000000   0.000000   0.000977   \n",
       "568  0.000000  0.000000  0.005859    ...       0.208980   0.000000   0.060547   \n",
       "569  0.001953  0.000000  0.005859    ...       0.000000   0.033203   0.003906   \n",
       "570  0.050781  0.000000  0.005859    ...       0.008789   0.000977   0.011719   \n",
       "571  0.000000  0.000000  0.005859    ...       0.043945   0.000000   0.002930   \n",
       "572  0.033203  0.000000  0.000000    ...       0.000977   0.000000   0.005859   \n",
       "573  0.019531  0.000000  0.000000    ...       0.022461   0.000000   0.006836   \n",
       "574  0.005859  0.000000  0.005859    ...       0.064453   0.000000   0.013672   \n",
       "575  0.015625  0.000000  0.007812    ...       0.000000   0.000000   0.006836   \n",
       "576  0.015625  0.000000  0.011719    ...       0.000000   0.000000   0.000000   \n",
       "577  0.003906  0.000000  0.009766    ...       0.078125   0.000000   0.022461   \n",
       "578  0.033203  0.000000  0.005859    ...       0.004883   0.009766   0.000977   \n",
       "579  0.052734  0.000000  0.007812    ...       0.106450   0.000000   0.012695   \n",
       "580  0.027344  0.000000  0.009766    ...       0.001953   0.000000   0.046875   \n",
       "581  0.019531  0.000000  0.005859    ...       0.106450   0.000000   0.033203   \n",
       "582  0.052734  0.000000  0.000000    ...       0.032227   0.000000   0.008789   \n",
       "583  0.037109  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "584  0.025391  0.000000  0.005859    ...       0.346680   0.000000   0.047852   \n",
       "585  0.064453  0.000000  0.005859    ...       0.000000   0.000000   0.005859   \n",
       "586  0.023438  0.000000  0.021484    ...       0.000000   0.000000   0.006836   \n",
       "587  0.013672  0.000000  0.007812    ...       0.000000   0.000000   0.000000   \n",
       "588  0.013672  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "589  0.017578  0.000000  0.005859    ...       0.098633   0.000000   0.004883   \n",
       "590  0.017578  0.005859  0.000000    ...       0.012695   0.004883   0.004883   \n",
       "591  0.025391  0.000000  0.000000    ...       0.073242   0.000000   0.028320   \n",
       "592  0.039062  0.000000  0.003906    ...       0.003906   0.000000   0.000977   \n",
       "593  0.001953  0.005859  0.000000    ...       0.107420   0.012695   0.016602   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.000977   0.015625   0.000000   0.000000   0.000000   0.003906   \n",
       "1     0.001953   0.013672   0.000000   0.000000   0.000977   0.037109   \n",
       "2     0.000000   0.000000   0.000000   0.000000   0.015625   0.000000   \n",
       "3     0.036133   0.013672   0.000000   0.000000   0.089844   0.000000   \n",
       "4     0.010742   0.041016   0.000000   0.000000   0.007812   0.009766   \n",
       "5     0.000977   0.049805   0.000000   0.000000   0.027344   0.000000   \n",
       "6     0.004883   0.030273   0.000000   0.000000   0.000977   0.000000   \n",
       "7     0.000000   0.011719   0.000000   0.000000   0.003906   0.002930   \n",
       "8     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9     0.007812   0.013672   0.001953   0.000000   0.029297   0.000000   \n",
       "10    0.006836   0.076172   0.000000   0.000000   0.000000   0.035156   \n",
       "11    0.000977   0.009766   0.000000   0.000000   0.000000   0.010742   \n",
       "12    0.000000   0.026367   0.000000   0.000000   0.000000   0.000000   \n",
       "13    0.000977   0.020508   0.000000   0.000000   0.002930   0.002930   \n",
       "14    0.000977   0.016602   0.000000   0.000000   0.003906   0.000000   \n",
       "15    0.039062   0.045898   0.000000   0.000000   0.050781   0.005859   \n",
       "16    0.000000   0.007812   0.000000   0.000000   0.000000   0.000000   \n",
       "17    0.000000   0.005859   0.000000   0.000000   0.000000   0.022461   \n",
       "18    0.004883   0.018555   0.019531   0.000000   0.032227   0.013672   \n",
       "19    0.000000   0.010742   0.000000   0.000000   0.000000   0.031250   \n",
       "20    0.000000   0.005859   0.000000   0.000000   0.000000   0.011719   \n",
       "21    0.000000   0.007812   0.000000   0.000000   0.000000   0.014648   \n",
       "22    0.015625   0.000000   0.531250   0.000000   0.035156   0.000000   \n",
       "23    0.033203   0.005859   0.000977   0.000000   0.062500   0.043945   \n",
       "24    0.003906   0.000000   0.000000   0.000000   0.053711   0.000000   \n",
       "25    0.000000   0.006836   0.000000   0.004883   0.000977   0.009766   \n",
       "26    0.033203   0.019531   0.001953   0.000000   0.029297   0.000000   \n",
       "27    0.002930   0.005859   0.000000   0.000000   0.070312   0.000000   \n",
       "28    0.000000   0.012695   0.000000   0.000000   0.000000   0.000000   \n",
       "29    0.000000   0.009766   0.000000   0.000977   0.000000   0.008789   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "564   0.066406   0.027344   0.000000   0.000977   0.064453   0.005859   \n",
       "565   0.000977   0.000000   0.000000   0.000000   0.001953   0.000000   \n",
       "566   0.001953   0.012695   0.000000   0.000000   0.033203   0.005859   \n",
       "567   0.041016   0.002930   0.035156   0.000000   0.085938   0.000000   \n",
       "568   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "569   0.013672   0.047852   0.010742   0.000000   0.038086   0.015625   \n",
       "570   0.001953   0.023438   0.000000   0.000000   0.017578   0.000000   \n",
       "571   0.003906   0.024414   0.000977   0.000000   0.027344   0.007812   \n",
       "572   0.000000   0.017578   0.000000   0.000000   0.000000   0.024414   \n",
       "573   0.007812   0.034180   0.000000   0.000000   0.004883   0.000977   \n",
       "574   0.000000   0.002930   0.000000   0.000000   0.000000   0.007812   \n",
       "575   0.000977   0.014648   0.000000   0.000000   0.000000   0.037109   \n",
       "576   0.000977   0.000000   0.204100   0.000000   0.049805   0.000000   \n",
       "577   0.000000   0.000977   0.000000   0.000000   0.000000   0.000000   \n",
       "578   0.004883   0.043945   0.000000   0.000000   0.017578   0.005859   \n",
       "579   0.000000   0.012695   0.000000   0.000000   0.000000   0.037109   \n",
       "580   0.000000   0.036133   0.000000   0.000000   0.000000   0.000000   \n",
       "581   0.000977   0.010742   0.000000   0.000000   0.000000   0.035156   \n",
       "582   0.002930   0.028320   0.000000   0.000000   0.004883   0.008789   \n",
       "583   0.152340   0.024414   0.001953   0.000000   0.046875   0.004883   \n",
       "584   0.000000   0.007812   0.000000   0.000000   0.000000   0.001953   \n",
       "585   0.011719   0.008789   0.000000   0.000000   0.008789   0.000000   \n",
       "586   0.000000   0.021484   0.000000   0.000000   0.000977   0.028320   \n",
       "587   0.004883   0.013672   0.000000   0.000000   0.000977   0.086914   \n",
       "588   0.126950   0.000977   0.154300   0.000000   0.123050   0.000000   \n",
       "589   0.000000   0.003906   0.000000   0.000000   0.018555   0.000000   \n",
       "590   0.002930   0.009766   0.000000   0.000000   0.090820   0.000000   \n",
       "591   0.000000   0.001953   0.000000   0.000000   0.000000   0.042969   \n",
       "592   0.000000   0.011719   0.000000   0.000000   0.000000   0.011719   \n",
       "593   0.000977   0.004883   0.000000   0.000000   0.015625   0.000000   \n",
       "\n",
       "     texture64  \n",
       "0     0.053711  \n",
       "1     0.044922  \n",
       "2     0.000000  \n",
       "3     0.008789  \n",
       "4     0.007812  \n",
       "5     0.021484  \n",
       "6     0.024414  \n",
       "7     0.048828  \n",
       "8     0.008789  \n",
       "9     0.007812  \n",
       "10    0.000000  \n",
       "11    0.012695  \n",
       "12    0.018555  \n",
       "13    0.033203  \n",
       "14    0.011719  \n",
       "15    0.009766  \n",
       "16    0.014648  \n",
       "17    0.030273  \n",
       "18    0.005859  \n",
       "19    0.001953  \n",
       "20    0.047852  \n",
       "21    0.044922  \n",
       "22    0.000000  \n",
       "23    0.000000  \n",
       "24    0.031250  \n",
       "25    0.007812  \n",
       "26    0.026367  \n",
       "27    0.000000  \n",
       "28    0.000000  \n",
       "29    0.002930  \n",
       "..         ...  \n",
       "564   0.009766  \n",
       "565   0.006836  \n",
       "566   0.002930  \n",
       "567   0.000000  \n",
       "568   0.000977  \n",
       "569   0.000977  \n",
       "570   0.031250  \n",
       "571   0.002930  \n",
       "572   0.088867  \n",
       "573   0.031250  \n",
       "574   0.000977  \n",
       "575   0.047852  \n",
       "576   0.000000  \n",
       "577   0.017578  \n",
       "578   0.028320  \n",
       "579   0.001953  \n",
       "580   0.011719  \n",
       "581   0.011719  \n",
       "582   0.046875  \n",
       "583   0.000000  \n",
       "584   0.020508  \n",
       "585   0.016602  \n",
       "586   0.060547  \n",
       "587   0.000000  \n",
       "588   0.000000  \n",
       "589   0.000977  \n",
       "590   0.016602  \n",
       "591   0.006836  \n",
       "592   0.018555  \n",
       "593   0.017578  \n",
       "\n",
       "[594 rows x 193 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>594.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>780.673401</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.028425</td>\n",
       "      <td>0.031858</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.037497</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035291</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>0.015881</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>0.020970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>465.646977</td>\n",
       "      <td>0.019585</td>\n",
       "      <td>0.038351</td>\n",
       "      <td>0.025719</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>0.017122</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064482</td>\n",
       "      <td>0.026934</td>\n",
       "      <td>0.022318</td>\n",
       "      <td>0.024771</td>\n",
       "      <td>0.014898</td>\n",
       "      <td>0.052530</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.034704</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.023407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>368.500000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>774.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.013184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1184.500000</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1583.000000</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.189450</td>\n",
       "      <td>0.167970</td>\n",
       "      <td>0.164060</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.271480</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353520</td>\n",
       "      <td>0.441410</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.177730</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.606450</td>\n",
       "      <td>0.123050</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.149410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     margin1     margin2     margin3     margin4  \\\n",
       "count   594.000000  594.000000  594.000000  594.000000  594.000000   \n",
       "mean    780.673401    0.017562    0.028425    0.031858    0.022556   \n",
       "std     465.646977    0.019585    0.038351    0.025719    0.028797   \n",
       "min       4.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     368.500000    0.001953    0.001953    0.013672    0.005859   \n",
       "50%     774.000000    0.009766    0.010743    0.023438    0.013672   \n",
       "75%    1184.500000    0.028809    0.041016    0.042969    0.027344   \n",
       "max    1583.000000    0.085938    0.189450    0.167970    0.164060   \n",
       "\n",
       "          margin5     margin6     margin7     margin8     margin9     ...      \\\n",
       "count  594.000000  594.000000  594.000000  594.000000  594.000000     ...       \n",
       "mean     0.014527    0.037497    0.019222    0.001085    0.007092     ...       \n",
       "std      0.018029    0.051372    0.017122    0.002697    0.009515     ...       \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000     ...       \n",
       "25%      0.001953    0.000000    0.005859    0.000000    0.001953     ...       \n",
       "50%      0.007812    0.013672    0.015625    0.000000    0.005859     ...       \n",
       "75%      0.019531    0.056641    0.029297    0.000000    0.007812     ...       \n",
       "max      0.093750    0.271480    0.087891    0.021484    0.083984     ...       \n",
       "\n",
       "        texture55   texture56   texture57   texture58   texture59   texture60  \\\n",
       "count  594.000000  594.000000  594.000000  594.000000  594.000000  594.000000   \n",
       "mean     0.035291    0.005923    0.015033    0.011762    0.015881    0.011217   \n",
       "std      0.064482    0.026934    0.022318    0.024771    0.014898    0.052530   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000977    0.000000    0.004883    0.000000   \n",
       "50%      0.003906    0.000000    0.005859    0.001953    0.012695    0.000000   \n",
       "75%      0.038086    0.000000    0.019531    0.010498    0.022461    0.000000   \n",
       "max      0.353520    0.441410    0.153320    0.177730    0.083984    0.606450   \n",
       "\n",
       "        texture61   texture62   texture63   texture64  \n",
       "count  594.000000  594.000000  594.000000  594.000000  \n",
       "mean     0.002617    0.019975    0.009389    0.020970  \n",
       "std      0.011204    0.034704    0.013457    0.023407  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    0.000977  \n",
       "50%      0.000000    0.003418    0.002930    0.013184  \n",
       "75%      0.000000    0.022461    0.014648    0.032227  \n",
       "max      0.123050    0.247070    0.086914    0.149410  \n",
       "\n",
       "[8 rows x 193 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053081</td>\n",
       "      <td>-0.078534</td>\n",
       "      <td>-0.028574</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>-0.074927</td>\n",
       "      <td>-0.007302</td>\n",
       "      <td>0.056806</td>\n",
       "      <td>-0.023060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024129</td>\n",
       "      <td>0.030386</td>\n",
       "      <td>-0.010921</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>-0.071493</td>\n",
       "      <td>-0.011355</td>\n",
       "      <td>0.053860</td>\n",
       "      <td>0.084747</td>\n",
       "      <td>0.030623</td>\n",
       "      <td>-0.015542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin1</th>\n",
       "      <td>-0.053081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773549</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>-0.314669</td>\n",
       "      <td>-0.489724</td>\n",
       "      <td>0.758771</td>\n",
       "      <td>0.109006</td>\n",
       "      <td>-0.124012</td>\n",
       "      <td>-0.206356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095548</td>\n",
       "      <td>-0.085175</td>\n",
       "      <td>0.119563</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>-0.213600</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>-0.054816</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>-0.043275</td>\n",
       "      <td>0.026005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin2</th>\n",
       "      <td>-0.078534</td>\n",
       "      <td>0.773549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.228772</td>\n",
       "      <td>-0.312056</td>\n",
       "      <td>-0.476718</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>-0.055350</td>\n",
       "      <td>-0.089673</td>\n",
       "      <td>-0.164098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160932</td>\n",
       "      <td>-0.073517</td>\n",
       "      <td>0.160765</td>\n",
       "      <td>-0.041497</td>\n",
       "      <td>-0.205489</td>\n",
       "      <td>0.151150</td>\n",
       "      <td>-0.066164</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-0.092966</td>\n",
       "      <td>-0.008868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin3</th>\n",
       "      <td>-0.028574</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>-0.228772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164221</td>\n",
       "      <td>-0.205369</td>\n",
       "      <td>-0.175065</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>-0.022426</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071901</td>\n",
       "      <td>0.060990</td>\n",
       "      <td>-0.105442</td>\n",
       "      <td>-0.015316</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>-0.036324</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>-0.061109</td>\n",
       "      <td>-0.147547</td>\n",
       "      <td>-0.019413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin4</th>\n",
       "      <td>0.043150</td>\n",
       "      <td>-0.314669</td>\n",
       "      <td>-0.312056</td>\n",
       "      <td>0.164221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>-0.266132</td>\n",
       "      <td>-0.307664</td>\n",
       "      <td>-0.105205</td>\n",
       "      <td>0.235059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101321</td>\n",
       "      <td>-0.021906</td>\n",
       "      <td>-0.027073</td>\n",
       "      <td>-0.078067</td>\n",
       "      <td>0.066583</td>\n",
       "      <td>-0.046185</td>\n",
       "      <td>0.337250</td>\n",
       "      <td>-0.078419</td>\n",
       "      <td>-0.010801</td>\n",
       "      <td>-0.004705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin5</th>\n",
       "      <td>0.007684</td>\n",
       "      <td>-0.489724</td>\n",
       "      <td>-0.476718</td>\n",
       "      <td>-0.205369</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.455129</td>\n",
       "      <td>-0.086207</td>\n",
       "      <td>0.049554</td>\n",
       "      <td>0.185162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159061</td>\n",
       "      <td>0.053379</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.105663</td>\n",
       "      <td>0.257331</td>\n",
       "      <td>-0.025405</td>\n",
       "      <td>-0.103416</td>\n",
       "      <td>0.126172</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>-0.173556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin6</th>\n",
       "      <td>-0.074927</td>\n",
       "      <td>0.758771</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>-0.175065</td>\n",
       "      <td>-0.266132</td>\n",
       "      <td>-0.455129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.071411</td>\n",
       "      <td>-0.121303</td>\n",
       "      <td>-0.170549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146204</td>\n",
       "      <td>-0.093891</td>\n",
       "      <td>0.168194</td>\n",
       "      <td>-0.032881</td>\n",
       "      <td>-0.216723</td>\n",
       "      <td>0.110988</td>\n",
       "      <td>-0.081673</td>\n",
       "      <td>-0.037976</td>\n",
       "      <td>-0.089921</td>\n",
       "      <td>0.046455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin7</th>\n",
       "      <td>-0.007302</td>\n",
       "      <td>0.109006</td>\n",
       "      <td>-0.055350</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>-0.307664</td>\n",
       "      <td>-0.086207</td>\n",
       "      <td>-0.071411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037644</td>\n",
       "      <td>-0.320724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085227</td>\n",
       "      <td>-0.080115</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.066956</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>-0.030828</td>\n",
       "      <td>-0.186244</td>\n",
       "      <td>-0.022211</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.046681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin8</th>\n",
       "      <td>0.056806</td>\n",
       "      <td>-0.124012</td>\n",
       "      <td>-0.089673</td>\n",
       "      <td>-0.022426</td>\n",
       "      <td>-0.105205</td>\n",
       "      <td>0.049554</td>\n",
       "      <td>-0.121303</td>\n",
       "      <td>0.037644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.080295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>-0.047895</td>\n",
       "      <td>-0.030139</td>\n",
       "      <td>-0.040318</td>\n",
       "      <td>-0.041441</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>-0.073920</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.013802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin9</th>\n",
       "      <td>-0.023060</td>\n",
       "      <td>-0.206356</td>\n",
       "      <td>-0.164098</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.235059</td>\n",
       "      <td>0.185162</td>\n",
       "      <td>-0.170549</td>\n",
       "      <td>-0.320724</td>\n",
       "      <td>-0.080295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073856</td>\n",
       "      <td>0.104036</td>\n",
       "      <td>0.109902</td>\n",
       "      <td>-0.085610</td>\n",
       "      <td>0.025699</td>\n",
       "      <td>-0.038042</td>\n",
       "      <td>0.095002</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>-0.149321</td>\n",
       "      <td>-0.095424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin10</th>\n",
       "      <td>-0.006432</td>\n",
       "      <td>0.428822</td>\n",
       "      <td>0.176159</td>\n",
       "      <td>-0.029816</td>\n",
       "      <td>-0.217483</td>\n",
       "      <td>-0.320386</td>\n",
       "      <td>0.226868</td>\n",
       "      <td>0.612331</td>\n",
       "      <td>-0.049991</td>\n",
       "      <td>-0.302118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076746</td>\n",
       "      <td>-0.072610</td>\n",
       "      <td>0.079052</td>\n",
       "      <td>0.087768</td>\n",
       "      <td>-0.126513</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>-0.149181</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.105641</td>\n",
       "      <td>0.093366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin11</th>\n",
       "      <td>-0.047977</td>\n",
       "      <td>0.781549</td>\n",
       "      <td>0.849201</td>\n",
       "      <td>-0.256124</td>\n",
       "      <td>-0.220119</td>\n",
       "      <td>-0.548524</td>\n",
       "      <td>0.743649</td>\n",
       "      <td>-0.055520</td>\n",
       "      <td>-0.117326</td>\n",
       "      <td>-0.191377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141750</td>\n",
       "      <td>-0.034858</td>\n",
       "      <td>0.105781</td>\n",
       "      <td>-0.028392</td>\n",
       "      <td>-0.237362</td>\n",
       "      <td>0.133325</td>\n",
       "      <td>-0.011383</td>\n",
       "      <td>-0.024234</td>\n",
       "      <td>-0.056761</td>\n",
       "      <td>0.019686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin12</th>\n",
       "      <td>0.001819</td>\n",
       "      <td>-0.524443</td>\n",
       "      <td>-0.483467</td>\n",
       "      <td>-0.009882</td>\n",
       "      <td>-0.180179</td>\n",
       "      <td>0.413255</td>\n",
       "      <td>-0.451356</td>\n",
       "      <td>-0.139712</td>\n",
       "      <td>0.124678</td>\n",
       "      <td>-0.034615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145951</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>-0.163237</td>\n",
       "      <td>0.103672</td>\n",
       "      <td>0.237560</td>\n",
       "      <td>-0.042541</td>\n",
       "      <td>-0.060805</td>\n",
       "      <td>0.085389</td>\n",
       "      <td>0.067379</td>\n",
       "      <td>0.023136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin13</th>\n",
       "      <td>-0.049372</td>\n",
       "      <td>0.462842</td>\n",
       "      <td>0.660399</td>\n",
       "      <td>-0.064409</td>\n",
       "      <td>-0.252150</td>\n",
       "      <td>-0.489214</td>\n",
       "      <td>0.503663</td>\n",
       "      <td>-0.106658</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>-0.085411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265179</td>\n",
       "      <td>-0.014806</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>-0.060017</td>\n",
       "      <td>-0.219646</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>-0.122140</td>\n",
       "      <td>-0.034450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin14</th>\n",
       "      <td>0.059849</td>\n",
       "      <td>-0.379531</td>\n",
       "      <td>-0.320515</td>\n",
       "      <td>0.158219</td>\n",
       "      <td>0.379220</td>\n",
       "      <td>0.106122</td>\n",
       "      <td>-0.306552</td>\n",
       "      <td>-0.351568</td>\n",
       "      <td>0.077201</td>\n",
       "      <td>0.338621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024271</td>\n",
       "      <td>0.107388</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>-0.079880</td>\n",
       "      <td>0.034195</td>\n",
       "      <td>-0.059274</td>\n",
       "      <td>0.272856</td>\n",
       "      <td>-0.003158</td>\n",
       "      <td>-0.078160</td>\n",
       "      <td>-0.019282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin15</th>\n",
       "      <td>-0.027199</td>\n",
       "      <td>-0.552260</td>\n",
       "      <td>-0.526286</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>-0.234273</td>\n",
       "      <td>0.540390</td>\n",
       "      <td>-0.499602</td>\n",
       "      <td>-0.020095</td>\n",
       "      <td>0.094224</td>\n",
       "      <td>-0.061898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122381</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.172893</td>\n",
       "      <td>0.107174</td>\n",
       "      <td>0.276411</td>\n",
       "      <td>-0.043083</td>\n",
       "      <td>-0.116002</td>\n",
       "      <td>0.101005</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>-0.000694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin16</th>\n",
       "      <td>0.079662</td>\n",
       "      <td>-0.064834</td>\n",
       "      <td>-0.074365</td>\n",
       "      <td>-0.040667</td>\n",
       "      <td>-0.017333</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>-0.071210</td>\n",
       "      <td>-0.024897</td>\n",
       "      <td>0.335955</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076427</td>\n",
       "      <td>-0.017699</td>\n",
       "      <td>-0.034254</td>\n",
       "      <td>-0.035941</td>\n",
       "      <td>-0.037795</td>\n",
       "      <td>-0.009281</td>\n",
       "      <td>-0.020357</td>\n",
       "      <td>-0.009151</td>\n",
       "      <td>-0.007771</td>\n",
       "      <td>-0.004063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin17</th>\n",
       "      <td>-0.035232</td>\n",
       "      <td>0.291559</td>\n",
       "      <td>0.110797</td>\n",
       "      <td>-0.107460</td>\n",
       "      <td>-0.069347</td>\n",
       "      <td>-0.228345</td>\n",
       "      <td>0.077631</td>\n",
       "      <td>0.461286</td>\n",
       "      <td>-0.002844</td>\n",
       "      <td>-0.253823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044637</td>\n",
       "      <td>-0.086129</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>0.026570</td>\n",
       "      <td>-0.029041</td>\n",
       "      <td>-0.038409</td>\n",
       "      <td>-0.124372</td>\n",
       "      <td>-0.035676</td>\n",
       "      <td>0.126152</td>\n",
       "      <td>0.042327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin18</th>\n",
       "      <td>0.044095</td>\n",
       "      <td>0.222736</td>\n",
       "      <td>0.320858</td>\n",
       "      <td>-0.088816</td>\n",
       "      <td>0.051832</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>0.196095</td>\n",
       "      <td>-0.135123</td>\n",
       "      <td>-0.012535</td>\n",
       "      <td>-0.075608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074677</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>-0.045910</td>\n",
       "      <td>-0.022644</td>\n",
       "      <td>-0.180710</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>-0.050741</td>\n",
       "      <td>-0.033799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin19</th>\n",
       "      <td>0.040505</td>\n",
       "      <td>-0.273754</td>\n",
       "      <td>-0.252120</td>\n",
       "      <td>-0.084204</td>\n",
       "      <td>0.400266</td>\n",
       "      <td>0.227421</td>\n",
       "      <td>-0.309225</td>\n",
       "      <td>-0.076838</td>\n",
       "      <td>-0.034999</td>\n",
       "      <td>0.164839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100991</td>\n",
       "      <td>-0.025468</td>\n",
       "      <td>0.263540</td>\n",
       "      <td>-0.164528</td>\n",
       "      <td>0.058517</td>\n",
       "      <td>-0.111007</td>\n",
       "      <td>0.230107</td>\n",
       "      <td>-0.130427</td>\n",
       "      <td>0.074373</td>\n",
       "      <td>0.024504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin20</th>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.389580</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>-0.037440</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>-0.341488</td>\n",
       "      <td>0.218495</td>\n",
       "      <td>0.311759</td>\n",
       "      <td>-0.091445</td>\n",
       "      <td>-0.246071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032006</td>\n",
       "      <td>-0.057160</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>-0.201035</td>\n",
       "      <td>0.028969</td>\n",
       "      <td>-0.100641</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.066308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin21</th>\n",
       "      <td>0.043163</td>\n",
       "      <td>-0.465394</td>\n",
       "      <td>-0.460889</td>\n",
       "      <td>0.065255</td>\n",
       "      <td>-0.129486</td>\n",
       "      <td>0.148439</td>\n",
       "      <td>-0.438660</td>\n",
       "      <td>0.020308</td>\n",
       "      <td>0.168175</td>\n",
       "      <td>0.042928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029752</td>\n",
       "      <td>0.056532</td>\n",
       "      <td>-0.154593</td>\n",
       "      <td>-0.025700</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>-0.066246</td>\n",
       "      <td>-0.017666</td>\n",
       "      <td>-0.012576</td>\n",
       "      <td>0.114639</td>\n",
       "      <td>0.120255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin22</th>\n",
       "      <td>0.065193</td>\n",
       "      <td>-0.396545</td>\n",
       "      <td>-0.359684</td>\n",
       "      <td>-0.288024</td>\n",
       "      <td>-0.196579</td>\n",
       "      <td>0.351477</td>\n",
       "      <td>-0.347130</td>\n",
       "      <td>-0.127864</td>\n",
       "      <td>0.096561</td>\n",
       "      <td>0.131491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086113</td>\n",
       "      <td>0.064356</td>\n",
       "      <td>-0.113550</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.078140</td>\n",
       "      <td>-0.027570</td>\n",
       "      <td>-0.094826</td>\n",
       "      <td>0.122838</td>\n",
       "      <td>0.074553</td>\n",
       "      <td>0.021181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin23</th>\n",
       "      <td>0.045306</td>\n",
       "      <td>-0.153779</td>\n",
       "      <td>-0.141892</td>\n",
       "      <td>-0.150664</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>0.197436</td>\n",
       "      <td>-0.136857</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.044345</td>\n",
       "      <td>0.026551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029044</td>\n",
       "      <td>-0.044237</td>\n",
       "      <td>0.254292</td>\n",
       "      <td>-0.091261</td>\n",
       "      <td>-0.063507</td>\n",
       "      <td>-0.044597</td>\n",
       "      <td>-0.051263</td>\n",
       "      <td>-0.074246</td>\n",
       "      <td>0.101806</td>\n",
       "      <td>0.008550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin24</th>\n",
       "      <td>-0.001282</td>\n",
       "      <td>-0.276779</td>\n",
       "      <td>-0.274558</td>\n",
       "      <td>-0.264893</td>\n",
       "      <td>-0.107239</td>\n",
       "      <td>0.136985</td>\n",
       "      <td>-0.284026</td>\n",
       "      <td>0.083492</td>\n",
       "      <td>-0.022074</td>\n",
       "      <td>0.078159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071690</td>\n",
       "      <td>-0.043192</td>\n",
       "      <td>0.082387</td>\n",
       "      <td>-0.048173</td>\n",
       "      <td>0.035218</td>\n",
       "      <td>-0.067283</td>\n",
       "      <td>-0.125326</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>0.113317</td>\n",
       "      <td>0.102556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin25</th>\n",
       "      <td>0.040460</td>\n",
       "      <td>-0.479373</td>\n",
       "      <td>-0.417931</td>\n",
       "      <td>-0.069305</td>\n",
       "      <td>0.062092</td>\n",
       "      <td>0.413877</td>\n",
       "      <td>-0.395700</td>\n",
       "      <td>-0.304669</td>\n",
       "      <td>0.069273</td>\n",
       "      <td>0.232797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086115</td>\n",
       "      <td>0.112696</td>\n",
       "      <td>-0.077155</td>\n",
       "      <td>0.067716</td>\n",
       "      <td>0.097354</td>\n",
       "      <td>-0.034106</td>\n",
       "      <td>0.076806</td>\n",
       "      <td>0.151123</td>\n",
       "      <td>-0.022241</td>\n",
       "      <td>-0.079320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin26</th>\n",
       "      <td>0.005537</td>\n",
       "      <td>-0.181478</td>\n",
       "      <td>-0.354864</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.065622</td>\n",
       "      <td>0.204234</td>\n",
       "      <td>-0.324175</td>\n",
       "      <td>0.483942</td>\n",
       "      <td>-0.022088</td>\n",
       "      <td>-0.212183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159286</td>\n",
       "      <td>-0.048409</td>\n",
       "      <td>-0.046828</td>\n",
       "      <td>0.019915</td>\n",
       "      <td>0.041627</td>\n",
       "      <td>-0.045724</td>\n",
       "      <td>-0.154131</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>0.153522</td>\n",
       "      <td>0.019188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin27</th>\n",
       "      <td>0.032548</td>\n",
       "      <td>-0.353273</td>\n",
       "      <td>-0.312422</td>\n",
       "      <td>-0.169631</td>\n",
       "      <td>-0.098996</td>\n",
       "      <td>0.313578</td>\n",
       "      <td>-0.292051</td>\n",
       "      <td>-0.224719</td>\n",
       "      <td>0.163186</td>\n",
       "      <td>0.308742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019722</td>\n",
       "      <td>0.071222</td>\n",
       "      <td>-0.070113</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>0.029587</td>\n",
       "      <td>-0.028382</td>\n",
       "      <td>-0.035041</td>\n",
       "      <td>0.124498</td>\n",
       "      <td>-0.039221</td>\n",
       "      <td>-0.051501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin28</th>\n",
       "      <td>0.006222</td>\n",
       "      <td>-0.559708</td>\n",
       "      <td>-0.564831</td>\n",
       "      <td>0.168950</td>\n",
       "      <td>0.449570</td>\n",
       "      <td>0.672994</td>\n",
       "      <td>-0.539296</td>\n",
       "      <td>-0.112346</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.228237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200178</td>\n",
       "      <td>0.039610</td>\n",
       "      <td>-0.112500</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>0.287045</td>\n",
       "      <td>-0.030730</td>\n",
       "      <td>0.097737</td>\n",
       "      <td>0.043851</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>-0.160188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin29</th>\n",
       "      <td>0.042058</td>\n",
       "      <td>0.102492</td>\n",
       "      <td>0.091433</td>\n",
       "      <td>0.248149</td>\n",
       "      <td>0.434055</td>\n",
       "      <td>-0.478354</td>\n",
       "      <td>0.040905</td>\n",
       "      <td>-0.225355</td>\n",
       "      <td>-0.112383</td>\n",
       "      <td>0.183891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095548</td>\n",
       "      <td>0.036195</td>\n",
       "      <td>-0.054262</td>\n",
       "      <td>-0.075455</td>\n",
       "      <td>-0.113618</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>0.347806</td>\n",
       "      <td>-0.089469</td>\n",
       "      <td>-0.033504</td>\n",
       "      <td>0.043764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture35</th>\n",
       "      <td>0.011477</td>\n",
       "      <td>-0.004914</td>\n",
       "      <td>-0.015764</td>\n",
       "      <td>0.058091</td>\n",
       "      <td>0.311953</td>\n",
       "      <td>-0.158820</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>-0.198401</td>\n",
       "      <td>-0.005798</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052907</td>\n",
       "      <td>-0.102531</td>\n",
       "      <td>0.141274</td>\n",
       "      <td>-0.330283</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.178829</td>\n",
       "      <td>0.339455</td>\n",
       "      <td>-0.332167</td>\n",
       "      <td>0.202983</td>\n",
       "      <td>0.199283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture36</th>\n",
       "      <td>0.077701</td>\n",
       "      <td>-0.119765</td>\n",
       "      <td>-0.100324</td>\n",
       "      <td>0.170634</td>\n",
       "      <td>0.377576</td>\n",
       "      <td>-0.095531</td>\n",
       "      <td>-0.122359</td>\n",
       "      <td>-0.217246</td>\n",
       "      <td>-0.051644</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018725</td>\n",
       "      <td>0.136234</td>\n",
       "      <td>-0.043316</td>\n",
       "      <td>-0.083239</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>-0.055935</td>\n",
       "      <td>0.720787</td>\n",
       "      <td>-0.039888</td>\n",
       "      <td>-0.086766</td>\n",
       "      <td>0.014676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture37</th>\n",
       "      <td>0.005927</td>\n",
       "      <td>-0.037853</td>\n",
       "      <td>-0.024614</td>\n",
       "      <td>-0.046280</td>\n",
       "      <td>-0.075150</td>\n",
       "      <td>0.152993</td>\n",
       "      <td>-0.002438</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>-0.041764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172002</td>\n",
       "      <td>-0.044476</td>\n",
       "      <td>-0.216758</td>\n",
       "      <td>0.153860</td>\n",
       "      <td>-0.065948</td>\n",
       "      <td>0.371657</td>\n",
       "      <td>-0.068894</td>\n",
       "      <td>0.174871</td>\n",
       "      <td>-0.034625</td>\n",
       "      <td>-0.254716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture38</th>\n",
       "      <td>-0.015149</td>\n",
       "      <td>0.087449</td>\n",
       "      <td>0.146434</td>\n",
       "      <td>-0.068787</td>\n",
       "      <td>-0.064274</td>\n",
       "      <td>-0.106026</td>\n",
       "      <td>0.143641</td>\n",
       "      <td>-0.038714</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.052136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179214</td>\n",
       "      <td>-0.109673</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>-0.356145</td>\n",
       "      <td>-0.226714</td>\n",
       "      <td>-0.183605</td>\n",
       "      <td>-0.017244</td>\n",
       "      <td>-0.322130</td>\n",
       "      <td>-0.087577</td>\n",
       "      <td>0.379235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture39</th>\n",
       "      <td>-0.003046</td>\n",
       "      <td>-0.035971</td>\n",
       "      <td>-0.048092</td>\n",
       "      <td>0.054462</td>\n",
       "      <td>-0.113431</td>\n",
       "      <td>-0.136078</td>\n",
       "      <td>-0.009596</td>\n",
       "      <td>0.144267</td>\n",
       "      <td>0.064654</td>\n",
       "      <td>-0.125638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228493</td>\n",
       "      <td>-0.120553</td>\n",
       "      <td>-0.072297</td>\n",
       "      <td>-0.219698</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>-0.161698</td>\n",
       "      <td>-0.082567</td>\n",
       "      <td>-0.190047</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>0.588546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture40</th>\n",
       "      <td>-0.030524</td>\n",
       "      <td>-0.031326</td>\n",
       "      <td>-0.042448</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>-0.068769</td>\n",
       "      <td>-0.060359</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.144842</td>\n",
       "      <td>0.047971</td>\n",
       "      <td>-0.119682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233073</td>\n",
       "      <td>-0.109537</td>\n",
       "      <td>-0.088102</td>\n",
       "      <td>-0.080099</td>\n",
       "      <td>0.175983</td>\n",
       "      <td>-0.126555</td>\n",
       "      <td>-0.106460</td>\n",
       "      <td>-0.110457</td>\n",
       "      <td>0.054784</td>\n",
       "      <td>0.156412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture41</th>\n",
       "      <td>-0.058889</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>-0.035450</td>\n",
       "      <td>0.035080</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>-0.004248</td>\n",
       "      <td>0.169376</td>\n",
       "      <td>-0.013744</td>\n",
       "      <td>-0.136369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132521</td>\n",
       "      <td>-0.079031</td>\n",
       "      <td>-0.144386</td>\n",
       "      <td>-0.076520</td>\n",
       "      <td>-0.061453</td>\n",
       "      <td>-0.070815</td>\n",
       "      <td>-0.084851</td>\n",
       "      <td>-0.101465</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.050637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture42</th>\n",
       "      <td>0.062093</td>\n",
       "      <td>0.098142</td>\n",
       "      <td>0.078702</td>\n",
       "      <td>-0.065273</td>\n",
       "      <td>0.152589</td>\n",
       "      <td>-0.149533</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>-0.141200</td>\n",
       "      <td>-0.068442</td>\n",
       "      <td>-0.067649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134870</td>\n",
       "      <td>0.044533</td>\n",
       "      <td>-0.151141</td>\n",
       "      <td>0.032632</td>\n",
       "      <td>-0.024830</td>\n",
       "      <td>-0.038028</td>\n",
       "      <td>0.564136</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.270722</td>\n",
       "      <td>-0.033518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture43</th>\n",
       "      <td>-0.027366</td>\n",
       "      <td>-0.054724</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>-0.001174</td>\n",
       "      <td>-0.142654</td>\n",
       "      <td>-0.027338</td>\n",
       "      <td>-0.042935</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.049215</td>\n",
       "      <td>-0.121188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185916</td>\n",
       "      <td>-0.107071</td>\n",
       "      <td>-0.025032</td>\n",
       "      <td>-0.216058</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>-0.151033</td>\n",
       "      <td>-0.092296</td>\n",
       "      <td>-0.165789</td>\n",
       "      <td>0.048990</td>\n",
       "      <td>0.508884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture44</th>\n",
       "      <td>0.024721</td>\n",
       "      <td>0.097241</td>\n",
       "      <td>0.042231</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>-0.088522</td>\n",
       "      <td>-0.154120</td>\n",
       "      <td>0.040304</td>\n",
       "      <td>0.133701</td>\n",
       "      <td>0.083342</td>\n",
       "      <td>-0.101064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256892</td>\n",
       "      <td>-0.088973</td>\n",
       "      <td>-0.067311</td>\n",
       "      <td>-0.230779</td>\n",
       "      <td>-0.110808</td>\n",
       "      <td>-0.128579</td>\n",
       "      <td>-0.077966</td>\n",
       "      <td>-0.198373</td>\n",
       "      <td>-0.082845</td>\n",
       "      <td>0.285781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture45</th>\n",
       "      <td>-0.007016</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.106957</td>\n",
       "      <td>-0.083382</td>\n",
       "      <td>-0.061325</td>\n",
       "      <td>-0.046319</td>\n",
       "      <td>0.102352</td>\n",
       "      <td>0.068074</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>0.049504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010180</td>\n",
       "      <td>-0.133509</td>\n",
       "      <td>0.729756</td>\n",
       "      <td>-0.253574</td>\n",
       "      <td>-0.031881</td>\n",
       "      <td>-0.152737</td>\n",
       "      <td>-0.091636</td>\n",
       "      <td>-0.329445</td>\n",
       "      <td>0.092229</td>\n",
       "      <td>0.112980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture46</th>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.070921</td>\n",
       "      <td>0.098007</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>-0.152604</td>\n",
       "      <td>0.104725</td>\n",
       "      <td>-0.109145</td>\n",
       "      <td>0.010991</td>\n",
       "      <td>0.130410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397446</td>\n",
       "      <td>-0.068857</td>\n",
       "      <td>0.330856</td>\n",
       "      <td>-0.336664</td>\n",
       "      <td>-0.228092</td>\n",
       "      <td>-0.171549</td>\n",
       "      <td>0.116433</td>\n",
       "      <td>-0.293625</td>\n",
       "      <td>-0.160386</td>\n",
       "      <td>0.255229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture47</th>\n",
       "      <td>-0.037107</td>\n",
       "      <td>-0.136604</td>\n",
       "      <td>-0.062435</td>\n",
       "      <td>-0.110506</td>\n",
       "      <td>-0.070031</td>\n",
       "      <td>0.055671</td>\n",
       "      <td>-0.090270</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>-0.010259</td>\n",
       "      <td>0.016121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158000</td>\n",
       "      <td>-0.093926</td>\n",
       "      <td>0.223804</td>\n",
       "      <td>-0.305374</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>-0.225753</td>\n",
       "      <td>-0.085471</td>\n",
       "      <td>-0.264302</td>\n",
       "      <td>0.086202</td>\n",
       "      <td>0.360671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture48</th>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.023215</td>\n",
       "      <td>0.060191</td>\n",
       "      <td>0.041935</td>\n",
       "      <td>0.101103</td>\n",
       "      <td>-0.037777</td>\n",
       "      <td>0.031531</td>\n",
       "      <td>-0.209406</td>\n",
       "      <td>-0.069154</td>\n",
       "      <td>0.245759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464150</td>\n",
       "      <td>-0.027516</td>\n",
       "      <td>0.105581</td>\n",
       "      <td>-0.232863</td>\n",
       "      <td>-0.249658</td>\n",
       "      <td>-0.111697</td>\n",
       "      <td>0.191034</td>\n",
       "      <td>-0.215532</td>\n",
       "      <td>-0.188162</td>\n",
       "      <td>-0.223695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture49</th>\n",
       "      <td>0.040592</td>\n",
       "      <td>-0.035698</td>\n",
       "      <td>-0.097200</td>\n",
       "      <td>-0.066709</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>0.109285</td>\n",
       "      <td>-0.143383</td>\n",
       "      <td>0.055760</td>\n",
       "      <td>-0.036332</td>\n",
       "      <td>-0.081973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279787</td>\n",
       "      <td>0.031736</td>\n",
       "      <td>-0.221959</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>0.351001</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>0.053891</td>\n",
       "      <td>0.234115</td>\n",
       "      <td>0.416406</td>\n",
       "      <td>-0.121240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture50</th>\n",
       "      <td>0.065070</td>\n",
       "      <td>-0.104085</td>\n",
       "      <td>-0.142888</td>\n",
       "      <td>-0.074093</td>\n",
       "      <td>0.061479</td>\n",
       "      <td>0.168941</td>\n",
       "      <td>-0.168645</td>\n",
       "      <td>0.016636</td>\n",
       "      <td>-0.047341</td>\n",
       "      <td>-0.020585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170590</td>\n",
       "      <td>-0.021807</td>\n",
       "      <td>-0.181581</td>\n",
       "      <td>0.451533</td>\n",
       "      <td>0.214233</td>\n",
       "      <td>-0.056448</td>\n",
       "      <td>-0.028759</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>-0.106436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture51</th>\n",
       "      <td>0.042094</td>\n",
       "      <td>-0.076203</td>\n",
       "      <td>-0.085996</td>\n",
       "      <td>-0.018616</td>\n",
       "      <td>-0.025640</td>\n",
       "      <td>0.131768</td>\n",
       "      <td>-0.121988</td>\n",
       "      <td>-0.063331</td>\n",
       "      <td>-0.024245</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143776</td>\n",
       "      <td>0.622604</td>\n",
       "      <td>-0.229996</td>\n",
       "      <td>0.307044</td>\n",
       "      <td>-0.041230</td>\n",
       "      <td>0.116986</td>\n",
       "      <td>-0.011826</td>\n",
       "      <td>0.511406</td>\n",
       "      <td>-0.066616</td>\n",
       "      <td>-0.215327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture52</th>\n",
       "      <td>0.020134</td>\n",
       "      <td>-0.123478</td>\n",
       "      <td>-0.088673</td>\n",
       "      <td>0.077691</td>\n",
       "      <td>0.336566</td>\n",
       "      <td>-0.019252</td>\n",
       "      <td>-0.108100</td>\n",
       "      <td>-0.181662</td>\n",
       "      <td>-0.004657</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090858</td>\n",
       "      <td>0.118009</td>\n",
       "      <td>-0.109912</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>0.140571</td>\n",
       "      <td>-0.081631</td>\n",
       "      <td>0.225149</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>-0.005077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture53</th>\n",
       "      <td>-0.035839</td>\n",
       "      <td>-0.108301</td>\n",
       "      <td>-0.138591</td>\n",
       "      <td>0.040983</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>0.204153</td>\n",
       "      <td>-0.077668</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>-0.006605</td>\n",
       "      <td>-0.106245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316584</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>-0.313005</td>\n",
       "      <td>0.398652</td>\n",
       "      <td>0.304327</td>\n",
       "      <td>0.006425</td>\n",
       "      <td>-0.134226</td>\n",
       "      <td>0.287243</td>\n",
       "      <td>-0.014522</td>\n",
       "      <td>-0.233678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture54</th>\n",
       "      <td>-0.063260</td>\n",
       "      <td>-0.103474</td>\n",
       "      <td>-0.088763</td>\n",
       "      <td>-0.008583</td>\n",
       "      <td>-0.151736</td>\n",
       "      <td>0.076201</td>\n",
       "      <td>-0.082116</td>\n",
       "      <td>0.086128</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>-0.076219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196600</td>\n",
       "      <td>-0.037667</td>\n",
       "      <td>-0.233084</td>\n",
       "      <td>0.024124</td>\n",
       "      <td>0.264310</td>\n",
       "      <td>-0.125217</td>\n",
       "      <td>-0.116668</td>\n",
       "      <td>0.073513</td>\n",
       "      <td>0.064187</td>\n",
       "      <td>-0.026801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture55</th>\n",
       "      <td>0.024129</td>\n",
       "      <td>0.095548</td>\n",
       "      <td>0.160932</td>\n",
       "      <td>0.071901</td>\n",
       "      <td>-0.101321</td>\n",
       "      <td>-0.159061</td>\n",
       "      <td>0.146204</td>\n",
       "      <td>-0.085227</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>0.073856</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.044593</td>\n",
       "      <td>0.092112</td>\n",
       "      <td>-0.237519</td>\n",
       "      <td>-0.291978</td>\n",
       "      <td>-0.114019</td>\n",
       "      <td>-0.015190</td>\n",
       "      <td>-0.212793</td>\n",
       "      <td>-0.231096</td>\n",
       "      <td>-0.127655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture56</th>\n",
       "      <td>0.030386</td>\n",
       "      <td>-0.085175</td>\n",
       "      <td>-0.073517</td>\n",
       "      <td>0.060990</td>\n",
       "      <td>-0.021906</td>\n",
       "      <td>0.053379</td>\n",
       "      <td>-0.093891</td>\n",
       "      <td>-0.080115</td>\n",
       "      <td>-0.047895</td>\n",
       "      <td>0.104036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119757</td>\n",
       "      <td>0.118076</td>\n",
       "      <td>-0.037848</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>0.175092</td>\n",
       "      <td>-0.050280</td>\n",
       "      <td>-0.119383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture57</th>\n",
       "      <td>-0.010921</td>\n",
       "      <td>0.119563</td>\n",
       "      <td>0.160765</td>\n",
       "      <td>-0.105442</td>\n",
       "      <td>-0.027073</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.168194</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>-0.030139</td>\n",
       "      <td>0.109902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092112</td>\n",
       "      <td>-0.119757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.268992</td>\n",
       "      <td>-0.152891</td>\n",
       "      <td>-0.140901</td>\n",
       "      <td>-0.045883</td>\n",
       "      <td>-0.319452</td>\n",
       "      <td>-0.022197</td>\n",
       "      <td>0.207494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture58</th>\n",
       "      <td>0.032719</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>-0.041497</td>\n",
       "      <td>-0.015316</td>\n",
       "      <td>-0.078067</td>\n",
       "      <td>0.105663</td>\n",
       "      <td>-0.032881</td>\n",
       "      <td>0.066956</td>\n",
       "      <td>-0.040318</td>\n",
       "      <td>-0.085610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237519</td>\n",
       "      <td>0.118076</td>\n",
       "      <td>-0.268992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074182</td>\n",
       "      <td>0.171537</td>\n",
       "      <td>-0.092276</td>\n",
       "      <td>0.616210</td>\n",
       "      <td>-0.031069</td>\n",
       "      <td>-0.304614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture59</th>\n",
       "      <td>-0.071493</td>\n",
       "      <td>-0.213600</td>\n",
       "      <td>-0.205489</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>0.066583</td>\n",
       "      <td>0.257331</td>\n",
       "      <td>-0.216723</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>-0.041441</td>\n",
       "      <td>0.025699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291978</td>\n",
       "      <td>-0.037848</td>\n",
       "      <td>-0.152891</td>\n",
       "      <td>0.074182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.167058</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>-0.110914</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.030935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture60</th>\n",
       "      <td>-0.011355</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.151150</td>\n",
       "      <td>-0.036324</td>\n",
       "      <td>-0.046185</td>\n",
       "      <td>-0.025405</td>\n",
       "      <td>0.110988</td>\n",
       "      <td>-0.030828</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>-0.038042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114019</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>-0.140901</td>\n",
       "      <td>0.171537</td>\n",
       "      <td>-0.167058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.048102</td>\n",
       "      <td>0.235210</td>\n",
       "      <td>-0.080099</td>\n",
       "      <td>-0.183907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture61</th>\n",
       "      <td>0.053860</td>\n",
       "      <td>-0.054816</td>\n",
       "      <td>-0.066164</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.337250</td>\n",
       "      <td>-0.103416</td>\n",
       "      <td>-0.081673</td>\n",
       "      <td>-0.186244</td>\n",
       "      <td>-0.073920</td>\n",
       "      <td>0.095002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015190</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>-0.045883</td>\n",
       "      <td>-0.092276</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>-0.048102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066475</td>\n",
       "      <td>-0.065108</td>\n",
       "      <td>0.072695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture62</th>\n",
       "      <td>0.084747</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-0.061109</td>\n",
       "      <td>-0.078419</td>\n",
       "      <td>0.126172</td>\n",
       "      <td>-0.037976</td>\n",
       "      <td>-0.022211</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212793</td>\n",
       "      <td>0.175092</td>\n",
       "      <td>-0.319452</td>\n",
       "      <td>0.616210</td>\n",
       "      <td>-0.110914</td>\n",
       "      <td>0.235210</td>\n",
       "      <td>-0.066475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056610</td>\n",
       "      <td>-0.284343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture63</th>\n",
       "      <td>0.030623</td>\n",
       "      <td>-0.043275</td>\n",
       "      <td>-0.092966</td>\n",
       "      <td>-0.147547</td>\n",
       "      <td>-0.010801</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>-0.089921</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>-0.149321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231096</td>\n",
       "      <td>-0.050280</td>\n",
       "      <td>-0.022197</td>\n",
       "      <td>-0.031069</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>-0.080099</td>\n",
       "      <td>-0.065108</td>\n",
       "      <td>-0.056610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.126769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture64</th>\n",
       "      <td>-0.015542</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>-0.008868</td>\n",
       "      <td>-0.019413</td>\n",
       "      <td>-0.004705</td>\n",
       "      <td>-0.173556</td>\n",
       "      <td>0.046455</td>\n",
       "      <td>0.046681</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>-0.095424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127655</td>\n",
       "      <td>-0.119383</td>\n",
       "      <td>0.207494</td>\n",
       "      <td>-0.304614</td>\n",
       "      <td>0.030935</td>\n",
       "      <td>-0.183907</td>\n",
       "      <td>0.072695</td>\n",
       "      <td>-0.284343</td>\n",
       "      <td>0.126769</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   margin1   margin2   margin3   margin4   margin5  \\\n",
       "id         1.000000 -0.053081 -0.078534 -0.028574  0.043150  0.007684   \n",
       "margin1   -0.053081  1.000000  0.773549 -0.179688 -0.314669 -0.489724   \n",
       "margin2   -0.078534  0.773549  1.000000 -0.228772 -0.312056 -0.476718   \n",
       "margin3   -0.028574 -0.179688 -0.228772  1.000000  0.164221 -0.205369   \n",
       "margin4    0.043150 -0.314669 -0.312056  0.164221  1.000000  0.018378   \n",
       "margin5    0.007684 -0.489724 -0.476718 -0.205369  0.018378  1.000000   \n",
       "margin6   -0.074927  0.758771  0.839972 -0.175065 -0.266132 -0.455129   \n",
       "margin7   -0.007302  0.109006 -0.055350  0.019900 -0.307664 -0.086207   \n",
       "margin8    0.056806 -0.124012 -0.089673 -0.022426 -0.105205  0.049554   \n",
       "margin9   -0.023060 -0.206356 -0.164098  0.043036  0.235059  0.185162   \n",
       "margin10  -0.006432  0.428822  0.176159 -0.029816 -0.217483 -0.320386   \n",
       "margin11  -0.047977  0.781549  0.849201 -0.256124 -0.220119 -0.548524   \n",
       "margin12   0.001819 -0.524443 -0.483467 -0.009882 -0.180179  0.413255   \n",
       "margin13  -0.049372  0.462842  0.660399 -0.064409 -0.252150 -0.489214   \n",
       "margin14   0.059849 -0.379531 -0.320515  0.158219  0.379220  0.106122   \n",
       "margin15  -0.027199 -0.552260 -0.526286  0.017402 -0.234273  0.540390   \n",
       "margin16   0.079662 -0.064834 -0.074365 -0.040667 -0.017333  0.000334   \n",
       "margin17  -0.035232  0.291559  0.110797 -0.107460 -0.069347 -0.228345   \n",
       "margin18   0.044095  0.222736  0.320858 -0.088816  0.051832 -0.407037   \n",
       "margin19   0.040505 -0.273754 -0.252120 -0.084204  0.400266  0.227421   \n",
       "margin20   0.000579  0.389580  0.129412 -0.037440  0.013256 -0.341488   \n",
       "margin21   0.043163 -0.465394 -0.460889  0.065255 -0.129486  0.148439   \n",
       "margin22   0.065193 -0.396545 -0.359684 -0.288024 -0.196579  0.351477   \n",
       "margin23   0.045306 -0.153779 -0.141892 -0.150664  0.025925  0.197436   \n",
       "margin24  -0.001282 -0.276779 -0.274558 -0.264893 -0.107239  0.136985   \n",
       "margin25   0.040460 -0.479373 -0.417931 -0.069305  0.062092  0.413877   \n",
       "margin26   0.005537 -0.181478 -0.354864  0.016747  0.065622  0.204234   \n",
       "margin27   0.032548 -0.353273 -0.312422 -0.169631 -0.098996  0.313578   \n",
       "margin28   0.006222 -0.559708 -0.564831  0.168950  0.449570  0.672994   \n",
       "margin29   0.042058  0.102492  0.091433  0.248149  0.434055 -0.478354   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "texture35  0.011477 -0.004914 -0.015764  0.058091  0.311953 -0.158820   \n",
       "texture36  0.077701 -0.119765 -0.100324  0.170634  0.377576 -0.095531   \n",
       "texture37  0.005927 -0.037853 -0.024614 -0.046280 -0.075150  0.152993   \n",
       "texture38 -0.015149  0.087449  0.146434 -0.068787 -0.064274 -0.106026   \n",
       "texture39 -0.003046 -0.035971 -0.048092  0.054462 -0.113431 -0.136078   \n",
       "texture40 -0.030524 -0.031326 -0.042448  0.104570 -0.068769 -0.060359   \n",
       "texture41 -0.058889 -0.001119 -0.035450  0.035080 -0.030751  0.018565   \n",
       "texture42  0.062093  0.098142  0.078702 -0.065273  0.152589 -0.149533   \n",
       "texture43 -0.027366 -0.054724 -0.039035 -0.001174 -0.142654 -0.027338   \n",
       "texture44  0.024721  0.097241  0.042231  0.053400 -0.088522 -0.154120   \n",
       "texture45 -0.007016  0.065417  0.106957 -0.083382 -0.061325 -0.046319   \n",
       "texture46  0.016247  0.070921  0.098007  0.048375  0.035326 -0.152604   \n",
       "texture47 -0.037107 -0.136604 -0.062435 -0.110506 -0.070031  0.055671   \n",
       "texture48  0.009849  0.023215  0.060191  0.041935  0.101103 -0.037777   \n",
       "texture49  0.040592 -0.035698 -0.097200 -0.066709  0.066062  0.109285   \n",
       "texture50  0.065070 -0.104085 -0.142888 -0.074093  0.061479  0.168941   \n",
       "texture51  0.042094 -0.076203 -0.085996 -0.018616 -0.025640  0.131768   \n",
       "texture52  0.020134 -0.123478 -0.088673  0.077691  0.336566 -0.019252   \n",
       "texture53 -0.035839 -0.108301 -0.138591  0.040983 -0.075484  0.204153   \n",
       "texture54 -0.063260 -0.103474 -0.088763 -0.008583 -0.151736  0.076201   \n",
       "texture55  0.024129  0.095548  0.160932  0.071901 -0.101321 -0.159061   \n",
       "texture56  0.030386 -0.085175 -0.073517  0.060990 -0.021906  0.053379   \n",
       "texture57 -0.010921  0.119563  0.160765 -0.105442 -0.027073  0.008987   \n",
       "texture58  0.032719  0.026546 -0.041497 -0.015316 -0.078067  0.105663   \n",
       "texture59 -0.071493 -0.213600 -0.205489  0.044178  0.066583  0.257331   \n",
       "texture60 -0.011355  0.153934  0.151150 -0.036324 -0.046185 -0.025405   \n",
       "texture61  0.053860 -0.054816 -0.066164  0.128882  0.337250 -0.103416   \n",
       "texture62  0.084747  0.010933 -0.028058 -0.061109 -0.078419  0.126172   \n",
       "texture63  0.030623 -0.043275 -0.092966 -0.147547 -0.010801  0.006287   \n",
       "texture64 -0.015542  0.026005 -0.008868 -0.019413 -0.004705 -0.173556   \n",
       "\n",
       "            margin6   margin7   margin8   margin9    ...      texture55  \\\n",
       "id        -0.074927 -0.007302  0.056806 -0.023060    ...       0.024129   \n",
       "margin1    0.758771  0.109006 -0.124012 -0.206356    ...       0.095548   \n",
       "margin2    0.839972 -0.055350 -0.089673 -0.164098    ...       0.160932   \n",
       "margin3   -0.175065  0.019900 -0.022426  0.043036    ...       0.071901   \n",
       "margin4   -0.266132 -0.307664 -0.105205  0.235059    ...      -0.101321   \n",
       "margin5   -0.455129 -0.086207  0.049554  0.185162    ...      -0.159061   \n",
       "margin6    1.000000 -0.071411 -0.121303 -0.170549    ...       0.146204   \n",
       "margin7   -0.071411  1.000000  0.037644 -0.320724    ...      -0.085227   \n",
       "margin8   -0.121303  0.037644  1.000000 -0.080295    ...       0.024552   \n",
       "margin9   -0.170549 -0.320724 -0.080295  1.000000    ...       0.073856   \n",
       "margin10   0.226868  0.612331 -0.049991 -0.302118    ...      -0.076746   \n",
       "margin11   0.743649 -0.055520 -0.117326 -0.191377    ...       0.141750   \n",
       "margin12  -0.451356 -0.139712  0.124678 -0.034615    ...      -0.145951   \n",
       "margin13   0.503663 -0.106658  0.009598 -0.085411    ...       0.265179   \n",
       "margin14  -0.306552 -0.351568  0.077201  0.338621    ...      -0.024271   \n",
       "margin15  -0.499602 -0.020095  0.094224 -0.061898    ...      -0.122381   \n",
       "margin16  -0.071210 -0.024897  0.335955 -0.006539    ...       0.076427   \n",
       "margin17   0.077631  0.461286 -0.002844 -0.253823    ...      -0.044637   \n",
       "margin18   0.196095 -0.135123 -0.012535 -0.075608    ...       0.074677   \n",
       "margin19  -0.309225 -0.076838 -0.034999  0.164839    ...      -0.100991   \n",
       "margin20   0.218495  0.311759 -0.091445 -0.246071    ...      -0.032006   \n",
       "margin21  -0.438660  0.020308  0.168175  0.042928    ...      -0.029752   \n",
       "margin22  -0.347130 -0.127864  0.096561  0.131491    ...      -0.086113   \n",
       "margin23  -0.136857 -0.009349 -0.044345  0.026551    ...       0.029044   \n",
       "margin24  -0.284026  0.083492 -0.022074  0.078159    ...      -0.071690   \n",
       "margin25  -0.395700 -0.304669  0.069273  0.232797    ...      -0.086115   \n",
       "margin26  -0.324175  0.483942 -0.022088 -0.212183    ...      -0.159286   \n",
       "margin27  -0.292051 -0.224719  0.163186  0.308742    ...      -0.019722   \n",
       "margin28  -0.539296 -0.112346  0.003251  0.228237    ...      -0.200178   \n",
       "margin29   0.040905 -0.225355 -0.112383  0.183891    ...       0.095548   \n",
       "...             ...       ...       ...       ...    ...            ...   \n",
       "texture35  0.000698 -0.198401 -0.005798  0.072623    ...       0.052907   \n",
       "texture36 -0.122359 -0.217246 -0.051644  0.148328    ...      -0.018725   \n",
       "texture37 -0.002438  0.001360  0.008963 -0.041764    ...      -0.172002   \n",
       "texture38  0.143641 -0.038714  0.007253  0.052136    ...       0.179214   \n",
       "texture39 -0.009596  0.144267  0.064654 -0.125638    ...      -0.228493   \n",
       "texture40  0.017061  0.144842  0.047971 -0.119682    ...      -0.233073   \n",
       "texture41 -0.004248  0.169376 -0.013744 -0.136369    ...      -0.132521   \n",
       "texture42  0.026136 -0.141200 -0.068442 -0.067649    ...      -0.134870   \n",
       "texture43 -0.042935  0.134600  0.049215 -0.121188    ...      -0.185916   \n",
       "texture44  0.040304  0.133701  0.083342 -0.101064    ...       0.256892   \n",
       "texture45  0.102352  0.068074 -0.028694  0.049504    ...      -0.010180   \n",
       "texture46  0.104725 -0.109145  0.010991  0.130410    ...       0.397446   \n",
       "texture47 -0.090270  0.025315 -0.010259  0.016121    ...      -0.158000   \n",
       "texture48  0.031531 -0.209406 -0.069154  0.245759    ...       0.464150   \n",
       "texture49 -0.143383  0.055760 -0.036332 -0.081973    ...      -0.279787   \n",
       "texture50 -0.168645  0.016636 -0.047341 -0.020585    ...      -0.170590   \n",
       "texture51 -0.121988 -0.063331 -0.024245  0.027571    ...      -0.143776   \n",
       "texture52 -0.108100 -0.181662 -0.004657 -0.002368    ...      -0.090858   \n",
       "texture53 -0.077668  0.060495 -0.006605 -0.106245    ...      -0.316584   \n",
       "texture54 -0.082116  0.086128  0.000925 -0.076219    ...      -0.196600   \n",
       "texture55  0.146204 -0.085227  0.024552  0.073856    ...       1.000000   \n",
       "texture56 -0.093891 -0.080115 -0.047895  0.104036    ...      -0.044593   \n",
       "texture57  0.168194  0.005970 -0.030139  0.109902    ...       0.092112   \n",
       "texture58 -0.032881  0.066956 -0.040318 -0.085610    ...      -0.237519   \n",
       "texture59 -0.216723  0.014231 -0.041441  0.025699    ...      -0.291978   \n",
       "texture60  0.110988 -0.030828  0.012588 -0.038042    ...      -0.114019   \n",
       "texture61 -0.081673 -0.186244 -0.073920  0.095002    ...      -0.015190   \n",
       "texture62 -0.037976 -0.022211  0.000104 -0.057942    ...      -0.212793   \n",
       "texture63 -0.089921  0.064795  0.000549 -0.149321    ...      -0.231096   \n",
       "texture64  0.046455  0.046681  0.013802 -0.095424    ...      -0.127655   \n",
       "\n",
       "           texture56  texture57  texture58  texture59  texture60  texture61  \\\n",
       "id          0.030386  -0.010921   0.032719  -0.071493  -0.011355   0.053860   \n",
       "margin1    -0.085175   0.119563   0.026546  -0.213600   0.153934  -0.054816   \n",
       "margin2    -0.073517   0.160765  -0.041497  -0.205489   0.151150  -0.066164   \n",
       "margin3     0.060990  -0.105442  -0.015316   0.044178  -0.036324   0.128882   \n",
       "margin4    -0.021906  -0.027073  -0.078067   0.066583  -0.046185   0.337250   \n",
       "margin5     0.053379   0.008987   0.105663   0.257331  -0.025405  -0.103416   \n",
       "margin6    -0.093891   0.168194  -0.032881  -0.216723   0.110988  -0.081673   \n",
       "margin7    -0.080115   0.005970   0.066956   0.014231  -0.030828  -0.186244   \n",
       "margin8    -0.047895  -0.030139  -0.040318  -0.041441   0.012588  -0.073920   \n",
       "margin9     0.104036   0.109902  -0.085610   0.025699  -0.038042   0.095002   \n",
       "margin10   -0.072610   0.079052   0.087768  -0.126513   0.018067  -0.149181   \n",
       "margin11   -0.034858   0.105781  -0.028392  -0.237362   0.133325  -0.011383   \n",
       "margin12    0.020268  -0.163237   0.103672   0.237560  -0.042541  -0.060805   \n",
       "margin13   -0.014806   0.030994  -0.060017  -0.219646   0.062000  -0.011905   \n",
       "margin14    0.107388  -0.004474  -0.079880   0.034195  -0.059274   0.272856   \n",
       "margin15    0.014954  -0.172893   0.107174   0.276411  -0.043083  -0.116002   \n",
       "margin16   -0.017699  -0.034254  -0.035941  -0.037795  -0.009281  -0.020357   \n",
       "margin17   -0.086129   0.010728   0.026570  -0.029041  -0.038409  -0.124372   \n",
       "margin18    0.000899  -0.045910  -0.022644  -0.180710   0.050425   0.071060   \n",
       "margin19   -0.025468   0.263540  -0.164528   0.058517  -0.111007   0.230107   \n",
       "margin20   -0.057160   0.009616   0.115400  -0.201035   0.028969  -0.100641   \n",
       "margin21    0.056532  -0.154593  -0.025700   0.113636  -0.066246  -0.017666   \n",
       "margin22    0.064356  -0.113550   0.068471   0.078140  -0.027570  -0.094826   \n",
       "margin23   -0.044237   0.254292  -0.091261  -0.063507  -0.044597  -0.051263   \n",
       "margin24   -0.043192   0.082387  -0.048173   0.035218  -0.067283  -0.125326   \n",
       "margin25    0.112696  -0.077155   0.067716   0.097354  -0.034106   0.076806   \n",
       "margin26   -0.048409  -0.046828   0.019915   0.041627  -0.045724  -0.154131   \n",
       "margin27    0.071222  -0.070113   0.028736   0.029587  -0.028382  -0.035041   \n",
       "margin28    0.039610  -0.112500   0.014729   0.287045  -0.030730   0.097737   \n",
       "margin29    0.036195  -0.054262  -0.075455  -0.113618   0.004610   0.347806   \n",
       "...              ...        ...        ...        ...        ...        ...   \n",
       "texture35  -0.102531   0.141274  -0.330283  -0.000116  -0.178829   0.339455   \n",
       "texture36   0.136234  -0.043316  -0.083239   0.007433  -0.055935   0.720787   \n",
       "texture37  -0.044476  -0.216758   0.153860  -0.065948   0.371657  -0.068894   \n",
       "texture38  -0.109673   0.629213  -0.356145  -0.226714  -0.183605  -0.017244   \n",
       "texture39  -0.120553  -0.072297  -0.219698   0.071793  -0.161698  -0.082567   \n",
       "texture40  -0.109537  -0.088102  -0.080099   0.175983  -0.126555  -0.106460   \n",
       "texture41  -0.079031  -0.144386  -0.076520  -0.061453  -0.070815  -0.084851   \n",
       "texture42   0.044533  -0.151141   0.032632  -0.024830  -0.038028   0.564136   \n",
       "texture43  -0.107071  -0.025032  -0.216058   0.103457  -0.151033  -0.092296   \n",
       "texture44  -0.088973  -0.067311  -0.230779  -0.110808  -0.128579  -0.077966   \n",
       "texture45  -0.133509   0.729756  -0.253574  -0.031881  -0.152737  -0.091636   \n",
       "texture46  -0.068857   0.330856  -0.336664  -0.228092  -0.171549   0.116433   \n",
       "texture47  -0.093926   0.223804  -0.305374   0.367753  -0.225753  -0.085471   \n",
       "texture48  -0.027516   0.105581  -0.232863  -0.249658  -0.111697   0.191034   \n",
       "texture49   0.031736  -0.221959   0.431138   0.351001  -0.004990   0.053891   \n",
       "texture50  -0.021807  -0.181581   0.451533   0.214233  -0.056448  -0.028759   \n",
       "texture51   0.622604  -0.229996   0.307044  -0.041230   0.116986  -0.011826   \n",
       "texture52   0.118009  -0.109912  -0.005146   0.140571  -0.081631   0.225149   \n",
       "texture53   0.002383  -0.313005   0.398652   0.304327   0.006425  -0.134226   \n",
       "texture54  -0.037667  -0.233084   0.024124   0.264310  -0.125217  -0.116668   \n",
       "texture55  -0.044593   0.092112  -0.237519  -0.291978  -0.114019  -0.015190   \n",
       "texture56   1.000000  -0.119757   0.118076  -0.037848   0.003775   0.031717   \n",
       "texture57  -0.119757   1.000000  -0.268992  -0.152891  -0.140901  -0.045883   \n",
       "texture58   0.118076  -0.268992   1.000000   0.074182   0.171537  -0.092276   \n",
       "texture59  -0.037848  -0.152891   0.074182   1.000000  -0.167058   0.007776   \n",
       "texture60   0.003775  -0.140901   0.171537  -0.167058   1.000000  -0.048102   \n",
       "texture61   0.031717  -0.045883  -0.092276   0.007776  -0.048102   1.000000   \n",
       "texture62   0.175092  -0.319452   0.616210  -0.110914   0.235210  -0.066475   \n",
       "texture63  -0.050280  -0.022197  -0.031069   0.083541  -0.080099  -0.065108   \n",
       "texture64  -0.119383   0.207494  -0.304614   0.030935  -0.183907   0.072695   \n",
       "\n",
       "           texture62  texture63  texture64  \n",
       "id          0.084747   0.030623  -0.015542  \n",
       "margin1     0.010933  -0.043275   0.026005  \n",
       "margin2    -0.028058  -0.092966  -0.008868  \n",
       "margin3    -0.061109  -0.147547  -0.019413  \n",
       "margin4    -0.078419  -0.010801  -0.004705  \n",
       "margin5     0.126172   0.006287  -0.173556  \n",
       "margin6    -0.037976  -0.089921   0.046455  \n",
       "margin7    -0.022211   0.064795   0.046681  \n",
       "margin8     0.000104   0.000549   0.013802  \n",
       "margin9    -0.057942  -0.149321  -0.095424  \n",
       "margin10    0.003667   0.105641   0.093366  \n",
       "margin11   -0.024234  -0.056761   0.019686  \n",
       "margin12    0.085389   0.067379   0.023136  \n",
       "margin13    0.011038  -0.122140  -0.034450  \n",
       "margin14   -0.003158  -0.078160  -0.019282  \n",
       "margin15    0.101005   0.020220  -0.000694  \n",
       "margin16   -0.009151  -0.007771  -0.004063  \n",
       "margin17   -0.035676   0.126152   0.042327  \n",
       "margin18    0.023176  -0.050741  -0.033799  \n",
       "margin19   -0.130427   0.074373   0.024504  \n",
       "margin20    0.050417   0.055054   0.066308  \n",
       "margin21   -0.012576   0.114639   0.120255  \n",
       "margin22    0.122838   0.074553   0.021181  \n",
       "margin23   -0.074246   0.101806   0.008550  \n",
       "margin24   -0.025083   0.113317   0.102556  \n",
       "margin25    0.151123  -0.022241  -0.079320  \n",
       "margin26    0.031357   0.153522   0.019188  \n",
       "margin27    0.124498  -0.039221  -0.051501  \n",
       "margin28    0.043851   0.019208  -0.160188  \n",
       "margin29   -0.089469  -0.033504   0.043764  \n",
       "...              ...        ...        ...  \n",
       "texture35  -0.332167   0.202983   0.199283  \n",
       "texture36  -0.039888  -0.086766   0.014676  \n",
       "texture37   0.174871  -0.034625  -0.254716  \n",
       "texture38  -0.322130  -0.087577   0.379235  \n",
       "texture39  -0.190047   0.109013   0.588546  \n",
       "texture40  -0.110457   0.054784   0.156412  \n",
       "texture41  -0.101465   0.028329   0.050637  \n",
       "texture42   0.008614   0.270722  -0.033518  \n",
       "texture43  -0.165789   0.048990   0.508884  \n",
       "texture44  -0.198373  -0.082845   0.285781  \n",
       "texture45  -0.329445   0.092229   0.112980  \n",
       "texture46  -0.293625  -0.160386   0.255229  \n",
       "texture47  -0.264302   0.086202   0.360671  \n",
       "texture48  -0.215532  -0.188162  -0.223695  \n",
       "texture49   0.234115   0.416406  -0.121240  \n",
       "texture50   0.195700   0.084123  -0.106436  \n",
       "texture51   0.511406  -0.066616  -0.215327  \n",
       "texture52   0.029619   0.067200  -0.005077  \n",
       "texture53   0.287243  -0.014522  -0.233678  \n",
       "texture54   0.073513   0.064187  -0.026801  \n",
       "texture55  -0.212793  -0.231096  -0.127655  \n",
       "texture56   0.175092  -0.050280  -0.119383  \n",
       "texture57  -0.319452  -0.022197   0.207494  \n",
       "texture58   0.616210  -0.031069  -0.304614  \n",
       "texture59  -0.110914   0.083541   0.030935  \n",
       "texture60   0.235210  -0.080099  -0.183907  \n",
       "texture61  -0.066475  -0.065108   0.072695  \n",
       "texture62   1.000000  -0.056610  -0.284343  \n",
       "texture63  -0.056610   1.000000   0.126769  \n",
       "texture64  -0.284343   0.126769   1.000000  \n",
       "\n",
       "[193 rows x 193 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.959595959596\n",
      "0.958477508651\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 14.]\n",
      "1.0\n",
      "1.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(12)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "output = clf.predict_proba(X_test).astype(str)\n",
    "output = np.insert(output,0,ids,axis=1)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "for word in output:\n",
    "    open_file_object.writerow(word)\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.48661074, -0.13135701, -0.33095592, ..., -0.39487147,\n",
       "        -0.65214311,  0.26239707],\n",
       "       [-0.58560191, -0.73488047, -0.02856115, ..., -0.49497401,\n",
       "         2.18166976,  0.13364087],\n",
       "       [-0.58560191, -0.48340807, -0.48219201, ..., -0.52001246,\n",
       "         0.83564129, -0.72463124],\n",
       "       ..., \n",
       "       [-0.78358426, -0.63430181, -1.23821765, ...,  0.1807566 ,\n",
       "        -0.65214311, -0.76756462],\n",
       "       [-0.88257543, -0.73488047,  0.5762671 , ..., -0.52001246,\n",
       "        -0.51045972, -0.72463124],\n",
       "       [ 0.30542003, -0.23196142, -0.02856115, ...,  0.08065405,\n",
       "         1.18988604,  0.13364087]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "x_train = train.drop(['id', 'species'], axis=1).values\n",
    "le = LabelEncoder().fit(train['species'])\n",
    "y_train = le.transform(train['species'])\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-276b3028a2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'size' is not defined"
     ]
    }
   ],
   "source": [
    "size(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190080"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "x_train = train.drop(['id', 'species'], axis=1).values\n",
    "le = LabelEncoder().fit(train['species'])\n",
    "y_train = le.transform(train['species'])\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 192)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "x_train = train.drop(['id', 'species'], axis=1).values\n",
    "le = LabelEncoder().fit(train['species'])\n",
    "y_train = le.transform(train['species'])\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "params = {'C':[1, 10, 50, 100, 500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "log_reg = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "clf = GridSearchCV(log_reg, params, scoring='log_loss', refit='True', n_jobs=1, cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(\"best params: \" + str(clf.best_params_))\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "  print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std(), params))\n",
    "  print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:127: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8689068a8200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1241\u001b[0;31m             for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,\n\u001b[0;32m--> 726\u001b[0;31m                                      maxiter=max_iter, tol=tol)\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             coef_, intercept_, n_iter_i, = _fit_liblinear(\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36mnewton_cg\u001b[0;34m(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Inner loop: solve the Newton update by conjugate gradient, to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# avoid inverting the Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mxsupi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhess_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0malphak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_cg\u001b[0;34m(fhess_p, fgrad, maxiter, tol)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mdri1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mbetai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdri1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdri0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mpsupi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mri\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbetai\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpsupi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mdri0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdri1\u001b[0m          \u001b[0;31m# update np.dot(ri,ri) for next time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "params = {'C':[1, 10, 50, 100, 500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "log_reg = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "clf = GridSearchCV(log_reg, params, scoring='log_loss', refit='True', n_jobs=1, cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "print ('Predicted class %s' % (clf.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-1b37358adff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1241\u001b[0;31m             for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,\n\u001b[0;32m--> 726\u001b[0;31m                                      maxiter=max_iter, tol=tol)\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             coef_, intercept_, n_iter_i, = _fit_liblinear(\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36mnewton_cg\u001b[0;34m(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Inner loop: solve the Newton update by conjugate gradient, to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# avoid inverting the Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mxsupi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhess_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0malphak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_cg\u001b[0;34m(fhess_p, fgrad, maxiter, tol)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mAp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhess_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# check curvature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mcurv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mhessp\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# r_yhat holds the result of applying the R-operator on the multinomial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# estimator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minter_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr_yhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "params = {'C':[1, 10, 50, 100, 500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "log_reg = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "clf = GridSearchCV(log_reg, params, refit='True', n_jobs=1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "print ('Predicted class %s' % (clf.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "params = {'C':[1, 10, 50, 100, 500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "log_reg = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "clf = GridSearchCV(log_reg, params, refit='True')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "print ('Predicted class %s' % (clf.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "predict = clf.predict(X_test)\n",
    "output = clf.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 51.,  50.,   1.,  19.,  14.,   3.,   3.,  28.,  84.,   8.,  43.,\n",
       "        74.,  75.,  10.,  52.,  46.,  45.,  73.,  13.,  71.,  61.,  68.,\n",
       "        57.,  77.,   1.,  70.,  28.,  15.,  35.,  70.,  53.,  74.,  47.,\n",
       "        50.,   4.,  36.,  14.,  55.,  36.,  93.,   8.,  32.,   8.,   9.,\n",
       "        71.,  70.,  38.,  23.,  94.,  18.,  17.,   5.,  55.,  94.,  14.,\n",
       "        86.,  62.,  33.,  51.,  54.,  88.,  56.,  21.,  59.,  65.,  11.,\n",
       "        48.,   5.,  13.,   4.,  54.,  57.,  29.,   7.,  31.,  98.,  92.,\n",
       "        84.,  25.,  10.,  61.,  43.,  85.,  24.,   1.,   2.,  23.,  83.,\n",
       "        39.,  22.,  48.,  90.,  25.,  21.,  37.,  56.,  41.,  95.,   7.,\n",
       "        89.,  98.,  77.,   3.,  12.,  31.,  84.,  53.,  96.,  64.,  70.,\n",
       "        93.,  93.,  67.,  30.,   8.,  88.,  60.,  87.,   6.,  57.,  34.,\n",
       "        34.,  60.,  17.,  75.,  27.,  51.,  88.,  39.,  23.,  23.,   2.,\n",
       "        41.,  61.,  24.,  97.,  29.,  28.,  68.,  81.,  42.,  51.,  86.,\n",
       "        62.,  60.,  52.,  95.,  81.,  42.,  96.,  95.,  20.,  59.,  35.,\n",
       "        86.,   1.,  26.,  28.,  43.,  75.,  20.,  60.,  46.,  79.,  22.,\n",
       "        79.,  69.,  87.,  65.,  97.,  75.,  21.,  29.,  21.,  11.,  10.,\n",
       "        58.,  94.,  27.,  22.,  15.,  45.,  89.,  54.,  43.,   5.,  23.,\n",
       "        94.,  40.,  49.,  89.,  72.,  36.,  11.,  81.,  95.,  18.,  91.,\n",
       "        29.,  64.,  80.,   6.,  78.,  45.,  28.,   9.,  78.,  90.,  44.,\n",
       "        89.,  92.,  80.,   2.,  59.,   0.,  96.,  70.,  32.,  96.,  78.,\n",
       "        91.,  69.,  44.,  36.,   5.,  60.,  49.,  58.,  94.,  67.,  92.,\n",
       "        88.,  90.,  79.,  25.,  29.,  18.,   0.,  76.,  27.,  70.,  71.,\n",
       "        44.,  70.,  32.,  90.,  30.,  82.,  34.,  30.,  82.,  96.,  48.,\n",
       "        65.,  57.,  64.,  26.,  53.,  55.,  73.,   9.,  86.,  83.,  26.,\n",
       "        30.,  63.,  13.,  22.,  79.,  63.,  12.,  78.,  36.,  14.,  27.,\n",
       "        25.,  67.,  36.,  20.,  54.,  76.,  69.,  67.,  97.,  80.,  44.,\n",
       "        92.,  69.,  23.,  21.,  16.,  51.,  33.,  77.,  16.,  11.,  97.,\n",
       "         1.,  52.,  39.,  24.,  52.,  42.,  52.,   2.,  73.,  96.,  83.,\n",
       "        88.,   9.,  63.,  50.,  16.,  37.,  87.,  95.,   3.,  35.,  83.,\n",
       "        60.,  59.,  58.,   0.,  79.,  62.,  96.,  93.,  68.,  55.,  46.,\n",
       "        19.,  46.,  94.,  18.,   0.,  33.,  89.,  39.,  62.,  48.,  42.,\n",
       "         6.,  31.,  91.,  73.,  81.,  12.,  85.,  47.,   6.,  79.,   2.,\n",
       "        22.,  35.,  43.,   6.,  80.,  78.,  82.,   5.,  61.,  39.,  43.,\n",
       "        33.,  69.,  56.,  71.,  45.,  59.,  42.,  66.,  86.,  98.,  83.,\n",
       "        90.,  64.,  82.,  11.,  79.,  56.,  76.,  49.,  48.,  20.,  74.,\n",
       "        15.,  33.,  49.,  89.,  44.,  17.,  35.,  14.,  55.,  23.,  34.,\n",
       "        44.,  32.,  30.,  36.,   9.,  72.,  31.,  61.,  50.,  82.,  34.,\n",
       "        28.,  22.,  92.,  72.,  11.,  19.,   4.,  87.,  51.,  80.,  39.,\n",
       "        84.,  32.,  66.,  36.,  41.,  68.,  80.,   4.,  26.,  68.,  96.,\n",
       "        20.,  39.,  34.,  39.,  56.,  73.,  76.,  84.,   7.,  67.,  37.,\n",
       "         8.,  95.,  85.,  62.,  10.,  65.,  48.,   2.,  94.,  69.,  41.,\n",
       "        52.,   3.,  49.,  47.,  76.,  52.,  94.,  26.,  88.,  63.,  45.,\n",
       "        39.,  66.,  87.,  75.,  74.,   7.,  64.,  65.,  78.,  63.,  56.,\n",
       "        21.,  61.,  88.,  62.,  91.,  59.,  52.,  74.,  15.,  85.,   8.,\n",
       "        66.,  57.,  83.,  82.,  72.,  62.,  96.,   7.,  67.,  66.,  57.,\n",
       "        66.,  75.,  35.,  18.,   9.,  54.,  91.,  65.,  19.,  15.,  10.,\n",
       "        24.,  71.,  69.,  48.,  39.,  98.,  16.,  19.,  45.,  74.,   6.,\n",
       "        67.,  42.,  34.,  80.,  47.,  85.,  28.,  85.,  47.,  25.,  27.,\n",
       "        58.,  68.,  84.,  97.,  63.,  97.,  76.,  81.,  87.,  77.,  13.,\n",
       "         0.,  28.,  41.,  14.,  12.,  33.,  69.,  46.,   4.,   4.,  47.,\n",
       "        30.,  19.,  95.,  13.,  77.,  98.,   5.,  89.,  72.,  53.,  32.,\n",
       "        77.,  40.,  68.,  26.,  92.,  16.,  81.,  37.,  14.,  93.,  80.,\n",
       "        53.,  46.,  25.,  50.,  17.,  37.,  93.,   0.,  20.,  54.,  10.,\n",
       "        91.,  40.,  81.,  53.,  97.,  27.,   1.,  12.,  54.,  73.,  15.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -1.70358243e+01,  -2.14422031e+01,  -2.15763378e+01, ...,\n",
       "         -1.92970999e+01,  -2.02532413e+01,  -1.71789456e+01],\n",
       "       [ -2.08925881e+01,  -1.68434130e+01,  -1.75636582e+01, ...,\n",
       "         -1.15273037e+01,  -2.32006616e+01,  -1.01244651e+01],\n",
       "       [ -1.44675282e+01,  -5.51407198e-03,  -1.23982440e+01, ...,\n",
       "         -2.07518275e+01,  -2.27375710e+01,  -7.47759996e+00],\n",
       "       ..., \n",
       "       [ -1.16740278e+01,  -1.59360725e+01,  -1.44079665e+01, ...,\n",
       "         -1.60671022e+01,  -2.11948902e+01,  -1.38634472e+01],\n",
       "       [ -1.83431588e+01,  -1.51549664e+01,  -9.68782482e+00, ...,\n",
       "         -1.66021778e+01,  -1.89010122e+01,  -1.21125747e+01],\n",
       "       [ -3.40463512e+01,  -1.26506343e+01,  -1.10833074e+01, ...,\n",
       "         -1.67398500e+01,  -1.74334025e+01,  -1.81351897e+01]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "#predict = clf.predict(X_test)\n",
    "output = clf.predict_log_proba(X_test)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0014683098551391087"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][predict[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-c128949a0caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "int(output[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-803d93dab7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msumv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msumv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msumv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msumv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msumv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m594\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msumv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "sumv =0\n",
    "for i in range(594) :\n",
    "    sumv = sumv + output[i][predict[i]]\n",
    "sumv = ((-1)*sumv)/594;\n",
    "sumv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10222796943033395"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumv =0\n",
    "for i in range(594) :\n",
    "    sumv = sumv + output[i][predict[i]]\n",
    "sumv = ((-1)*sumv)/594;\n",
    "sumv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.826989619377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -4.50901828e+01,  -3.48658995e+01,  -4.24689700e+01, ...,\n",
       "         -2.54993920e+01,  -3.62458964e+01,  -1.79831327e+01],\n",
       "       [ -5.32375403e+01,  -3.29984270e+01,  -3.69772489e+01, ...,\n",
       "         -1.98380466e+01,  -4.48752528e+01,  -1.50398845e+01],\n",
       "       [ -3.74342113e+01,  -1.09712044e-05,  -2.94220970e+01, ...,\n",
       "         -6.47353626e+01,  -6.51368262e+01,  -2.28611802e+01],\n",
       "       ..., \n",
       "       [ -1.50883530e+01,  -2.02201969e+01,  -2.42835448e+01, ...,\n",
       "         -2.92285878e+01,  -3.97423977e+01,  -1.25390159e+01],\n",
       "       [ -6.99567903e+01,  -3.93789502e+01,  -3.45007901e+01, ...,\n",
       "         -4.72474907e+01,  -4.34425288e+01,  -2.36766392e+01],\n",
       "       [ -9.61031801e+01,  -5.18498281e+01,  -2.03036945e+01, ...,\n",
       "         -2.67114847e+01,  -3.33478501e+01,  -3.60437673e+01]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "logistic = LogisticRegression(solver='newton-cg',C=20,max_iter=500)\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.0001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "#predict = clf.predict(X_test)\n",
    "output = clf.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.911428571429\n",
      "0.747404844291\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.944636678201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "params = {'C':[1, 10, 50, 100, 500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "logi = LogisticRegression(solver='newton-cg',max_iter=500)\n",
    "logistic = GridSearchCV(logi, params, refit='True')\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=600)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "#predict = clf.predict(X_test)\n",
    "output = clf.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "1.0\n",
      "0.927335640138\n",
      "Neural Network\n",
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.944636678201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "params = {'C':[1,5,8, 10,12,15,25,50,75,100,500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "logi = LogisticRegression(solver='newton-cg',max_iter=500)\n",
    "logistic = GridSearchCV(logi, params, refit='True')\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=600)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "#predict = clf.predict(X_test)\n",
    "output = clf.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "0.897142857143\n",
      "0.650519031142\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.937716262976\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "params = {'C':[1,5,8, 10,12,15,25,50,75,100,500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "logi = LogisticRegression(solver='newton-cg',max_iter=500)\n",
    "logistic = GridSearchCV(logi, params, refit='True')\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "output = logistic.predict_proba(X_test).astype(str)\n",
    "output = np.insert(output,0,ids,axis=1)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict1.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "for word in output:\n",
    "    open_file_object.writerow(word)\n",
    "predictions_file.close()\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 24.]\n",
      "1.0\n",
      "0.927335640138\n",
      "Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhumihar/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class [ 14.]\n",
      "1.0\n",
      "0.944636678201\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.species = le.fit_transform(processed_df.species)\n",
    "    return processed_df\n",
    "train = pd.read_csv('train.csv')\n",
    "train = preprocess_df(train)\n",
    "y = train['species'].values\n",
    "train = train.drop(['id'],axis=1)\n",
    "X = train.values\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:700,1:]\n",
    "X_cv = X[701:,1:]\n",
    "y_train = X[:700,0]\n",
    "y_cv = X[701: ,0]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = np.array(test['id'])\n",
    "test = test.drop(['id'],axis=1)\n",
    "X_test = test.values\n",
    "params = {'C':[1,5,8, 10,12,15,25,50,75,100,500, 1000, 2000], 'tol': [0.001, 0.0001, 0.005]}\n",
    "logi = LogisticRegression(solver='newton-cg',max_iter=500)\n",
    "logistic = GridSearchCV(logi, params, refit='True')\n",
    "logistic.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (logistic.predict(X_test[300,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(logistic.score (X_train ,y_train))\n",
    "print(logistic.score (X_cv ,y_cv))\n",
    "print(\"Neural Network\")\n",
    "clf = MLPClassifier(activation='logistic',solver='lbfgs', alpha=0.001, max_iter=500)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Predicted class %s' % (clf.predict(X_test[4,:])))\n",
    "#print ('Probabilities for each class %s'% logistic.predict_proba(X_train[0,:]))\n",
    "print(clf.score (X_train ,y_train))\n",
    "print(clf.score (X_cv ,y_cv))\n",
    "output = logistic.predict_proba(X_test).astype(str)\n",
    "output = np.insert(output,0,ids,axis=1)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict1.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "for word in output:\n",
    "    open_file_object.writerow(word)\n",
    "predictions_file.close()\n",
    "\n",
    "output1 = clf.predict_proba(X_test).astype(str)\n",
    "output1 = np.insert(output,0,ids,axis=1)\n",
    "train = pd.read_csv('train.csv')\n",
    "leaf_class = train['species'].unique()\n",
    "leaf_class = np.sort(leaf_class)\n",
    "leaf_class = np.insert(leaf_class,0,\"id\");\n",
    "predictions_file = open(\"predict.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow(leaf_class)\n",
    "for word in output:\n",
    "    open_file_object.writerow(word)\n",
    "predictions_file.close()\n",
    "\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
